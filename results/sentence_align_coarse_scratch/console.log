Loaded config: {'data': {'batch_size': 64, 'image_size': 224, 'num_workers': 4, 'path': 'src/data/', 'seq_len': 16, 'vocab_size': 5000}, 'model': {'align_text_source': 'coarse', 'backbone': 'vit_tiny_patch16_224', 'freeze_sentence_encoder': True, 'fusion_include_coarse': False, 'fusion_include_fine': False, 'fusion_include_image': True, 'latent_dim': 512, 'method_option': 'alignment', 'num_classes': 5, 'sentence_encoder_device': 'cuda', 'sentence_encoder_name': 'train_sentence_encoder/output_coarse', 'sentence_pretrained': False, 'use_coarse': False, 'use_fine': False, 'use_sentence_encoder': True}, 'project': {'name': 'FireDamageClassification', 'version': '1.0'}, 'training': {'align_temperature': 0.07, 'drop_path_rate': 0.1, 'dropout': 0.2, 'epochs': 70, 'lambda_align': 0.1, 'lambda_cls': 1.0, 'lambda_vae': 0.1, 'lr': '1e-4', 'weight_decay': '1e-3'}}
Results will be saved to: results/sentence_align_coarse_scratch
Using device: cuda
Model Type: method
Splitting by Region:
Train Regions: ['GLA', 'VAL', 'MOS', 'CAS', 'MIL', 'POST', 'CRE', 'DIN', 'MON', 'SCU', 'FOR', 'BEU']
Val Regions: ['BEA', 'AUG', 'ZOG']
Test Regions: ['CAL', 'FAI', 'DIX']
Scanning images in src/data/image for mode train...
Loaded 5393 samples for mode train
Scanning images in src/data/image for mode val...
Loaded 1316 samples for mode val
Scanning images in src/data/image for mode test...
Loaded 1122 samples for mode test
Calculating class weights for WeightedRandomSampler...
Train Class Counts: {1: 204, 0: 2281, 4: 766, 3: 2060, 2: 82}
Data loaded: Train=5393, Val=1316, Test=1122
/root/shared-nvme/fire/src/models/method.py:243: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  checkpoint = torch.load(ckpt_path, map_location="cpu")
Parameters: 11,932,103
FLOPs: 1083166208.0
Starting training loop...
Batch 0: Loss 1.7198 (Cls: 1.2874) LR: 0.000004
Batch 10: Loss 1.6057 (Cls: 1.1823) LR: 0.000004
Batch 20: Loss 1.5296 (Cls: 1.1073) LR: 0.000004
Batch 30: Loss 1.5396 (Cls: 1.1132) LR: 0.000004
Batch 40: Loss 1.5492 (Cls: 1.1239) LR: 0.000004
Batch 50: Loss 1.5540 (Cls: 1.1239) LR: 0.000004
Batch 60: Loss 1.5651 (Cls: 1.1338) LR: 0.000004
Batch 70: Loss 1.6175 (Cls: 1.1864) LR: 0.000004
Batch 80: Loss 1.6060 (Cls: 1.1869) LR: 0.000004
Epoch 1: Loss 1.5760, Acc 0.2080
/root/miniconda3/envs/fire/lib/python3.10/site-packages/torch/nn/modules/transformer.py:502: UserWarning: The PyTorch API of nested tensors is in prototype stage and will change in the near future. (Triggered internally at /opt/conda/conda-bld/pytorch_1729647352509/work/aten/src/ATen/NestedTensorImpl.cpp:178.)
  output = torch._nested_tensor_from_mask(
Val Loss 1.5972, Val Acc 0.2021
New best model saved with Val Acc: 0.2021
Batch 0: Loss 1.6036 (Cls: 1.1769) LR: 0.000005
Batch 10: Loss 1.5899 (Cls: 1.1719) LR: 0.000005
Batch 20: Loss 1.5756 (Cls: 1.1448) LR: 0.000005
Batch 30: Loss 1.5763 (Cls: 1.1435) LR: 0.000005
Batch 40: Loss 1.5261 (Cls: 1.0963) LR: 0.000005
Batch 50: Loss 1.5807 (Cls: 1.1512) LR: 0.000005
Batch 60: Loss 1.6113 (Cls: 1.1853) LR: 0.000006
Batch 70: Loss 1.5349 (Cls: 1.1083) LR: 0.000006
Batch 80: Loss 1.5335 (Cls: 1.0984) LR: 0.000006
Epoch 2: Loss 1.5495, Acc 0.2145
Val Loss 1.6138, Val Acc 0.2036
New best model saved with Val Acc: 0.2036
Batch 0: Loss 1.5701 (Cls: 1.1472) LR: 0.000006
Batch 10: Loss 1.5072 (Cls: 1.0889) LR: 0.000006
Batch 20: Loss 1.4957 (Cls: 1.0722) LR: 0.000007
Batch 30: Loss 1.4742 (Cls: 1.0474) LR: 0.000007
Batch 40: Loss 1.5145 (Cls: 1.0944) LR: 0.000007
Batch 50: Loss 1.5665 (Cls: 1.1398) LR: 0.000008
Batch 60: Loss 1.4865 (Cls: 1.0627) LR: 0.000008
Batch 70: Loss 1.4800 (Cls: 1.0614) LR: 0.000008
Batch 80: Loss 1.5279 (Cls: 1.1121) LR: 0.000009
Epoch 3: Loss 1.5355, Acc 0.2229
Val Loss 1.5482, Val Acc 0.3078
New best model saved with Val Acc: 0.3078
Batch 0: Loss 1.5191 (Cls: 1.0977) LR: 0.000009
Batch 10: Loss 1.5275 (Cls: 1.1072) LR: 0.000009
Batch 20: Loss 1.5366 (Cls: 1.1118) LR: 0.000010
Batch 30: Loss 1.5875 (Cls: 1.1603) LR: 0.000010
Batch 40: Loss 1.4766 (Cls: 1.0612) LR: 0.000010
Batch 50: Loss 1.5780 (Cls: 1.1507) LR: 0.000011
Batch 60: Loss 1.5419 (Cls: 1.1307) LR: 0.000011
Batch 70: Loss 1.4884 (Cls: 1.0693) LR: 0.000012
Batch 80: Loss 1.4909 (Cls: 1.0695) LR: 0.000012
Epoch 4: Loss 1.5266, Acc 0.2381
Val Loss 1.5819, Val Acc 0.2622
Batch 0: Loss 1.5663 (Cls: 1.1447) LR: 0.000012
Batch 10: Loss 1.5062 (Cls: 1.0867) LR: 0.000013
Batch 20: Loss 1.4361 (Cls: 1.0123) LR: 0.000013
Batch 30: Loss 1.5522 (Cls: 1.1306) LR: 0.000014
Batch 40: Loss 1.4688 (Cls: 1.0492) LR: 0.000014
Batch 50: Loss 1.5083 (Cls: 1.0855) LR: 0.000015
Batch 60: Loss 1.4893 (Cls: 1.0722) LR: 0.000015
Batch 70: Loss 1.5508 (Cls: 1.1279) LR: 0.000016
Batch 80: Loss 1.4694 (Cls: 1.0495) LR: 0.000017
Epoch 5: Loss 1.5174, Acc 0.2388
Val Loss 1.5399, Val Acc 0.4331
New best model saved with Val Acc: 0.4331
Batch 0: Loss 1.5019 (Cls: 1.0818) LR: 0.000017
Batch 10: Loss 1.4692 (Cls: 1.0491) LR: 0.000017
Batch 20: Loss 1.5350 (Cls: 1.1097) LR: 0.000018
Batch 30: Loss 1.5421 (Cls: 1.1196) LR: 0.000019
Batch 40: Loss 1.4904 (Cls: 1.0740) LR: 0.000019
Batch 50: Loss 1.5891 (Cls: 1.1608) LR: 0.000020
Batch 60: Loss 1.4467 (Cls: 1.0289) LR: 0.000021
Batch 70: Loss 1.5005 (Cls: 1.0836) LR: 0.000021
Batch 80: Loss 1.4937 (Cls: 1.0711) LR: 0.000022
Epoch 6: Loss 1.5039, Acc 0.2564
Val Loss 1.5021, Val Acc 0.4506
New best model saved with Val Acc: 0.4506
Batch 0: Loss 1.4526 (Cls: 1.0389) LR: 0.000022
Batch 10: Loss 1.5837 (Cls: 1.1658) LR: 0.000023
Batch 20: Loss 1.5085 (Cls: 1.0931) LR: 0.000023
Batch 30: Loss 1.5880 (Cls: 1.1763) LR: 0.000024
Batch 40: Loss 1.4772 (Cls: 1.0594) LR: 0.000025
Batch 50: Loss 1.5172 (Cls: 1.0980) LR: 0.000026
Batch 60: Loss 1.5357 (Cls: 1.1243) LR: 0.000026
Batch 70: Loss 1.4734 (Cls: 1.0580) LR: 0.000027
Batch 80: Loss 1.5200 (Cls: 1.1076) LR: 0.000028
Epoch 7: Loss 1.4927, Acc 0.2661
Val Loss 1.6417, Val Acc 0.2226
Batch 0: Loss 1.4853 (Cls: 1.0682) LR: 0.000028
Batch 10: Loss 1.5929 (Cls: 1.1696) LR: 0.000029
Batch 20: Loss 1.5944 (Cls: 1.1695) LR: 0.000030
Batch 30: Loss 1.4645 (Cls: 1.0576) LR: 0.000030
Batch 40: Loss 1.4607 (Cls: 1.0538) LR: 0.000031
Batch 50: Loss 1.4886 (Cls: 1.0726) LR: 0.000032
Batch 60: Loss 1.4479 (Cls: 1.0405) LR: 0.000033
Batch 70: Loss 1.4289 (Cls: 1.0262) LR: 0.000033
Batch 80: Loss 1.5071 (Cls: 1.1058) LR: 0.000034
Epoch 8: Loss 1.4705, Acc 0.2870
Val Loss 1.4615, Val Acc 0.4666
New best model saved with Val Acc: 0.4666
Batch 0: Loss 1.4589 (Cls: 1.0514) LR: 0.000035
Batch 10: Loss 1.4882 (Cls: 1.0760) LR: 0.000035
Batch 20: Loss 1.5624 (Cls: 1.1470) LR: 0.000036
Batch 30: Loss 1.5372 (Cls: 1.1304) LR: 0.000037
Batch 40: Loss 1.4407 (Cls: 1.0441) LR: 0.000038
Batch 50: Loss 1.4018 (Cls: 1.0002) LR: 0.000039
Batch 60: Loss 1.4604 (Cls: 1.0495) LR: 0.000039
Batch 70: Loss 1.4929 (Cls: 1.0831) LR: 0.000040
Batch 80: Loss 1.5003 (Cls: 1.0874) LR: 0.000041
Epoch 9: Loss 1.4554, Acc 0.2980
Val Loss 1.4457, Val Acc 0.4400
Batch 0: Loss 1.4582 (Cls: 1.0426) LR: 0.000041
Batch 10: Loss 1.4226 (Cls: 1.0240) LR: 0.000042
Batch 20: Loss 1.4343 (Cls: 1.0310) LR: 0.000043
Batch 30: Loss 1.3911 (Cls: 0.9922) LR: 0.000044
Batch 40: Loss 1.4272 (Cls: 1.0201) LR: 0.000045
Batch 50: Loss 1.4438 (Cls: 1.0409) LR: 0.000046
Batch 60: Loss 1.3715 (Cls: 0.9680) LR: 0.000046
Batch 70: Loss 1.4738 (Cls: 1.0577) LR: 0.000047
Batch 80: Loss 1.3934 (Cls: 0.9925) LR: 0.000048
Epoch 10: Loss 1.4343, Acc 0.3217
Val Loss 1.4156, Val Acc 0.4924
New best model saved with Val Acc: 0.4924
Batch 0: Loss 1.4610 (Cls: 1.0564) LR: 0.000049
Batch 10: Loss 1.3483 (Cls: 0.9498) LR: 0.000049
Batch 20: Loss 1.3607 (Cls: 0.9632) LR: 0.000050
Batch 30: Loss 1.2790 (Cls: 0.8874) LR: 0.000051
Batch 40: Loss 1.4125 (Cls: 1.0138) LR: 0.000052
Batch 50: Loss 1.4225 (Cls: 1.0189) LR: 0.000053
Batch 60: Loss 1.4169 (Cls: 1.0184) LR: 0.000054
Batch 70: Loss 1.3630 (Cls: 0.9761) LR: 0.000054
Batch 80: Loss 1.4973 (Cls: 1.0937) LR: 0.000055
Epoch 11: Loss 1.4065, Acc 0.3486
Val Loss 1.3595, Val Acc 0.5790
New best model saved with Val Acc: 0.5790
Batch 0: Loss 1.4303 (Cls: 1.0406) LR: 0.000056
Batch 10: Loss 1.3679 (Cls: 0.9654) LR: 0.000057
Batch 20: Loss 1.3499 (Cls: 0.9678) LR: 0.000057
Batch 30: Loss 1.4170 (Cls: 1.0243) LR: 0.000058
Batch 40: Loss 1.4135 (Cls: 1.0164) LR: 0.000059
Batch 50: Loss 1.3112 (Cls: 0.9318) LR: 0.000060
Batch 60: Loss 1.3614 (Cls: 0.9740) LR: 0.000061
Batch 70: Loss 1.5200 (Cls: 1.1052) LR: 0.000062
Batch 80: Loss 1.4429 (Cls: 1.0397) LR: 0.000062
Epoch 12: Loss 1.4111, Acc 0.3531
Val Loss 1.4035, Val Acc 0.5091
Batch 0: Loss 1.3829 (Cls: 0.9909) LR: 0.000063
Batch 10: Loss 1.4810 (Cls: 1.0726) LR: 0.000064
Batch 20: Loss 1.3703 (Cls: 0.9817) LR: 0.000064
Batch 30: Loss 1.3542 (Cls: 0.9544) LR: 0.000065
Batch 40: Loss 1.3963 (Cls: 1.0033) LR: 0.000066
Batch 50: Loss 1.3600 (Cls: 0.9691) LR: 0.000067
Batch 60: Loss 1.4107 (Cls: 1.0178) LR: 0.000068
Batch 70: Loss 1.3569 (Cls: 0.9704) LR: 0.000068
Batch 80: Loss 1.3848 (Cls: 0.9905) LR: 0.000069
Epoch 13: Loss 1.3868, Acc 0.3614
Val Loss 1.4170, Val Acc 0.4688
Batch 0: Loss 1.3062 (Cls: 0.9268) LR: 0.000070
Batch 10: Loss 1.3645 (Cls: 0.9873) LR: 0.000070
Batch 20: Loss 1.3651 (Cls: 0.9829) LR: 0.000071
Batch 30: Loss 1.3792 (Cls: 0.9837) LR: 0.000072
Batch 40: Loss 1.3815 (Cls: 0.9938) LR: 0.000073
Batch 50: Loss 1.3599 (Cls: 0.9724) LR: 0.000074
Batch 60: Loss 1.3689 (Cls: 0.9718) LR: 0.000074
Batch 70: Loss 1.3433 (Cls: 0.9521) LR: 0.000075
Batch 80: Loss 1.3827 (Cls: 0.9951) LR: 0.000076
Epoch 14: Loss 1.3822, Acc 0.3766
Val Loss 1.4063, Val Acc 0.4818
Batch 0: Loss 1.3979 (Cls: 1.0023) LR: 0.000076
Batch 10: Loss 1.2659 (Cls: 0.8823) LR: 0.000077
Batch 20: Loss 1.3153 (Cls: 0.9313) LR: 0.000078
Batch 30: Loss 1.3985 (Cls: 1.0008) LR: 0.000078
Batch 40: Loss 1.4050 (Cls: 1.0197) LR: 0.000079
Batch 50: Loss 1.3682 (Cls: 0.9748) LR: 0.000080
Batch 60: Loss 1.4597 (Cls: 1.0502) LR: 0.000080
Batch 70: Loss 1.4207 (Cls: 1.0204) LR: 0.000081
Batch 80: Loss 1.3109 (Cls: 0.9152) LR: 0.000082
Epoch 15: Loss 1.3736, Acc 0.3777
Val Loss 1.3733, Val Acc 0.5616
Batch 0: Loss 1.3180 (Cls: 0.9404) LR: 0.000082
Batch 10: Loss 1.4177 (Cls: 1.0150) LR: 0.000083
Batch 20: Loss 1.3558 (Cls: 0.9666) LR: 0.000083
Batch 30: Loss 1.3105 (Cls: 0.9342) LR: 0.000084
Batch 40: Loss 1.2734 (Cls: 0.8905) LR: 0.000085
Batch 50: Loss 1.2311 (Cls: 0.8622) LR: 0.000085
Batch 60: Loss 1.3145 (Cls: 0.9349) LR: 0.000086
Batch 70: Loss 1.3732 (Cls: 0.9901) LR: 0.000086
Batch 80: Loss 1.4219 (Cls: 1.0150) LR: 0.000087
Epoch 16: Loss 1.3525, Acc 0.4000
Val Loss 1.3891, Val Acc 0.5327
Batch 0: Loss 1.3849 (Cls: 0.9940) LR: 0.000087
Batch 10: Loss 1.3139 (Cls: 0.9349) LR: 0.000088
Batch 20: Loss 1.3389 (Cls: 0.9527) LR: 0.000088
Batch 30: Loss 1.4343 (Cls: 1.0426) LR: 0.000089
Batch 40: Loss 1.3426 (Cls: 0.9585) LR: 0.000089
Batch 50: Loss 1.3029 (Cls: 0.9298) LR: 0.000090
Batch 60: Loss 1.3851 (Cls: 0.9956) LR: 0.000091
Batch 70: Loss 1.3887 (Cls: 0.9941) LR: 0.000091
Batch 80: Loss 1.3378 (Cls: 0.9503) LR: 0.000092
Epoch 17: Loss 1.3597, Acc 0.3938
Val Loss 1.4040, Val Acc 0.4673
Batch 0: Loss 1.4429 (Cls: 1.0405) LR: 0.000092
Batch 10: Loss 1.3129 (Cls: 0.9214) LR: 0.000092
Batch 20: Loss 1.3281 (Cls: 0.9450) LR: 0.000093
Batch 30: Loss 1.3478 (Cls: 0.9604) LR: 0.000093
Batch 40: Loss 1.3838 (Cls: 1.0064) LR: 0.000094
Batch 50: Loss 1.2983 (Cls: 0.9244) LR: 0.000094
Batch 60: Loss 1.3425 (Cls: 0.9520) LR: 0.000094
Batch 70: Loss 1.3240 (Cls: 0.9411) LR: 0.000095
Batch 80: Loss 1.2740 (Cls: 0.8926) LR: 0.000095
Epoch 18: Loss 1.3594, Acc 0.3829
Val Loss 1.5581, Val Acc 0.4650
Batch 0: Loss 1.5004 (Cls: 1.1065) LR: 0.000095
Batch 10: Loss 1.3752 (Cls: 0.9794) LR: 0.000096
Batch 20: Loss 1.3955 (Cls: 1.0038) LR: 0.000096
Batch 30: Loss 1.3015 (Cls: 0.9155) LR: 0.000096
Batch 40: Loss 1.4001 (Cls: 1.0110) LR: 0.000097
Batch 50: Loss 1.3290 (Cls: 0.9545) LR: 0.000097
Batch 60: Loss 1.3106 (Cls: 0.9263) LR: 0.000097
Batch 70: Loss 1.4049 (Cls: 1.0103) LR: 0.000098
Batch 80: Loss 1.3364 (Cls: 0.9480) LR: 0.000098
Epoch 19: Loss 1.3439, Acc 0.4031
Val Loss 1.5984, Val Acc 0.3944
Batch 0: Loss 1.4257 (Cls: 1.0369) LR: 0.000098
Batch 10: Loss 1.3764 (Cls: 0.9785) LR: 0.000098
Batch 20: Loss 1.2998 (Cls: 0.9208) LR: 0.000098
Batch 30: Loss 1.3857 (Cls: 0.9940) LR: 0.000099
Batch 40: Loss 1.4121 (Cls: 1.0301) LR: 0.000099
Batch 50: Loss 1.3671 (Cls: 0.9772) LR: 0.000099
Batch 60: Loss 1.3387 (Cls: 0.9693) LR: 0.000099
Batch 70: Loss 1.2869 (Cls: 0.9131) LR: 0.000099
Batch 80: Loss 1.3003 (Cls: 0.9188) LR: 0.000099
Epoch 20: Loss 1.3370, Acc 0.4052
Val Loss 1.3531, Val Acc 0.5821
New best model saved with Val Acc: 0.5821
Batch 0: Loss 1.3627 (Cls: 0.9796) LR: 0.000099
Batch 10: Loss 1.4331 (Cls: 1.0372) LR: 0.000100
Batch 20: Loss 1.3483 (Cls: 0.9719) LR: 0.000100
Batch 30: Loss 1.3142 (Cls: 0.9226) LR: 0.000100
Batch 40: Loss 1.3165 (Cls: 0.9371) LR: 0.000100
Batch 50: Loss 1.3181 (Cls: 0.9388) LR: 0.000100
Batch 60: Loss 1.1564 (Cls: 0.8013) LR: 0.000100
Batch 70: Loss 1.2469 (Cls: 0.8712) LR: 0.000100
Batch 80: Loss 1.3064 (Cls: 0.9263) LR: 0.000100
Epoch 21: Loss 1.3196, Acc 0.4274
Val Loss 1.3115, Val Acc 0.5980
New best model saved with Val Acc: 0.5980
Batch 0: Loss 1.2836 (Cls: 0.9160) LR: 0.000100
Batch 10: Loss 1.2811 (Cls: 0.9107) LR: 0.000100
Batch 20: Loss 1.2869 (Cls: 0.9064) LR: 0.000100
Batch 30: Loss 1.2830 (Cls: 0.9131) LR: 0.000100
Batch 40: Loss 1.3078 (Cls: 0.9347) LR: 0.000100
Batch 50: Loss 1.3359 (Cls: 0.9522) LR: 0.000100
Batch 60: Loss 1.4994 (Cls: 1.0864) LR: 0.000100
Batch 70: Loss 1.3659 (Cls: 0.9819) LR: 0.000100
Batch 80: Loss 1.3098 (Cls: 0.9301) LR: 0.000100
Epoch 22: Loss 1.3167, Acc 0.4348
Val Loss 1.3279, Val Acc 0.5600
Batch 0: Loss 1.3890 (Cls: 0.9912) LR: 0.000100
Batch 10: Loss 1.3896 (Cls: 1.0042) LR: 0.000100
Batch 20: Loss 1.3563 (Cls: 0.9694) LR: 0.000100
Batch 30: Loss 1.2385 (Cls: 0.8648) LR: 0.000100
Batch 40: Loss 1.4000 (Cls: 1.0273) LR: 0.000100
Batch 50: Loss 1.3191 (Cls: 0.9416) LR: 0.000100
Batch 60: Loss 1.3342 (Cls: 0.9643) LR: 0.000100
Batch 70: Loss 1.3084 (Cls: 0.9328) LR: 0.000100
Batch 80: Loss 1.4270 (Cls: 1.0437) LR: 0.000100
Epoch 23: Loss 1.3118, Acc 0.4400
Val Loss 1.3236, Val Acc 0.5729
Batch 0: Loss 1.1444 (Cls: 0.7922) LR: 0.000100
Batch 10: Loss 1.3335 (Cls: 0.9536) LR: 0.000100
Batch 20: Loss 1.2851 (Cls: 0.9104) LR: 0.000099
Batch 30: Loss 1.3279 (Cls: 0.9464) LR: 0.000099
Batch 40: Loss 1.2296 (Cls: 0.8584) LR: 0.000099
Batch 50: Loss 1.1633 (Cls: 0.8004) LR: 0.000099
Batch 60: Loss 1.2763 (Cls: 0.9008) LR: 0.000099
Batch 70: Loss 1.1941 (Cls: 0.8317) LR: 0.000099
Batch 80: Loss 1.2602 (Cls: 0.8946) LR: 0.000099
Epoch 24: Loss 1.2981, Acc 0.4515
Val Loss 1.4904, Val Acc 0.4020
Batch 0: Loss 1.4123 (Cls: 1.0088) LR: 0.000099
Batch 10: Loss 1.2808 (Cls: 0.9147) LR: 0.000099
Batch 20: Loss 1.3153 (Cls: 0.9337) LR: 0.000099
Batch 30: Loss 1.4476 (Cls: 1.0483) LR: 0.000099
Batch 40: Loss 1.2731 (Cls: 0.9019) LR: 0.000099
Batch 50: Loss 1.3730 (Cls: 0.9887) LR: 0.000099
Batch 60: Loss 1.2479 (Cls: 0.8864) LR: 0.000099
Batch 70: Loss 1.3157 (Cls: 0.9311) LR: 0.000098
Batch 80: Loss 1.2658 (Cls: 0.8961) LR: 0.000098
Epoch 25: Loss 1.3066, Acc 0.4454
Val Loss 1.3349, Val Acc 0.5828
Batch 0: Loss 1.3219 (Cls: 0.9503) LR: 0.000098
Batch 10: Loss 1.2243 (Cls: 0.8731) LR: 0.000098
Batch 20: Loss 1.2337 (Cls: 0.8759) LR: 0.000098
Batch 30: Loss 1.2425 (Cls: 0.8723) LR: 0.000098
Batch 40: Loss 1.1622 (Cls: 0.8159) LR: 0.000098
Batch 50: Loss 1.3887 (Cls: 1.0130) LR: 0.000098
Batch 60: Loss 1.4206 (Cls: 1.0248) LR: 0.000098
Batch 70: Loss 1.3388 (Cls: 0.9600) LR: 0.000098
Batch 80: Loss 1.2026 (Cls: 0.8449) LR: 0.000097
Epoch 26: Loss 1.2840, Acc 0.4589
Val Loss 1.4120, Val Acc 0.4932
Batch 0: Loss 1.3341 (Cls: 0.9616) LR: 0.000097
Batch 10: Loss 1.2785 (Cls: 0.9091) LR: 0.000097
Batch 20: Loss 1.1650 (Cls: 0.8104) LR: 0.000097
Batch 30: Loss 1.2987 (Cls: 0.9226) LR: 0.000097
Batch 40: Loss 1.3505 (Cls: 0.9767) LR: 0.000097
Batch 50: Loss 1.2808 (Cls: 0.9031) LR: 0.000097
Batch 60: Loss 1.2957 (Cls: 0.9231) LR: 0.000097
Batch 70: Loss 1.2535 (Cls: 0.8925) LR: 0.000097
Batch 80: Loss 1.3953 (Cls: 1.0169) LR: 0.000096
Epoch 27: Loss 1.2908, Acc 0.4634
Val Loss 1.2847, Val Acc 0.6208
New best model saved with Val Acc: 0.6208
Batch 0: Loss 1.3131 (Cls: 0.9349) LR: 0.000096
Batch 10: Loss 1.2324 (Cls: 0.8727) LR: 0.000096
Batch 20: Loss 1.1559 (Cls: 0.8031) LR: 0.000096
Batch 30: Loss 1.2906 (Cls: 0.9262) LR: 0.000096
Batch 40: Loss 1.2641 (Cls: 0.9008) LR: 0.000096
Batch 50: Loss 1.3087 (Cls: 0.9370) LR: 0.000096
Batch 60: Loss 1.2662 (Cls: 0.9035) LR: 0.000095
Batch 70: Loss 1.3062 (Cls: 0.9291) LR: 0.000095
Batch 80: Loss 1.3577 (Cls: 0.9727) LR: 0.000095
Epoch 28: Loss 1.2703, Acc 0.4812
Val Loss 1.2810, Val Acc 0.6018
Batch 0: Loss 1.2122 (Cls: 0.8463) LR: 0.000095
Batch 10: Loss 1.3048 (Cls: 0.9357) LR: 0.000095
Batch 20: Loss 1.2410 (Cls: 0.8804) LR: 0.000095
Batch 30: Loss 1.3073 (Cls: 0.9343) LR: 0.000095
Batch 40: Loss 1.2445 (Cls: 0.8689) LR: 0.000094
Batch 50: Loss 1.2596 (Cls: 0.9020) LR: 0.000094
Batch 60: Loss 1.2019 (Cls: 0.8430) LR: 0.000094
Batch 70: Loss 1.2321 (Cls: 0.8620) LR: 0.000094
Batch 80: Loss 1.2127 (Cls: 0.8566) LR: 0.000094
Epoch 29: Loss 1.2789, Acc 0.4691
Val Loss 1.3331, Val Acc 0.5638
Batch 0: Loss 1.2187 (Cls: 0.8549) LR: 0.000094
Batch 10: Loss 1.2610 (Cls: 0.8755) LR: 0.000093
Batch 20: Loss 1.3232 (Cls: 0.9493) LR: 0.000093
Batch 30: Loss 1.2408 (Cls: 0.8799) LR: 0.000093
Batch 40: Loss 1.2787 (Cls: 0.9002) LR: 0.000093
Batch 50: Loss 1.2474 (Cls: 0.8819) LR: 0.000093
Batch 60: Loss 1.2619 (Cls: 0.8946) LR: 0.000092
Batch 70: Loss 1.2381 (Cls: 0.8829) LR: 0.000092
Batch 80: Loss 1.2680 (Cls: 0.8889) LR: 0.000092
Epoch 30: Loss 1.2737, Acc 0.4623
Val Loss 1.3849, Val Acc 0.5395
Batch 0: Loss 1.2557 (Cls: 0.8915) LR: 0.000092
Batch 10: Loss 1.3542 (Cls: 0.9648) LR: 0.000092
Batch 20: Loss 1.1798 (Cls: 0.8155) LR: 0.000091
Batch 30: Loss 1.3838 (Cls: 0.9865) LR: 0.000091
Batch 40: Loss 1.2557 (Cls: 0.8903) LR: 0.000091
Batch 50: Loss 1.4016 (Cls: 1.0236) LR: 0.000091
Batch 60: Loss 1.2816 (Cls: 0.9265) LR: 0.000091
Batch 70: Loss 1.2478 (Cls: 0.8954) LR: 0.000090
Batch 80: Loss 1.2476 (Cls: 0.8873) LR: 0.000090
Epoch 31: Loss 1.2568, Acc 0.4825
Val Loss 1.2601, Val Acc 0.6155
Batch 0: Loss 1.2695 (Cls: 0.8866) LR: 0.000090
Batch 10: Loss 1.2636 (Cls: 0.8927) LR: 0.000090
Batch 20: Loss 1.3322 (Cls: 0.9569) LR: 0.000090
Batch 30: Loss 1.2497 (Cls: 0.8859) LR: 0.000089
Batch 40: Loss 1.4459 (Cls: 1.0570) LR: 0.000089
Batch 50: Loss 1.2667 (Cls: 0.8948) LR: 0.000089
Batch 60: Loss 1.2702 (Cls: 0.8982) LR: 0.000089
Batch 70: Loss 1.3571 (Cls: 0.9663) LR: 0.000088
Batch 80: Loss 1.2383 (Cls: 0.8752) LR: 0.000088
Epoch 32: Loss 1.2616, Acc 0.4817
Val Loss 1.2606, Val Acc 0.6185
Batch 0: Loss 1.3340 (Cls: 0.9565) LR: 0.000088
Batch 10: Loss 1.3731 (Cls: 1.0039) LR: 0.000088
Batch 20: Loss 1.1970 (Cls: 0.8411) LR: 0.000088
Batch 30: Loss 1.2645 (Cls: 0.8928) LR: 0.000087
Batch 40: Loss 1.2629 (Cls: 0.8871) LR: 0.000087
Batch 50: Loss 1.2804 (Cls: 0.9127) LR: 0.000087
Batch 60: Loss 1.2144 (Cls: 0.8497) LR: 0.000087
Batch 70: Loss 1.2074 (Cls: 0.8376) LR: 0.000086
Batch 80: Loss 1.3008 (Cls: 0.9362) LR: 0.000086
Epoch 33: Loss 1.2667, Acc 0.4877
Val Loss 1.4599, Val Acc 0.4255
Batch 0: Loss 1.2465 (Cls: 0.8792) LR: 0.000086
Batch 10: Loss 1.2317 (Cls: 0.8699) LR: 0.000086
Batch 20: Loss 1.3374 (Cls: 0.9515) LR: 0.000085
Batch 30: Loss 1.3132 (Cls: 0.9439) LR: 0.000085
Batch 40: Loss 1.2235 (Cls: 0.8689) LR: 0.000085
Batch 50: Loss 1.1797 (Cls: 0.8304) LR: 0.000085
Batch 60: Loss 1.1907 (Cls: 0.8335) LR: 0.000084
Batch 70: Loss 1.4642 (Cls: 1.0712) LR: 0.000084
Batch 80: Loss 1.3389 (Cls: 0.9722) LR: 0.000084
Epoch 34: Loss 1.2490, Acc 0.4947
Val Loss 1.3183, Val Acc 0.5714
Batch 0: Loss 1.1777 (Cls: 0.8266) LR: 0.000084
Batch 10: Loss 1.2317 (Cls: 0.8735) LR: 0.000083
Batch 20: Loss 1.3005 (Cls: 0.9393) LR: 0.000083
Batch 30: Loss 1.1969 (Cls: 0.8404) LR: 0.000083
Batch 40: Loss 1.2185 (Cls: 0.8554) LR: 0.000082
Batch 50: Loss 1.2155 (Cls: 0.8627) LR: 0.000082
Batch 60: Loss 1.1725 (Cls: 0.8218) LR: 0.000082
Batch 70: Loss 1.1588 (Cls: 0.8055) LR: 0.000082
Batch 80: Loss 1.2887 (Cls: 0.9111) LR: 0.000081
Epoch 35: Loss 1.2289, Acc 0.5090
Val Loss 1.2812, Val Acc 0.5904
Batch 0: Loss 1.1353 (Cls: 0.7874) LR: 0.000081
Batch 10: Loss 1.1749 (Cls: 0.8111) LR: 0.000081
Batch 20: Loss 1.2520 (Cls: 0.8843) LR: 0.000081
Batch 30: Loss 1.2206 (Cls: 0.8593) LR: 0.000080
Batch 40: Loss 1.1842 (Cls: 0.8295) LR: 0.000080
Batch 50: Loss 1.3686 (Cls: 0.9846) LR: 0.000080
Batch 60: Loss 1.2075 (Cls: 0.8522) LR: 0.000079
Batch 70: Loss 1.2383 (Cls: 0.8808) LR: 0.000079
Batch 80: Loss 1.2840 (Cls: 0.9132) LR: 0.000079
Epoch 36: Loss 1.2284, Acc 0.5337
Val Loss 1.2318, Val Acc 0.6223
New best model saved with Val Acc: 0.6223
Batch 0: Loss 1.2240 (Cls: 0.8611) LR: 0.000079
Batch 10: Loss 1.2867 (Cls: 0.9262) LR: 0.000078
Batch 20: Loss 1.2228 (Cls: 0.8624) LR: 0.000078
Batch 30: Loss 1.2015 (Cls: 0.8352) LR: 0.000078
Batch 40: Loss 1.1880 (Cls: 0.8336) LR: 0.000077
Batch 50: Loss 1.1454 (Cls: 0.7968) LR: 0.000077
Batch 60: Loss 1.3288 (Cls: 0.9555) LR: 0.000077
Batch 70: Loss 1.1972 (Cls: 0.8484) LR: 0.000076
Batch 80: Loss 1.1652 (Cls: 0.8116) LR: 0.000076
Epoch 37: Loss 1.2353, Acc 0.5047
Val Loss 1.2615, Val Acc 0.5866
Batch 0: Loss 1.2824 (Cls: 0.9182) LR: 0.000076
Batch 10: Loss 1.2164 (Cls: 0.8536) LR: 0.000076
Batch 20: Loss 1.3125 (Cls: 0.9488) LR: 0.000075
Batch 30: Loss 1.1920 (Cls: 0.8319) LR: 0.000075
Batch 40: Loss 1.2577 (Cls: 0.8994) LR: 0.000075
Batch 50: Loss 1.2259 (Cls: 0.8663) LR: 0.000074
Batch 60: Loss 1.1873 (Cls: 0.8389) LR: 0.000074
Batch 70: Loss 1.2279 (Cls: 0.8660) LR: 0.000074
Batch 80: Loss 1.2284 (Cls: 0.8683) LR: 0.000073
Epoch 38: Loss 1.2183, Acc 0.5160
Val Loss 1.2624, Val Acc 0.6064
Batch 0: Loss 1.2645 (Cls: 0.8915) LR: 0.000073
Batch 10: Loss 1.1618 (Cls: 0.8130) LR: 0.000073
Batch 20: Loss 1.0357 (Cls: 0.7149) LR: 0.000072
Batch 30: Loss 1.2380 (Cls: 0.8789) LR: 0.000072
Batch 40: Loss 1.2811 (Cls: 0.8973) LR: 0.000072
Batch 50: Loss 1.3541 (Cls: 0.9723) LR: 0.000071
Batch 60: Loss 1.1186 (Cls: 0.7798) LR: 0.000071
Batch 70: Loss 1.2056 (Cls: 0.8500) LR: 0.000071
Batch 80: Loss 1.2954 (Cls: 0.9357) LR: 0.000070
Epoch 39: Loss 1.2163, Acc 0.5248
Val Loss 1.3051, Val Acc 0.5699
Batch 0: Loss 1.2712 (Cls: 0.9021) LR: 0.000070
Batch 10: Loss 1.2796 (Cls: 0.9131) LR: 0.000070
Batch 20: Loss 1.2000 (Cls: 0.8505) LR: 0.000069
Batch 30: Loss 1.2218 (Cls: 0.8667) LR: 0.000069
Batch 40: Loss 1.3293 (Cls: 0.9679) LR: 0.000069
Batch 50: Loss 1.2016 (Cls: 0.8433) LR: 0.000068
Batch 60: Loss 1.2128 (Cls: 0.8604) LR: 0.000068
Batch 70: Loss 1.0420 (Cls: 0.7052) LR: 0.000068
Batch 80: Loss 1.1559 (Cls: 0.8054) LR: 0.000067
Epoch 40: Loss 1.1951, Acc 0.5411
Val Loss 1.3442, Val Acc 0.5327
Batch 0: Loss 1.1992 (Cls: 0.8454) LR: 0.000067
Batch 10: Loss 1.3323 (Cls: 0.9535) LR: 0.000067
Batch 20: Loss 1.2990 (Cls: 0.9349) LR: 0.000066
Batch 30: Loss 1.3169 (Cls: 0.9436) LR: 0.000066
Batch 40: Loss 1.1980 (Cls: 0.8500) LR: 0.000066
Batch 50: Loss 1.1478 (Cls: 0.8008) LR: 0.000065
Batch 60: Loss 1.1928 (Cls: 0.8394) LR: 0.000065
Batch 70: Loss 1.1547 (Cls: 0.8043) LR: 0.000065
Batch 80: Loss 1.1819 (Cls: 0.8264) LR: 0.000064
Epoch 41: Loss 1.2003, Acc 0.5355
Val Loss 1.2653, Val Acc 0.6178
Batch 0: Loss 1.2347 (Cls: 0.8796) LR: 0.000064
Batch 10: Loss 1.1619 (Cls: 0.8101) LR: 0.000064
Batch 20: Loss 1.1744 (Cls: 0.8193) LR: 0.000063
Batch 30: Loss 1.3079 (Cls: 0.9309) LR: 0.000063
Batch 40: Loss 1.1382 (Cls: 0.7948) LR: 0.000063
Batch 50: Loss 1.2112 (Cls: 0.8545) LR: 0.000062
Batch 60: Loss 1.1461 (Cls: 0.7977) LR: 0.000062
Batch 70: Loss 1.2357 (Cls: 0.8810) LR: 0.000062
Batch 80: Loss 1.1967 (Cls: 0.8425) LR: 0.000061
Epoch 42: Loss 1.1795, Acc 0.5552
Val Loss 1.2294, Val Acc 0.6360
New best model saved with Val Acc: 0.6360
Batch 0: Loss 1.2023 (Cls: 0.8439) LR: 0.000061
Batch 10: Loss 1.1510 (Cls: 0.8025) LR: 0.000061
Batch 20: Loss 1.2451 (Cls: 0.8900) LR: 0.000060
Batch 30: Loss 1.1118 (Cls: 0.7649) LR: 0.000060
Batch 40: Loss 1.1932 (Cls: 0.8358) LR: 0.000060
Batch 50: Loss 1.2357 (Cls: 0.8833) LR: 0.000059
Batch 60: Loss 1.1939 (Cls: 0.8414) LR: 0.000059
Batch 70: Loss 1.1278 (Cls: 0.7892) LR: 0.000058
Batch 80: Loss 1.1588 (Cls: 0.8154) LR: 0.000058
Epoch 43: Loss 1.1877, Acc 0.5513
Val Loss 1.3435, Val Acc 0.5068
Batch 0: Loss 1.2495 (Cls: 0.8830) LR: 0.000058
Batch 10: Loss 1.2556 (Cls: 0.8942) LR: 0.000058
Batch 20: Loss 1.2050 (Cls: 0.8453) LR: 0.000057
Batch 30: Loss 1.1505 (Cls: 0.8103) LR: 0.000057
Batch 40: Loss 1.2069 (Cls: 0.8488) LR: 0.000056
Batch 50: Loss 1.1501 (Cls: 0.8023) LR: 0.000056
Batch 60: Loss 1.1481 (Cls: 0.8034) LR: 0.000056
Batch 70: Loss 1.2366 (Cls: 0.8759) LR: 0.000055
Batch 80: Loss 1.1609 (Cls: 0.8033) LR: 0.000055
Epoch 44: Loss 1.1741, Acc 0.5613
Val Loss 1.3654, Val Acc 0.4992
Batch 0: Loss 1.1906 (Cls: 0.8430) LR: 0.000055
Batch 10: Loss 1.1777 (Cls: 0.8246) LR: 0.000054
Batch 20: Loss 1.0870 (Cls: 0.7490) LR: 0.000054
Batch 30: Loss 1.0974 (Cls: 0.7584) LR: 0.000054
Batch 40: Loss 1.1952 (Cls: 0.8413) LR: 0.000053
Batch 50: Loss 1.1569 (Cls: 0.8051) LR: 0.000053
Batch 60: Loss 1.1230 (Cls: 0.7851) LR: 0.000052
Batch 70: Loss 1.1427 (Cls: 0.7938) LR: 0.000052
Batch 80: Loss 1.1264 (Cls: 0.7740) LR: 0.000052
Epoch 45: Loss 1.1547, Acc 0.5865
Val Loss 1.3043, Val Acc 0.5935
Batch 0: Loss 1.2405 (Cls: 0.8865) LR: 0.000052
Batch 10: Loss 1.1748 (Cls: 0.8193) LR: 0.000051
Batch 20: Loss 1.1355 (Cls: 0.7953) LR: 0.000051
Batch 30: Loss 1.1882 (Cls: 0.8449) LR: 0.000050
Batch 40: Loss 1.0788 (Cls: 0.7267) LR: 0.000050
Batch 50: Loss 1.3389 (Cls: 0.9753) LR: 0.000050
Batch 60: Loss 1.2920 (Cls: 0.9251) LR: 0.000049
Batch 70: Loss 1.1115 (Cls: 0.7718) LR: 0.000049
Batch 80: Loss 1.1229 (Cls: 0.7830) LR: 0.000049
Epoch 46: Loss 1.1719, Acc 0.5681
Val Loss 1.3158, Val Acc 0.6049
Batch 0: Loss 1.2208 (Cls: 0.8664) LR: 0.000048
Batch 10: Loss 1.1537 (Cls: 0.8079) LR: 0.000048
Batch 20: Loss 1.1446 (Cls: 0.8010) LR: 0.000048
Batch 30: Loss 1.2473 (Cls: 0.8901) LR: 0.000047
Batch 40: Loss 1.1974 (Cls: 0.8477) LR: 0.000047
Batch 50: Loss 1.1958 (Cls: 0.8471) LR: 0.000046
Batch 60: Loss 1.1486 (Cls: 0.7966) LR: 0.000046
Batch 70: Loss 1.1848 (Cls: 0.8264) LR: 0.000046
Batch 80: Loss 1.1006 (Cls: 0.7532) LR: 0.000045
Epoch 47: Loss 1.1530, Acc 0.5800
Val Loss 1.2966, Val Acc 0.5828
Batch 0: Loss 1.1419 (Cls: 0.7952) LR: 0.000045
Batch 10: Loss 1.2510 (Cls: 0.8920) LR: 0.000045
Batch 20: Loss 1.1568 (Cls: 0.7971) LR: 0.000044
Batch 30: Loss 1.0964 (Cls: 0.7598) LR: 0.000044
Batch 40: Loss 1.0503 (Cls: 0.7255) LR: 0.000044
Batch 50: Loss 1.2404 (Cls: 0.8754) LR: 0.000043
Batch 60: Loss 1.0785 (Cls: 0.7502) LR: 0.000043
Batch 70: Loss 1.1033 (Cls: 0.7704) LR: 0.000043
Batch 80: Loss 1.2297 (Cls: 0.8665) LR: 0.000042
Epoch 48: Loss 1.1527, Acc 0.5796
Val Loss 1.3754, Val Acc 0.5433
Batch 0: Loss 1.1865 (Cls: 0.8313) LR: 0.000042
Batch 10: Loss 1.1963 (Cls: 0.8447) LR: 0.000042
Batch 20: Loss 1.1444 (Cls: 0.8092) LR: 0.000041
Batch 30: Loss 1.0905 (Cls: 0.7573) LR: 0.000041
Batch 40: Loss 1.0925 (Cls: 0.7573) LR: 0.000040
Batch 50: Loss 1.0290 (Cls: 0.7019) LR: 0.000040
Batch 60: Loss 1.1576 (Cls: 0.8148) LR: 0.000040
Batch 70: Loss 1.1802 (Cls: 0.8276) LR: 0.000039
Batch 80: Loss 1.2576 (Cls: 0.9012) LR: 0.000039
Epoch 49: Loss 1.1352, Acc 0.6069
Val Loss 1.2952, Val Acc 0.5790
Batch 0: Loss 1.1775 (Cls: 0.8236) LR: 0.000039
Batch 10: Loss 1.0729 (Cls: 0.7429) LR: 0.000038
Batch 20: Loss 1.0679 (Cls: 0.7307) LR: 0.000038
Batch 30: Loss 1.1264 (Cls: 0.7865) LR: 0.000038
Batch 40: Loss 1.1029 (Cls: 0.7586) LR: 0.000037
Batch 50: Loss 1.1707 (Cls: 0.8150) LR: 0.000037
Batch 60: Loss 1.0865 (Cls: 0.7491) LR: 0.000037
Batch 70: Loss 1.2093 (Cls: 0.8558) LR: 0.000036
Batch 80: Loss 1.1631 (Cls: 0.8235) LR: 0.000036
Epoch 50: Loss 1.1141, Acc 0.6030
Val Loss 1.2252, Val Acc 0.6611
New best model saved with Val Acc: 0.6611
Batch 0: Loss 1.0850 (Cls: 0.7602) LR: 0.000036
Batch 10: Loss 1.1861 (Cls: 0.8331) LR: 0.000035
Batch 20: Loss 1.0936 (Cls: 0.7666) LR: 0.000035
Batch 30: Loss 1.0623 (Cls: 0.7322) LR: 0.000035
Batch 40: Loss 1.1937 (Cls: 0.8433) LR: 0.000034
Batch 50: Loss 1.0993 (Cls: 0.7514) LR: 0.000034
Batch 60: Loss 1.0492 (Cls: 0.7150) LR: 0.000034
Batch 70: Loss 1.2063 (Cls: 0.8555) LR: 0.000033
Batch 80: Loss 1.0684 (Cls: 0.7289) LR: 0.000033
Epoch 51: Loss 1.1241, Acc 0.6011
Val Loss 1.2671, Val Acc 0.6185
Batch 0: Loss 1.1659 (Cls: 0.8258) LR: 0.000033
Batch 10: Loss 1.1288 (Cls: 0.7720) LR: 0.000032
Batch 20: Loss 1.0772 (Cls: 0.7487) LR: 0.000032
Batch 30: Loss 1.0722 (Cls: 0.7443) LR: 0.000032
Batch 40: Loss 1.2085 (Cls: 0.8491) LR: 0.000031
Batch 50: Loss 1.0393 (Cls: 0.7128) LR: 0.000031
Batch 60: Loss 1.1424 (Cls: 0.8019) LR: 0.000031
Batch 70: Loss 1.0659 (Cls: 0.7368) LR: 0.000030
Batch 80: Loss 1.1777 (Cls: 0.8261) LR: 0.000030
Epoch 52: Loss 1.1107, Acc 0.6134
Val Loss 1.2929, Val Acc 0.5775
Batch 0: Loss 1.2001 (Cls: 0.8518) LR: 0.000030
Batch 10: Loss 1.0393 (Cls: 0.7084) LR: 0.000029
Batch 20: Loss 1.0201 (Cls: 0.6911) LR: 0.000029
Batch 30: Loss 1.0855 (Cls: 0.7495) LR: 0.000029
Batch 40: Loss 1.0528 (Cls: 0.7258) LR: 0.000028
Batch 50: Loss 1.0482 (Cls: 0.7038) LR: 0.000028
Batch 60: Loss 1.1587 (Cls: 0.8065) LR: 0.000028
Batch 70: Loss 1.0399 (Cls: 0.7122) LR: 0.000027
Batch 80: Loss 1.1592 (Cls: 0.8191) LR: 0.000027
Epoch 53: Loss 1.1060, Acc 0.6128
Val Loss 1.2299, Val Acc 0.6330
Batch 0: Loss 0.9963 (Cls: 0.6812) LR: 0.000027
Batch 10: Loss 1.1150 (Cls: 0.7874) LR: 0.000026
Batch 20: Loss 1.0138 (Cls: 0.6962) LR: 0.000026
Batch 30: Loss 1.1457 (Cls: 0.8058) LR: 0.000026
Batch 40: Loss 1.1725 (Cls: 0.8223) LR: 0.000025
Batch 50: Loss 1.2199 (Cls: 0.8750) LR: 0.000025
Batch 60: Loss 1.1481 (Cls: 0.8067) LR: 0.000025
Batch 70: Loss 1.1231 (Cls: 0.7850) LR: 0.000025
Batch 80: Loss 1.0935 (Cls: 0.7533) LR: 0.000024
Epoch 54: Loss 1.0996, Acc 0.6175
Val Loss 1.3045, Val Acc 0.5737
Batch 0: Loss 1.1171 (Cls: 0.7840) LR: 0.000024
Batch 10: Loss 1.0708 (Cls: 0.7374) LR: 0.000024
Batch 20: Loss 1.1451 (Cls: 0.7996) LR: 0.000023
Batch 30: Loss 1.0929 (Cls: 0.7509) LR: 0.000023
Batch 40: Loss 1.0158 (Cls: 0.6961) LR: 0.000023
Batch 50: Loss 1.1181 (Cls: 0.7836) LR: 0.000022
Batch 60: Loss 1.1553 (Cls: 0.8012) LR: 0.000022
Batch 70: Loss 1.3109 (Cls: 0.9422) LR: 0.000022
Batch 80: Loss 1.0893 (Cls: 0.7504) LR: 0.000021
Epoch 55: Loss 1.1056, Acc 0.6195
Val Loss 1.2748, Val Acc 0.6011
Batch 0: Loss 1.0689 (Cls: 0.7357) LR: 0.000021
Batch 10: Loss 1.0742 (Cls: 0.7472) LR: 0.000021
Batch 20: Loss 1.1158 (Cls: 0.7788) LR: 0.000021
Batch 30: Loss 1.0080 (Cls: 0.6793) LR: 0.000020
Batch 40: Loss 1.0569 (Cls: 0.7322) LR: 0.000020
Batch 50: Loss 1.0614 (Cls: 0.7226) LR: 0.000020
Batch 60: Loss 1.0739 (Cls: 0.7505) LR: 0.000020
Batch 70: Loss 1.1157 (Cls: 0.7891) LR: 0.000019
Batch 80: Loss 1.0978 (Cls: 0.7646) LR: 0.000019
Epoch 56: Loss 1.0890, Acc 0.6334
Val Loss 1.2562, Val Acc 0.6223
Batch 0: Loss 1.1194 (Cls: 0.7696) LR: 0.000019
Batch 10: Loss 1.1228 (Cls: 0.7796) LR: 0.000018
Batch 20: Loss 1.1084 (Cls: 0.7754) LR: 0.000018
Batch 30: Loss 1.1637 (Cls: 0.8290) LR: 0.000018
Batch 40: Loss 1.1181 (Cls: 0.7832) LR: 0.000018
Batch 50: Loss 1.0577 (Cls: 0.7272) LR: 0.000017
Batch 60: Loss 1.1521 (Cls: 0.8162) LR: 0.000017
Batch 70: Loss 1.0584 (Cls: 0.7301) LR: 0.000017
Batch 80: Loss 1.1321 (Cls: 0.7867) LR: 0.000016
Epoch 57: Loss 1.0767, Acc 0.6364
Val Loss 1.2295, Val Acc 0.6398
Batch 0: Loss 0.9873 (Cls: 0.6697) LR: 0.000016
Batch 10: Loss 1.1575 (Cls: 0.8141) LR: 0.000016
Batch 20: Loss 1.1178 (Cls: 0.7820) LR: 0.000016
Batch 30: Loss 1.0254 (Cls: 0.7032) LR: 0.000016
Batch 40: Loss 1.1221 (Cls: 0.7811) LR: 0.000015
Batch 50: Loss 1.1789 (Cls: 0.8300) LR: 0.000015
Batch 60: Loss 1.0824 (Cls: 0.7543) LR: 0.000015
Batch 70: Loss 1.0608 (Cls: 0.7291) LR: 0.000014
Batch 80: Loss 0.9874 (Cls: 0.6641) LR: 0.000014
Epoch 58: Loss 1.0833, Acc 0.6323
Val Loss 1.2482, Val Acc 0.6140
Batch 0: Loss 1.0599 (Cls: 0.7340) LR: 0.000014
Batch 10: Loss 1.0329 (Cls: 0.7087) LR: 0.000014
Batch 20: Loss 1.0967 (Cls: 0.7630) LR: 0.000014
Batch 30: Loss 1.0423 (Cls: 0.7187) LR: 0.000013
Batch 40: Loss 0.9804 (Cls: 0.6561) LR: 0.000013
Batch 50: Loss 1.1160 (Cls: 0.7843) LR: 0.000013
Batch 60: Loss 1.0575 (Cls: 0.7253) LR: 0.000012
Batch 70: Loss 1.0114 (Cls: 0.6877) LR: 0.000012
Batch 80: Loss 0.9272 (Cls: 0.6017) LR: 0.000012
Epoch 59: Loss 1.0765, Acc 0.6495
Val Loss 1.2152, Val Acc 0.6596
Batch 0: Loss 1.2008 (Cls: 0.8493) LR: 0.000012
Batch 10: Loss 1.0791 (Cls: 0.7375) LR: 0.000012
Batch 20: Loss 1.0166 (Cls: 0.6923) LR: 0.000011
Batch 30: Loss 1.1676 (Cls: 0.8257) LR: 0.000011
Batch 40: Loss 1.0643 (Cls: 0.7353) LR: 0.000011
Batch 50: Loss 1.0755 (Cls: 0.7500) LR: 0.000011
Batch 60: Loss 1.1160 (Cls: 0.7741) LR: 0.000010
Batch 70: Loss 1.0759 (Cls: 0.7444) LR: 0.000010
Batch 80: Loss 1.2192 (Cls: 0.8849) LR: 0.000010
Epoch 60: Loss 1.0768, Acc 0.6371
Val Loss 1.2685, Val Acc 0.6033
Batch 0: Loss 1.1870 (Cls: 0.8346) LR: 0.000010
Batch 10: Loss 1.0252 (Cls: 0.7066) LR: 0.000010
Batch 20: Loss 1.0501 (Cls: 0.7062) LR: 0.000009
Batch 30: Loss 1.0895 (Cls: 0.7636) LR: 0.000009
Batch 40: Loss 1.1189 (Cls: 0.7807) LR: 0.000009
Batch 50: Loss 1.0064 (Cls: 0.6735) LR: 0.000009
Batch 60: Loss 1.0070 (Cls: 0.6849) LR: 0.000009
Batch 70: Loss 0.9623 (Cls: 0.6455) LR: 0.000008
Batch 80: Loss 1.0961 (Cls: 0.7474) LR: 0.000008
Epoch 61: Loss 1.0718, Acc 0.6449
Val Loss 1.2230, Val Acc 0.6413
Batch 0: Loss 1.0883 (Cls: 0.7514) LR: 0.000008
Batch 10: Loss 1.0230 (Cls: 0.7011) LR: 0.000008
Batch 20: Loss 1.0541 (Cls: 0.7251) LR: 0.000008
Batch 30: Loss 1.0091 (Cls: 0.6863) LR: 0.000007
Batch 40: Loss 0.9087 (Cls: 0.6095) LR: 0.000007
Batch 50: Loss 1.0561 (Cls: 0.7339) LR: 0.000007
Batch 60: Loss 1.0331 (Cls: 0.7074) LR: 0.000007
Batch 70: Loss 1.1512 (Cls: 0.8134) LR: 0.000007
Batch 80: Loss 0.9624 (Cls: 0.6424) LR: 0.000006
Epoch 62: Loss 1.0519, Acc 0.6562
Val Loss 1.2335, Val Acc 0.6330
Batch 0: Loss 1.1301 (Cls: 0.7994) LR: 0.000006
Batch 10: Loss 1.0754 (Cls: 0.7400) LR: 0.000006
Batch 20: Loss 1.0071 (Cls: 0.6801) LR: 0.000006
Batch 30: Loss 1.2232 (Cls: 0.8643) LR: 0.000006
Batch 40: Loss 1.0902 (Cls: 0.7474) LR: 0.000006
Batch 50: Loss 1.0869 (Cls: 0.7628) LR: 0.000006
Batch 60: Loss 1.0225 (Cls: 0.6964) LR: 0.000005
Batch 70: Loss 1.0532 (Cls: 0.7298) LR: 0.000005
Batch 80: Loss 1.0068 (Cls: 0.6821) LR: 0.000005
Epoch 63: Loss 1.0681, Acc 0.6495
Val Loss 1.2400, Val Acc 0.6330
Batch 0: Loss 1.0683 (Cls: 0.7348) LR: 0.000005
Batch 10: Loss 1.1319 (Cls: 0.7948) LR: 0.000005
Batch 20: Loss 0.9756 (Cls: 0.6602) LR: 0.000005
Batch 30: Loss 1.1508 (Cls: 0.8049) LR: 0.000004
Batch 40: Loss 1.0563 (Cls: 0.7171) LR: 0.000004
Batch 50: Loss 0.9621 (Cls: 0.6484) LR: 0.000004
Batch 60: Loss 0.9798 (Cls: 0.6533) LR: 0.000004
Batch 70: Loss 1.0146 (Cls: 0.6959) LR: 0.000004
Batch 80: Loss 1.1251 (Cls: 0.7846) LR: 0.000004
Epoch 64: Loss 1.0632, Acc 0.6510
Val Loss 1.2526, Val Acc 0.6170
Batch 0: Loss 1.0607 (Cls: 0.7242) LR: 0.000004
Batch 10: Loss 1.0407 (Cls: 0.7157) LR: 0.000003
Batch 20: Loss 1.0721 (Cls: 0.7361) LR: 0.000003
Batch 30: Loss 1.0482 (Cls: 0.7273) LR: 0.000003
Batch 40: Loss 1.0607 (Cls: 0.7354) LR: 0.000003
Batch 50: Loss 0.9946 (Cls: 0.6750) LR: 0.000003
Batch 60: Loss 1.1353 (Cls: 0.7833) LR: 0.000003
Batch 70: Loss 1.1524 (Cls: 0.8109) LR: 0.000003
Batch 80: Loss 1.0099 (Cls: 0.6960) LR: 0.000003
Epoch 65: Loss 1.0566, Acc 0.6542
Val Loss 1.2389, Val Acc 0.6223
Batch 0: Loss 1.0964 (Cls: 0.7613) LR: 0.000003
Batch 10: Loss 0.9391 (Cls: 0.6236) LR: 0.000002
Batch 20: Loss 0.9818 (Cls: 0.6643) LR: 0.000002
Batch 30: Loss 0.9638 (Cls: 0.6473) LR: 0.000002
Batch 40: Loss 1.0444 (Cls: 0.7129) LR: 0.000002
Batch 50: Loss 1.0574 (Cls: 0.7373) LR: 0.000002
Batch 60: Loss 1.0700 (Cls: 0.7362) LR: 0.000002
Batch 70: Loss 1.1460 (Cls: 0.8047) LR: 0.000002
Batch 80: Loss 1.2070 (Cls: 0.8511) LR: 0.000002
Epoch 66: Loss 1.0550, Acc 0.6571
Val Loss 1.2413, Val Acc 0.6216
Batch 0: Loss 0.9879 (Cls: 0.6692) LR: 0.000002
Batch 10: Loss 0.9975 (Cls: 0.6823) LR: 0.000002
Batch 20: Loss 0.9336 (Cls: 0.6303) LR: 0.000001
Batch 30: Loss 1.0803 (Cls: 0.7473) LR: 0.000001
Batch 40: Loss 1.0397 (Cls: 0.7258) LR: 0.000001
Batch 50: Loss 1.0356 (Cls: 0.7101) LR: 0.000001
Batch 60: Loss 1.1053 (Cls: 0.7640) LR: 0.000001
Batch 70: Loss 1.0543 (Cls: 0.7343) LR: 0.000001
Batch 80: Loss 1.0726 (Cls: 0.7499) LR: 0.000001
Epoch 67: Loss 1.0585, Acc 0.6512
Val Loss 1.2472, Val Acc 0.6185
Batch 0: Loss 1.0431 (Cls: 0.7135) LR: 0.000001
Batch 10: Loss 0.9603 (Cls: 0.6353) LR: 0.000001
Batch 20: Loss 1.0729 (Cls: 0.7475) LR: 0.000001
Batch 30: Loss 1.1536 (Cls: 0.7963) LR: 0.000001
Batch 40: Loss 1.1142 (Cls: 0.7786) LR: 0.000001
Batch 50: Loss 1.1142 (Cls: 0.7760) LR: 0.000001
Batch 60: Loss 1.0171 (Cls: 0.6829) LR: 0.000001
Batch 70: Loss 1.1023 (Cls: 0.7675) LR: 0.000000
Batch 80: Loss 1.0272 (Cls: 0.7112) LR: 0.000000
Epoch 68: Loss 1.0392, Acc 0.6692
Val Loss 1.2465, Val Acc 0.6208
Batch 0: Loss 1.0301 (Cls: 0.7055) LR: 0.000000
Batch 10: Loss 0.9704 (Cls: 0.6643) LR: 0.000000
Batch 20: Loss 1.0567 (Cls: 0.7241) LR: 0.000000
Batch 30: Loss 1.1492 (Cls: 0.8104) LR: 0.000000
Batch 40: Loss 1.0422 (Cls: 0.7162) LR: 0.000000
Batch 50: Loss 1.0242 (Cls: 0.6932) LR: 0.000000
Batch 60: Loss 0.9899 (Cls: 0.6825) LR: 0.000000
Batch 70: Loss 1.0866 (Cls: 0.7499) LR: 0.000000
Batch 80: Loss 1.0742 (Cls: 0.7450) LR: 0.000000
Epoch 69: Loss 1.0452, Acc 0.6618
Val Loss 1.2414, Val Acc 0.6246
Batch 0: Loss 1.0957 (Cls: 0.7608) LR: 0.000000
Batch 10: Loss 0.9658 (Cls: 0.6523) LR: 0.000000
Batch 20: Loss 1.0426 (Cls: 0.7167) LR: 0.000000
Batch 30: Loss 1.0551 (Cls: 0.7338) LR: 0.000000
Batch 40: Loss 1.0275 (Cls: 0.7158) LR: 0.000000
Batch 50: Loss 1.0431 (Cls: 0.7129) LR: 0.000000
Batch 60: Loss 1.0024 (Cls: 0.6894) LR: 0.000000
Batch 70: Loss 1.0165 (Cls: 0.6927) LR: 0.000000
Batch 80: Loss 1.0331 (Cls: 0.7051) LR: 0.000000
Epoch 70: Loss 1.0351, Acc 0.6662
Val Loss 1.2418, Val Acc 0.6246
Running Test Evaluation...
/root/shared-nvme/fire/src/training/train.py:349: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Loaded best model for testing.
Test Loss: 1.3377, Test Acc: 0.5882
