Loaded config: {'data': {'batch_size': 32, 'image_size': 224, 'num_workers': 4, 'path': 'data/', 'seq_len': 50, 'vocab_size': 5000}, 'model': {'backbone': 'vit_base_patch16_224', 'latent_dim': 512, 'method_option': 'vae', 'num_classes': 5}, 'project': {'name': 'FireDamageClassification', 'version': '1.0'}, 'training': {'drop_path_rate': 0.1, 'dropout': 0.2, 'epochs': 50, 'lambda_align': 0.1, 'lambda_cls': 1.0, 'lambda_vae': 0.1, 'lr': '1e-4', 'weight_decay': '1e-3'}}
Results will be saved to: results/2_method_a_vae
Using device: cuda
Model Type: method
Splitting by Region:
Train Regions: ['GLA', 'VAL', 'MOS', 'CAS', 'MIL', 'POST', 'CRE', 'DIN', 'MON', 'SCU', 'FOR', 'BEU']
Val Regions: ['BEA', 'AUG', 'ZOG']
Test Regions: ['CAL', 'FAI', 'DIX']
Scanning images in data/image for mode train...
Loaded 5393 samples for mode train
Scanning images in data/image for mode val...
Loaded 1316 samples for mode val
Scanning images in data/image for mode test...
Loaded 1122 samples for mode test
Calculating class weights for WeightedRandomSampler...
Train Class Counts: {1: 204, 0: 2281, 4: 766, 3: 2060, 2: 82}
Data loaded: Train=5393, Val=1316, Test=1122
Parameters: 218,573,626
FLOPs: 37610572288.0
Starting training loop...
Batch 0: Loss 3.1893 (Cls: 1.2579) LR: 0.000004
Batch 10: Loss 2.8298 (Cls: 1.0384) LR: 0.000004
Batch 20: Loss 2.9309 (Cls: 1.2956) LR: 0.000004
Batch 30: Loss 2.7158 (Cls: 1.2119) LR: 0.000004
Batch 40: Loss 2.5760 (Cls: 1.1668) LR: 0.000004
Batch 50: Loss 2.3853 (Cls: 1.0994) LR: 0.000004
Batch 60: Loss 2.3579 (Cls: 1.1860) LR: 0.000004
Batch 70: Loss 2.2067 (Cls: 1.1110) LR: 0.000004
Batch 80: Loss 2.1348 (Cls: 1.1316) LR: 0.000004
Batch 90: Loss 2.0822 (Cls: 1.1553) LR: 0.000004
Batch 100: Loss 2.0928 (Cls: 1.2365) LR: 0.000004
Batch 110: Loss 1.8637 (Cls: 1.0842) LR: 0.000004
Batch 120: Loss 1.7841 (Cls: 1.0795) LR: 0.000005
Batch 130: Loss 1.8610 (Cls: 1.2175) LR: 0.000005
Batch 140: Loss 1.6833 (Cls: 1.1057) LR: 0.000005
Batch 150: Loss 1.6226 (Cls: 1.0728) LR: 0.000005
Batch 160: Loss 1.6459 (Cls: 1.1477) LR: 0.000005
Epoch 1: Loss 2.1993, Acc 0.2093
Val Loss 1.6157, Val Acc 0.1998
New best model saved with Val Acc: 0.1998
Batch 0: Loss 1.7198 (Cls: 1.2528) LR: 0.000005
Batch 10: Loss 1.7081 (Cls: 1.2647) LR: 0.000005
Batch 20: Loss 1.5058 (Cls: 1.0860) LR: 0.000005
Batch 30: Loss 1.6095 (Cls: 1.2117) LR: 0.000005
Batch 40: Loss 1.4287 (Cls: 1.0432) LR: 0.000006
Batch 50: Loss 1.5965 (Cls: 1.2227) LR: 0.000006
Batch 60: Loss 1.4459 (Cls: 1.0952) LR: 0.000006
Batch 70: Loss 1.5174 (Cls: 1.1667) LR: 0.000006
Batch 80: Loss 1.4865 (Cls: 1.1489) LR: 0.000006
Batch 90: Loss 1.4043 (Cls: 1.0901) LR: 0.000006
Batch 100: Loss 1.4448 (Cls: 1.1348) LR: 0.000007
Batch 110: Loss 1.4830 (Cls: 1.1777) LR: 0.000007
Batch 120: Loss 1.4386 (Cls: 1.1357) LR: 0.000007
Batch 130: Loss 1.4533 (Cls: 1.1532) LR: 0.000007
Batch 140: Loss 1.4110 (Cls: 1.1204) LR: 0.000008
Batch 150: Loss 1.4597 (Cls: 1.1646) LR: 0.000008
Batch 160: Loss 1.4324 (Cls: 1.1370) LR: 0.000008
Epoch 2: Loss 1.4615, Acc 0.2220
Val Loss 1.4218, Val Acc 0.2622
New best model saved with Val Acc: 0.2622
Batch 0: Loss 1.4200 (Cls: 1.1484) LR: 0.000008
Batch 10: Loss 1.4888 (Cls: 1.2057) LR: 0.000008
Batch 20: Loss 1.3711 (Cls: 1.0995) LR: 0.000009
Batch 30: Loss 1.4167 (Cls: 1.1419) LR: 0.000009
Batch 40: Loss 1.4630 (Cls: 1.1883) LR: 0.000009
Batch 50: Loss 1.4209 (Cls: 1.1472) LR: 0.000009
Batch 60: Loss 1.3695 (Cls: 1.0908) LR: 0.000010
Batch 70: Loss 1.2733 (Cls: 1.0116) LR: 0.000010
Batch 80: Loss 1.4184 (Cls: 1.1552) LR: 0.000010
Batch 90: Loss 1.4543 (Cls: 1.1951) LR: 0.000011
Batch 100: Loss 1.3310 (Cls: 1.0693) LR: 0.000011
Batch 110: Loss 1.3421 (Cls: 1.0764) LR: 0.000011
Batch 120: Loss 1.3513 (Cls: 1.1002) LR: 0.000012
Batch 130: Loss 1.3475 (Cls: 1.0985) LR: 0.000012
Batch 140: Loss 1.3739 (Cls: 1.1211) LR: 0.000012
Batch 150: Loss 1.4085 (Cls: 1.1656) LR: 0.000013
Batch 160: Loss 1.2872 (Cls: 1.0450) LR: 0.000013
Epoch 3: Loss 1.3711, Acc 0.2462
Val Loss 1.3528, Val Acc 0.2675
New best model saved with Val Acc: 0.2675
Batch 0: Loss 1.3626 (Cls: 1.1127) LR: 0.000013
Batch 10: Loss 1.3932 (Cls: 1.1418) LR: 0.000014
Batch 20: Loss 1.3353 (Cls: 1.0888) LR: 0.000014
Batch 30: Loss 1.3244 (Cls: 1.0987) LR: 0.000014
Batch 40: Loss 1.3653 (Cls: 1.1305) LR: 0.000015
Batch 50: Loss 1.2900 (Cls: 1.0603) LR: 0.000015
Batch 60: Loss 1.3482 (Cls: 1.1100) LR: 0.000015
Batch 70: Loss 1.3432 (Cls: 1.1076) LR: 0.000016
Batch 80: Loss 1.3054 (Cls: 1.0830) LR: 0.000016
Batch 90: Loss 1.3568 (Cls: 1.1187) LR: 0.000017
Batch 100: Loss 1.3258 (Cls: 1.1033) LR: 0.000017
Batch 110: Loss 1.2325 (Cls: 1.0274) LR: 0.000017
Batch 120: Loss 1.2322 (Cls: 1.0078) LR: 0.000018
Batch 130: Loss 1.2204 (Cls: 0.9930) LR: 0.000018
Batch 140: Loss 1.3144 (Cls: 1.0875) LR: 0.000019
Batch 150: Loss 1.2405 (Cls: 1.0191) LR: 0.000019
Batch 160: Loss 1.2596 (Cls: 1.0473) LR: 0.000020
Epoch 4: Loss 1.3131, Acc 0.2781
Val Loss 1.3098, Val Acc 0.3693
New best model saved with Val Acc: 0.3693
Batch 0: Loss 1.2240 (Cls: 1.0012) LR: 0.000020
Batch 10: Loss 1.2337 (Cls: 1.0233) LR: 0.000020
Batch 20: Loss 1.2524 (Cls: 1.0338) LR: 0.000021
Batch 30: Loss 1.3412 (Cls: 1.1103) LR: 0.000021
Batch 40: Loss 1.3105 (Cls: 1.0969) LR: 0.000022
Batch 50: Loss 1.3272 (Cls: 1.1221) LR: 0.000022
Batch 60: Loss 1.3139 (Cls: 1.1061) LR: 0.000023
Batch 70: Loss 1.2342 (Cls: 1.0198) LR: 0.000023
Batch 80: Loss 1.3037 (Cls: 1.0864) LR: 0.000024
Batch 90: Loss 1.2457 (Cls: 1.0370) LR: 0.000024
Batch 100: Loss 1.1740 (Cls: 0.9717) LR: 0.000025
Batch 110: Loss 1.2477 (Cls: 1.0413) LR: 0.000025
Batch 120: Loss 1.3076 (Cls: 1.1039) LR: 0.000026
Batch 130: Loss 1.3292 (Cls: 1.1206) LR: 0.000026
Batch 140: Loss 1.2629 (Cls: 1.0730) LR: 0.000027
Batch 150: Loss 1.3119 (Cls: 1.1110) LR: 0.000027
Batch 160: Loss 1.2864 (Cls: 1.0663) LR: 0.000028
Epoch 5: Loss 1.2750, Acc 0.2924
Val Loss 1.2474, Val Acc 0.3951
New best model saved with Val Acc: 0.3951
Batch 0: Loss 1.2710 (Cls: 1.0624) LR: 0.000028
Batch 10: Loss 1.1627 (Cls: 0.9679) LR: 0.000029
Batch 20: Loss 1.2265 (Cls: 1.0244) LR: 0.000029
Batch 30: Loss 1.2614 (Cls: 1.0621) LR: 0.000030
Batch 40: Loss 1.1132 (Cls: 0.9183) LR: 0.000030
Batch 50: Loss 1.2059 (Cls: 1.0009) LR: 0.000031
Batch 60: Loss 1.2586 (Cls: 1.0671) LR: 0.000031
Batch 70: Loss 1.2249 (Cls: 1.0329) LR: 0.000032
Batch 80: Loss 1.2771 (Cls: 1.0837) LR: 0.000032
Batch 90: Loss 1.2690 (Cls: 1.0667) LR: 0.000033
Batch 100: Loss 1.2565 (Cls: 1.0548) LR: 0.000033
Batch 110: Loss 1.1312 (Cls: 0.9440) LR: 0.000034
Batch 120: Loss 1.2131 (Cls: 1.0198) LR: 0.000035
Batch 130: Loss 1.1876 (Cls: 0.9913) LR: 0.000035
Batch 140: Loss 1.2679 (Cls: 1.0679) LR: 0.000036
Batch 150: Loss 1.2085 (Cls: 1.0233) LR: 0.000036
Batch 160: Loss 1.1617 (Cls: 0.9663) LR: 0.000037
Epoch 6: Loss 1.2285, Acc 0.3271
Val Loss 1.2183, Val Acc 0.3761
Batch 0: Loss 1.2005 (Cls: 1.0227) LR: 0.000037
Batch 10: Loss 1.2236 (Cls: 1.0514) LR: 0.000038
Batch 20: Loss 1.2604 (Cls: 1.0720) LR: 0.000038
Batch 30: Loss 1.1251 (Cls: 0.9261) LR: 0.000039
Batch 40: Loss 1.1673 (Cls: 0.9788) LR: 0.000040
Batch 50: Loss 1.1657 (Cls: 0.9794) LR: 0.000040
Batch 60: Loss 1.1773 (Cls: 0.9921) LR: 0.000041
Batch 70: Loss 1.2406 (Cls: 1.0640) LR: 0.000041
Batch 80: Loss 1.0689 (Cls: 0.8849) LR: 0.000042
Batch 90: Loss 1.1557 (Cls: 0.9868) LR: 0.000042
Batch 100: Loss 1.2678 (Cls: 1.0926) LR: 0.000043
Batch 110: Loss 1.2148 (Cls: 1.0410) LR: 0.000044
Batch 120: Loss 1.0121 (Cls: 0.8251) LR: 0.000044
Batch 130: Loss 1.2924 (Cls: 1.1129) LR: 0.000045
Batch 140: Loss 1.0553 (Cls: 0.8851) LR: 0.000045
Batch 150: Loss 1.1661 (Cls: 0.9959) LR: 0.000046
Batch 160: Loss 1.0336 (Cls: 0.8628) LR: 0.000047
Epoch 7: Loss 1.1888, Acc 0.3649
Val Loss 1.1529, Val Acc 0.4468
New best model saved with Val Acc: 0.4468
Batch 0: Loss 1.2407 (Cls: 1.0660) LR: 0.000047
Batch 10: Loss 1.3090 (Cls: 1.1361) LR: 0.000048
Batch 20: Loss 1.1656 (Cls: 0.9979) LR: 0.000048
Batch 30: Loss 1.1361 (Cls: 0.9781) LR: 0.000049
Batch 40: Loss 1.1724 (Cls: 1.0186) LR: 0.000049
Batch 50: Loss 1.0552 (Cls: 0.9037) LR: 0.000050
Batch 60: Loss 1.1346 (Cls: 0.9736) LR: 0.000051
Batch 70: Loss 1.2151 (Cls: 1.0682) LR: 0.000051
Batch 80: Loss 1.2656 (Cls: 1.1109) LR: 0.000052
Batch 90: Loss 1.1759 (Cls: 1.0187) LR: 0.000052
Batch 100: Loss 1.2629 (Cls: 1.1216) LR: 0.000053
Batch 110: Loss 1.1160 (Cls: 0.9581) LR: 0.000054
Batch 120: Loss 1.1114 (Cls: 0.9651) LR: 0.000054
Batch 130: Loss 1.0886 (Cls: 0.9364) LR: 0.000055
Batch 140: Loss 1.2754 (Cls: 1.1315) LR: 0.000055
Batch 150: Loss 1.2439 (Cls: 1.1093) LR: 0.000056
Batch 160: Loss 1.1268 (Cls: 0.9790) LR: 0.000057
Epoch 8: Loss 1.1563, Acc 0.3736
Val Loss 1.1504, Val Acc 0.4848
New best model saved with Val Acc: 0.4848
Batch 0: Loss 1.1787 (Cls: 1.0360) LR: 0.000057
Batch 10: Loss 1.0995 (Cls: 0.9429) LR: 0.000058
Batch 20: Loss 1.1352 (Cls: 0.9844) LR: 0.000058
Batch 30: Loss 1.2001 (Cls: 1.0492) LR: 0.000059
Batch 40: Loss 1.0255 (Cls: 0.8840) LR: 0.000059
Batch 50: Loss 1.1139 (Cls: 0.9669) LR: 0.000060
Batch 60: Loss 1.1997 (Cls: 1.0408) LR: 0.000061
Batch 70: Loss 1.2130 (Cls: 1.0613) LR: 0.000061
Batch 80: Loss 1.1064 (Cls: 0.9695) LR: 0.000062
Batch 90: Loss 1.3133 (Cls: 1.1689) LR: 0.000062
Batch 100: Loss 1.1165 (Cls: 0.9718) LR: 0.000063
Batch 110: Loss 1.0116 (Cls: 0.8662) LR: 0.000064
Batch 120: Loss 1.1495 (Cls: 1.0151) LR: 0.000064
Batch 130: Loss 0.9851 (Cls: 0.8506) LR: 0.000065
Batch 140: Loss 1.1177 (Cls: 0.9768) LR: 0.000065
Batch 150: Loss 1.1298 (Cls: 0.9816) LR: 0.000066
Batch 160: Loss 1.0152 (Cls: 0.8779) LR: 0.000066
Epoch 9: Loss 1.1260, Acc 0.3844
Val Loss 1.1468, Val Acc 0.5220
New best model saved with Val Acc: 0.5220
Batch 0: Loss 1.2555 (Cls: 1.1293) LR: 0.000067
Batch 10: Loss 1.0637 (Cls: 0.9249) LR: 0.000067
Batch 20: Loss 1.0339 (Cls: 0.8894) LR: 0.000068
Batch 30: Loss 1.0775 (Cls: 0.9389) LR: 0.000069
Batch 40: Loss 1.0853 (Cls: 0.9526) LR: 0.000069
Batch 50: Loss 1.0807 (Cls: 0.9406) LR: 0.000070
Batch 60: Loss 1.1645 (Cls: 1.0324) LR: 0.000070
Batch 70: Loss 1.0613 (Cls: 0.9238) LR: 0.000071
Batch 80: Loss 0.9784 (Cls: 0.8461) LR: 0.000071
Batch 90: Loss 1.1138 (Cls: 0.9796) LR: 0.000072
Batch 100: Loss 1.1067 (Cls: 0.9710) LR: 0.000072
Batch 110: Loss 1.0689 (Cls: 0.9266) LR: 0.000073
Batch 120: Loss 1.0977 (Cls: 0.9659) LR: 0.000074
Batch 130: Loss 1.1268 (Cls: 0.9933) LR: 0.000074
Batch 140: Loss 1.0369 (Cls: 0.9206) LR: 0.000075
Batch 150: Loss 1.0973 (Cls: 0.9663) LR: 0.000075
Batch 160: Loss 1.0995 (Cls: 0.9658) LR: 0.000076
Epoch 10: Loss 1.1043, Acc 0.3935
Val Loss 1.0332, Val Acc 0.5653
New best model saved with Val Acc: 0.5653
Batch 0: Loss 1.1494 (Cls: 1.0135) LR: 0.000076
Batch 10: Loss 1.0933 (Cls: 0.9586) LR: 0.000077
Batch 20: Loss 1.1598 (Cls: 1.0157) LR: 0.000077
Batch 30: Loss 1.1193 (Cls: 0.9728) LR: 0.000078
Batch 40: Loss 1.0037 (Cls: 0.8674) LR: 0.000078
Batch 50: Loss 1.0243 (Cls: 0.8920) LR: 0.000079
Batch 60: Loss 1.0777 (Cls: 0.9417) LR: 0.000079
Batch 70: Loss 1.0622 (Cls: 0.9366) LR: 0.000080
Batch 80: Loss 0.9842 (Cls: 0.8602) LR: 0.000080
Batch 90: Loss 1.1561 (Cls: 1.0139) LR: 0.000081
Batch 100: Loss 1.1255 (Cls: 0.9991) LR: 0.000081
Batch 110: Loss 1.0649 (Cls: 0.9215) LR: 0.000082
Batch 120: Loss 0.9814 (Cls: 0.8397) LR: 0.000082
Batch 130: Loss 1.0835 (Cls: 0.9536) LR: 0.000082
Batch 140: Loss 1.1797 (Cls: 1.0463) LR: 0.000083
Batch 150: Loss 1.0605 (Cls: 0.9263) LR: 0.000083
Batch 160: Loss 1.2822 (Cls: 1.1450) LR: 0.000084
Epoch 11: Loss 1.0917, Acc 0.4100
Val Loss 1.0171, Val Acc 0.5304
Batch 0: Loss 1.0425 (Cls: 0.9135) LR: 0.000084
Batch 10: Loss 1.0770 (Cls: 0.9344) LR: 0.000085
Batch 20: Loss 1.1393 (Cls: 1.0099) LR: 0.000085
Batch 30: Loss 1.0713 (Cls: 0.9440) LR: 0.000085
Batch 40: Loss 1.1145 (Cls: 0.9975) LR: 0.000086
Batch 50: Loss 1.0721 (Cls: 0.9344) LR: 0.000086
Batch 60: Loss 1.1882 (Cls: 1.0549) LR: 0.000087
Batch 70: Loss 1.0035 (Cls: 0.8757) LR: 0.000087
Batch 80: Loss 1.0071 (Cls: 0.8833) LR: 0.000088
Batch 90: Loss 1.0370 (Cls: 0.9169) LR: 0.000088
Batch 100: Loss 1.2030 (Cls: 1.0870) LR: 0.000088
Batch 110: Loss 1.0431 (Cls: 0.9130) LR: 0.000089
Batch 120: Loss 1.2486 (Cls: 1.1067) LR: 0.000089
Batch 130: Loss 1.2201 (Cls: 1.0852) LR: 0.000089
Batch 140: Loss 1.0636 (Cls: 0.9271) LR: 0.000090
Batch 150: Loss 1.0689 (Cls: 0.9409) LR: 0.000090
Batch 160: Loss 1.0236 (Cls: 0.8882) LR: 0.000091
Epoch 12: Loss 1.0762, Acc 0.4217
Val Loss 1.0736, Val Acc 0.4681
Batch 0: Loss 1.1161 (Cls: 0.9871) LR: 0.000091
Batch 10: Loss 1.1349 (Cls: 1.0119) LR: 0.000091
Batch 20: Loss 1.0632 (Cls: 0.9281) LR: 0.000092
Batch 30: Loss 1.0647 (Cls: 0.9352) LR: 0.000092
Batch 40: Loss 1.1776 (Cls: 1.0448) LR: 0.000092
Batch 50: Loss 1.0369 (Cls: 0.9011) LR: 0.000093
Batch 60: Loss 1.0966 (Cls: 0.9695) LR: 0.000093
Batch 70: Loss 1.0906 (Cls: 0.9603) LR: 0.000093
Batch 80: Loss 1.1979 (Cls: 1.0608) LR: 0.000093
Batch 90: Loss 1.0815 (Cls: 0.9481) LR: 0.000094
Batch 100: Loss 1.0405 (Cls: 0.9216) LR: 0.000094
Batch 110: Loss 1.0200 (Cls: 0.8917) LR: 0.000094
Batch 120: Loss 1.1357 (Cls: 1.0124) LR: 0.000095
Batch 130: Loss 1.0470 (Cls: 0.9245) LR: 0.000095
Batch 140: Loss 1.1791 (Cls: 1.0421) LR: 0.000095
Batch 150: Loss 1.0016 (Cls: 0.8769) LR: 0.000095
Batch 160: Loss 0.9581 (Cls: 0.8262) LR: 0.000096
Epoch 13: Loss 1.0713, Acc 0.4250
Val Loss 0.9306, Val Acc 0.6535
New best model saved with Val Acc: 0.6535
Batch 0: Loss 1.0140 (Cls: 0.8833) LR: 0.000096
Batch 10: Loss 0.9394 (Cls: 0.8180) LR: 0.000096
Batch 20: Loss 1.1421 (Cls: 1.0048) LR: 0.000096
Batch 30: Loss 1.0660 (Cls: 0.9436) LR: 0.000097
Batch 40: Loss 1.1926 (Cls: 1.0482) LR: 0.000097
Batch 50: Loss 1.1192 (Cls: 0.9859) LR: 0.000097
Batch 60: Loss 1.2587 (Cls: 1.1177) LR: 0.000097
Batch 70: Loss 0.9814 (Cls: 0.8550) LR: 0.000097
Batch 80: Loss 1.0202 (Cls: 0.8828) LR: 0.000098
Batch 90: Loss 1.0479 (Cls: 0.9348) LR: 0.000098
Batch 100: Loss 1.0840 (Cls: 0.9523) LR: 0.000098
Batch 110: Loss 0.9849 (Cls: 0.8490) LR: 0.000098
Batch 120: Loss 1.1111 (Cls: 0.9941) LR: 0.000098
Batch 130: Loss 1.0652 (Cls: 0.9309) LR: 0.000098
Batch 140: Loss 1.1517 (Cls: 1.0226) LR: 0.000099
Batch 150: Loss 0.9084 (Cls: 0.7818) LR: 0.000099
Batch 160: Loss 1.0063 (Cls: 0.8697) LR: 0.000099
Epoch 14: Loss 1.0465, Acc 0.4521
Val Loss 1.0210, Val Acc 0.5258
Batch 0: Loss 0.9778 (Cls: 0.8600) LR: 0.000099
Batch 10: Loss 1.0977 (Cls: 0.9741) LR: 0.000099
Batch 20: Loss 1.0909 (Cls: 0.9607) LR: 0.000099
Batch 30: Loss 1.1260 (Cls: 1.0070) LR: 0.000099
Batch 40: Loss 1.0251 (Cls: 0.9011) LR: 0.000099
Batch 50: Loss 0.9577 (Cls: 0.8358) LR: 0.000099
Batch 60: Loss 1.0005 (Cls: 0.8762) LR: 0.000100
Batch 70: Loss 1.1073 (Cls: 0.9790) LR: 0.000100
Batch 80: Loss 1.1383 (Cls: 1.0060) LR: 0.000100
Batch 90: Loss 1.1035 (Cls: 0.9767) LR: 0.000100
Batch 100: Loss 1.0513 (Cls: 0.9189) LR: 0.000100
Batch 110: Loss 0.9893 (Cls: 0.8724) LR: 0.000100
Batch 120: Loss 1.0614 (Cls: 0.9325) LR: 0.000100
Batch 130: Loss 1.1566 (Cls: 1.0282) LR: 0.000100
Batch 140: Loss 0.9866 (Cls: 0.8665) LR: 0.000100
Batch 150: Loss 1.0357 (Cls: 0.9020) LR: 0.000100
Batch 160: Loss 1.0410 (Cls: 0.9043) LR: 0.000100
Epoch 15: Loss 1.0387, Acc 0.4591
Val Loss 1.0195, Val Acc 0.5410
Batch 0: Loss 1.0240 (Cls: 0.8994) LR: 0.000100
Batch 10: Loss 1.0417 (Cls: 0.9162) LR: 0.000100
Batch 20: Loss 1.0197 (Cls: 0.8876) LR: 0.000100
Batch 30: Loss 0.9602 (Cls: 0.8428) LR: 0.000100
Batch 40: Loss 1.0355 (Cls: 0.9046) LR: 0.000100
Batch 50: Loss 1.0395 (Cls: 0.9164) LR: 0.000100
Batch 60: Loss 0.9995 (Cls: 0.8667) LR: 0.000100
Batch 70: Loss 0.8944 (Cls: 0.7719) LR: 0.000100
Batch 80: Loss 0.8266 (Cls: 0.6978) LR: 0.000100
Batch 90: Loss 0.9683 (Cls: 0.8435) LR: 0.000100
Batch 100: Loss 0.9630 (Cls: 0.8387) LR: 0.000100
Batch 110: Loss 1.0892 (Cls: 0.9595) LR: 0.000100
Batch 120: Loss 1.0809 (Cls: 0.9560) LR: 0.000100
Batch 130: Loss 0.9222 (Cls: 0.7923) LR: 0.000100
Batch 140: Loss 0.8864 (Cls: 0.7502) LR: 0.000100
Batch 150: Loss 1.0244 (Cls: 0.9026) LR: 0.000100
Batch 160: Loss 0.9048 (Cls: 0.7747) LR: 0.000100
Epoch 16: Loss 1.0040, Acc 0.4754
Val Loss 1.0184, Val Acc 0.5691
Batch 0: Loss 1.0344 (Cls: 0.9084) LR: 0.000100
Batch 10: Loss 0.9518 (Cls: 0.8349) LR: 0.000100
Batch 20: Loss 1.0845 (Cls: 0.9575) LR: 0.000100
Batch 30: Loss 0.9693 (Cls: 0.8453) LR: 0.000100
Batch 40: Loss 0.9502 (Cls: 0.8272) LR: 0.000100
Batch 50: Loss 0.9539 (Cls: 0.8324) LR: 0.000100
Batch 60: Loss 1.0952 (Cls: 0.9701) LR: 0.000100
Batch 70: Loss 1.0151 (Cls: 0.8850) LR: 0.000100
Batch 80: Loss 1.0946 (Cls: 0.9619) LR: 0.000100
Batch 90: Loss 0.8888 (Cls: 0.7646) LR: 0.000100
Batch 100: Loss 0.9618 (Cls: 0.8303) LR: 0.000099
Batch 110: Loss 1.0468 (Cls: 0.9212) LR: 0.000099
Batch 120: Loss 0.8490 (Cls: 0.7333) LR: 0.000099
Batch 130: Loss 0.9706 (Cls: 0.8512) LR: 0.000099
Batch 140: Loss 0.9219 (Cls: 0.8034) LR: 0.000099
Batch 150: Loss 1.0049 (Cls: 0.8932) LR: 0.000099
Batch 160: Loss 0.9541 (Cls: 0.8331) LR: 0.000099
Epoch 17: Loss 1.0066, Acc 0.4856
Val Loss 0.8605, Val Acc 0.6581
New best model saved with Val Acc: 0.6581
Batch 0: Loss 1.1685 (Cls: 1.0422) LR: 0.000099
Batch 10: Loss 1.0938 (Cls: 0.9767) LR: 0.000099
Batch 20: Loss 0.9835 (Cls: 0.8655) LR: 0.000099
Batch 30: Loss 1.1426 (Cls: 1.0091) LR: 0.000099
Batch 40: Loss 1.3028 (Cls: 1.1776) LR: 0.000099
Batch 50: Loss 0.9759 (Cls: 0.8537) LR: 0.000099
Batch 60: Loss 1.0735 (Cls: 0.9411) LR: 0.000099
Batch 70: Loss 1.0243 (Cls: 0.9086) LR: 0.000099
Batch 80: Loss 1.0892 (Cls: 0.9640) LR: 0.000099
Batch 90: Loss 1.1418 (Cls: 1.0205) LR: 0.000099
Batch 100: Loss 0.9594 (Cls: 0.8413) LR: 0.000099
Batch 110: Loss 0.8887 (Cls: 0.7641) LR: 0.000099
Batch 120: Loss 1.0547 (Cls: 0.9342) LR: 0.000099
Batch 130: Loss 0.9365 (Cls: 0.8170) LR: 0.000098
Batch 140: Loss 1.0417 (Cls: 0.9111) LR: 0.000098
Batch 150: Loss 0.9722 (Cls: 0.8547) LR: 0.000098
Batch 160: Loss 1.0248 (Cls: 0.9123) LR: 0.000098
Epoch 18: Loss 0.9983, Acc 0.4955
Val Loss 0.9735, Val Acc 0.5767
Batch 0: Loss 1.0101 (Cls: 0.8812) LR: 0.000098
Batch 10: Loss 1.0572 (Cls: 0.9324) LR: 0.000098
Batch 20: Loss 0.9467 (Cls: 0.8211) LR: 0.000098
Batch 30: Loss 0.9664 (Cls: 0.8358) LR: 0.000098
Batch 40: Loss 1.0291 (Cls: 0.8952) LR: 0.000098
Batch 50: Loss 1.0192 (Cls: 0.8990) LR: 0.000098
Batch 60: Loss 0.9161 (Cls: 0.7888) LR: 0.000098
Batch 70: Loss 0.9393 (Cls: 0.8138) LR: 0.000098
Batch 80: Loss 0.9819 (Cls: 0.8536) LR: 0.000098
Batch 90: Loss 1.0090 (Cls: 0.8939) LR: 0.000097
Batch 100: Loss 1.0944 (Cls: 0.9627) LR: 0.000097
Batch 110: Loss 0.9996 (Cls: 0.8843) LR: 0.000097
Batch 120: Loss 0.9369 (Cls: 0.8197) LR: 0.000097
Batch 130: Loss 0.9456 (Cls: 0.8232) LR: 0.000097
Batch 140: Loss 0.9691 (Cls: 0.8489) LR: 0.000097
Batch 150: Loss 1.0138 (Cls: 0.8882) LR: 0.000097
Batch 160: Loss 0.9678 (Cls: 0.8520) LR: 0.000097
Epoch 19: Loss 0.9831, Acc 0.5146
Val Loss 0.9007, Val Acc 0.6619
New best model saved with Val Acc: 0.6619
Batch 0: Loss 0.8431 (Cls: 0.7250) LR: 0.000097
Batch 10: Loss 1.0134 (Cls: 0.8783) LR: 0.000097
Batch 20: Loss 0.9879 (Cls: 0.8816) LR: 0.000097
Batch 30: Loss 1.0171 (Cls: 0.8976) LR: 0.000097
Batch 40: Loss 0.7357 (Cls: 0.6156) LR: 0.000096
Batch 50: Loss 0.9225 (Cls: 0.8064) LR: 0.000096
Batch 60: Loss 0.9428 (Cls: 0.8242) LR: 0.000096
Batch 70: Loss 1.0522 (Cls: 0.9307) LR: 0.000096
Batch 80: Loss 1.0131 (Cls: 0.8864) LR: 0.000096
Batch 90: Loss 1.0697 (Cls: 0.9517) LR: 0.000096
Batch 100: Loss 1.0209 (Cls: 0.9113) LR: 0.000096
Batch 110: Loss 0.8609 (Cls: 0.7343) LR: 0.000096
Batch 120: Loss 1.1041 (Cls: 0.9911) LR: 0.000096
Batch 130: Loss 1.0456 (Cls: 0.9182) LR: 0.000095
Batch 140: Loss 1.0457 (Cls: 0.9297) LR: 0.000095
Batch 150: Loss 0.9559 (Cls: 0.8316) LR: 0.000095
Batch 160: Loss 0.8500 (Cls: 0.7318) LR: 0.000095
Epoch 20: Loss 0.9703, Acc 0.5151
Val Loss 0.9568, Val Acc 0.6140
Batch 0: Loss 0.9645 (Cls: 0.8482) LR: 0.000095
Batch 10: Loss 0.9730 (Cls: 0.8622) LR: 0.000095
Batch 20: Loss 0.9525 (Cls: 0.8211) LR: 0.000095
Batch 30: Loss 0.9056 (Cls: 0.7815) LR: 0.000095
Batch 40: Loss 1.0123 (Cls: 0.8913) LR: 0.000095
Batch 50: Loss 0.9369 (Cls: 0.8152) LR: 0.000094
Batch 60: Loss 1.0206 (Cls: 0.8925) LR: 0.000094
Batch 70: Loss 0.9965 (Cls: 0.8688) LR: 0.000094
Batch 80: Loss 1.0570 (Cls: 0.9386) LR: 0.000094
Batch 90: Loss 0.7778 (Cls: 0.6663) LR: 0.000094
Batch 100: Loss 1.0400 (Cls: 0.9227) LR: 0.000094
Batch 110: Loss 1.0067 (Cls: 0.8749) LR: 0.000094
Batch 120: Loss 0.9500 (Cls: 0.8289) LR: 0.000094
Batch 130: Loss 1.0355 (Cls: 0.9264) LR: 0.000093
Batch 140: Loss 1.0270 (Cls: 0.9030) LR: 0.000093
Batch 150: Loss 0.9595 (Cls: 0.8351) LR: 0.000093
Batch 160: Loss 0.8204 (Cls: 0.6906) LR: 0.000093
Epoch 21: Loss 0.9645, Acc 0.5251
Val Loss 0.7898, Val Acc 0.7386
New best model saved with Val Acc: 0.7386
Batch 0: Loss 1.1957 (Cls: 1.0762) LR: 0.000093
Batch 10: Loss 0.9864 (Cls: 0.8700) LR: 0.000093
Batch 20: Loss 0.8852 (Cls: 0.7673) LR: 0.000093
Batch 30: Loss 0.9233 (Cls: 0.8018) LR: 0.000092
Batch 40: Loss 0.9313 (Cls: 0.8126) LR: 0.000092
Batch 50: Loss 1.0530 (Cls: 0.9373) LR: 0.000092
Batch 60: Loss 0.8498 (Cls: 0.7342) LR: 0.000092
Batch 70: Loss 1.0531 (Cls: 0.9291) LR: 0.000092
Batch 80: Loss 1.1112 (Cls: 1.0092) LR: 0.000092
Batch 90: Loss 0.8971 (Cls: 0.7892) LR: 0.000092
Batch 100: Loss 0.9757 (Cls: 0.8643) LR: 0.000091
Batch 110: Loss 0.8393 (Cls: 0.7103) LR: 0.000091
Batch 120: Loss 0.9069 (Cls: 0.7898) LR: 0.000091
Batch 130: Loss 0.9141 (Cls: 0.8079) LR: 0.000091
Batch 140: Loss 0.8473 (Cls: 0.7351) LR: 0.000091
Batch 150: Loss 0.9068 (Cls: 0.7777) LR: 0.000091
Batch 160: Loss 0.9215 (Cls: 0.8047) LR: 0.000091
Epoch 22: Loss 0.9418, Acc 0.5361
Val Loss 0.9496, Val Acc 0.6117
Batch 0: Loss 0.9690 (Cls: 0.8616) LR: 0.000090
Batch 10: Loss 0.9660 (Cls: 0.8395) LR: 0.000090
Batch 20: Loss 0.8894 (Cls: 0.7746) LR: 0.000090
Batch 30: Loss 0.8556 (Cls: 0.7494) LR: 0.000090
Batch 40: Loss 0.9723 (Cls: 0.8596) LR: 0.000090
Batch 50: Loss 1.1175 (Cls: 0.9935) LR: 0.000090
Batch 60: Loss 1.0067 (Cls: 0.8817) LR: 0.000089
Batch 70: Loss 1.0515 (Cls: 0.9361) LR: 0.000089
Batch 80: Loss 0.8175 (Cls: 0.7106) LR: 0.000089
Batch 90: Loss 0.8628 (Cls: 0.7497) LR: 0.000089
Batch 100: Loss 0.8687 (Cls: 0.7485) LR: 0.000089
Batch 110: Loss 0.8792 (Cls: 0.7618) LR: 0.000089
Batch 120: Loss 1.0457 (Cls: 0.9336) LR: 0.000088
Batch 130: Loss 0.9826 (Cls: 0.8673) LR: 0.000088
Batch 140: Loss 0.8431 (Cls: 0.7188) LR: 0.000088
Batch 150: Loss 0.9537 (Cls: 0.8333) LR: 0.000088
Batch 160: Loss 1.0884 (Cls: 0.9777) LR: 0.000088
Epoch 23: Loss 0.9402, Acc 0.5351
Val Loss 1.0919, Val Acc 0.5426
Batch 0: Loss 0.7410 (Cls: 0.6286) LR: 0.000088
Batch 10: Loss 0.8374 (Cls: 0.7218) LR: 0.000087
Batch 20: Loss 0.9586 (Cls: 0.8389) LR: 0.000087
Batch 30: Loss 0.9752 (Cls: 0.8521) LR: 0.000087
Batch 40: Loss 0.9847 (Cls: 0.8624) LR: 0.000087
Batch 50: Loss 1.0310 (Cls: 0.9209) LR: 0.000087
Batch 60: Loss 0.8987 (Cls: 0.7748) LR: 0.000087
Batch 70: Loss 0.8346 (Cls: 0.7209) LR: 0.000086
Batch 80: Loss 0.9600 (Cls: 0.8402) LR: 0.000086
Batch 90: Loss 0.9419 (Cls: 0.8179) LR: 0.000086
Batch 100: Loss 0.8576 (Cls: 0.7349) LR: 0.000086
Batch 110: Loss 1.0120 (Cls: 0.8814) LR: 0.000086
Batch 120: Loss 0.9714 (Cls: 0.8458) LR: 0.000085
Batch 130: Loss 0.9219 (Cls: 0.8151) LR: 0.000085
Batch 140: Loss 1.0630 (Cls: 0.9511) LR: 0.000085
Batch 150: Loss 0.9074 (Cls: 0.7935) LR: 0.000085
Batch 160: Loss 0.9149 (Cls: 0.8121) LR: 0.000085
Epoch 24: Loss 0.9257, Acc 0.5585
Val Loss 1.0046, Val Acc 0.6102
Batch 0: Loss 0.7448 (Cls: 0.6271) LR: 0.000085
Batch 10: Loss 0.8789 (Cls: 0.7646) LR: 0.000084
Batch 20: Loss 0.8039 (Cls: 0.6969) LR: 0.000084
Batch 30: Loss 0.8409 (Cls: 0.7336) LR: 0.000084
Batch 40: Loss 0.9839 (Cls: 0.8721) LR: 0.000084
Batch 50: Loss 0.7844 (Cls: 0.6710) LR: 0.000084
Batch 60: Loss 0.9474 (Cls: 0.8239) LR: 0.000083
Batch 70: Loss 0.8359 (Cls: 0.7245) LR: 0.000083
Batch 80: Loss 0.9093 (Cls: 0.8066) LR: 0.000083
Batch 90: Loss 0.9283 (Cls: 0.8262) LR: 0.000083
Batch 100: Loss 0.9050 (Cls: 0.7828) LR: 0.000083
Batch 110: Loss 0.8185 (Cls: 0.7045) LR: 0.000082
Batch 120: Loss 0.8829 (Cls: 0.7631) LR: 0.000082
Batch 130: Loss 0.7806 (Cls: 0.6686) LR: 0.000082
Batch 140: Loss 0.8717 (Cls: 0.7541) LR: 0.000082
Batch 150: Loss 1.0644 (Cls: 0.9352) LR: 0.000082
Batch 160: Loss 1.1468 (Cls: 1.0320) LR: 0.000081
Epoch 25: Loss 0.9237, Acc 0.5591
Val Loss 0.9123, Val Acc 0.6368
Batch 0: Loss 0.8479 (Cls: 0.7292) LR: 0.000081
Batch 10: Loss 0.9206 (Cls: 0.8086) LR: 0.000081
Batch 20: Loss 0.8978 (Cls: 0.7832) LR: 0.000081
Batch 30: Loss 1.0909 (Cls: 0.9672) LR: 0.000081
Batch 40: Loss 0.8062 (Cls: 0.7010) LR: 0.000080
Batch 50: Loss 0.7825 (Cls: 0.6683) LR: 0.000080
Batch 60: Loss 0.9818 (Cls: 0.8736) LR: 0.000080
Batch 70: Loss 0.8848 (Cls: 0.7766) LR: 0.000080
Batch 80: Loss 0.8058 (Cls: 0.6929) LR: 0.000079
Batch 90: Loss 1.0514 (Cls: 0.9404) LR: 0.000079
Batch 100: Loss 0.8863 (Cls: 0.7579) LR: 0.000079
Batch 110: Loss 0.9598 (Cls: 0.8512) LR: 0.000079
Batch 120: Loss 1.0280 (Cls: 0.9128) LR: 0.000079
Batch 130: Loss 0.8705 (Cls: 0.7530) LR: 0.000078
Batch 140: Loss 0.9341 (Cls: 0.8264) LR: 0.000078
Batch 150: Loss 0.9412 (Cls: 0.8176) LR: 0.000078
Batch 160: Loss 1.0263 (Cls: 0.9043) LR: 0.000078
Epoch 26: Loss 0.9012, Acc 0.5802
Val Loss 0.8794, Val Acc 0.6360
Batch 0: Loss 0.9525 (Cls: 0.8359) LR: 0.000078
Batch 10: Loss 0.7327 (Cls: 0.6153) LR: 0.000077
Batch 20: Loss 0.9655 (Cls: 0.8593) LR: 0.000077
Batch 30: Loss 0.8950 (Cls: 0.7816) LR: 0.000077
Batch 40: Loss 0.9795 (Cls: 0.8637) LR: 0.000077
Batch 50: Loss 0.7137 (Cls: 0.6013) LR: 0.000076
Batch 60: Loss 0.8572 (Cls: 0.7399) LR: 0.000076
Batch 70: Loss 0.8069 (Cls: 0.6884) LR: 0.000076
Batch 80: Loss 1.0367 (Cls: 0.9148) LR: 0.000076
Batch 90: Loss 0.8475 (Cls: 0.7345) LR: 0.000075
Batch 100: Loss 1.1247 (Cls: 1.0011) LR: 0.000075
Batch 110: Loss 0.9565 (Cls: 0.8466) LR: 0.000075
Batch 120: Loss 0.8085 (Cls: 0.6863) LR: 0.000075
Batch 130: Loss 0.9360 (Cls: 0.8183) LR: 0.000075
Batch 140: Loss 0.7786 (Cls: 0.6620) LR: 0.000074
Batch 150: Loss 0.8298 (Cls: 0.7189) LR: 0.000074
Batch 160: Loss 0.8551 (Cls: 0.7520) LR: 0.000074
Epoch 27: Loss 0.8888, Acc 0.5917
Val Loss 0.8431, Val Acc 0.6733
Batch 0: Loss 0.7466 (Cls: 0.6256) LR: 0.000074
Batch 10: Loss 0.8778 (Cls: 0.7652) LR: 0.000073
Batch 20: Loss 0.9292 (Cls: 0.8219) LR: 0.000073
Batch 30: Loss 0.9111 (Cls: 0.7931) LR: 0.000073
Batch 40: Loss 0.7897 (Cls: 0.6719) LR: 0.000073
Batch 50: Loss 0.9217 (Cls: 0.8073) LR: 0.000072
Batch 60: Loss 0.9086 (Cls: 0.7849) LR: 0.000072
Batch 70: Loss 0.8716 (Cls: 0.7579) LR: 0.000072
Batch 80: Loss 0.8407 (Cls: 0.7212) LR: 0.000072
Batch 90: Loss 0.7760 (Cls: 0.6642) LR: 0.000072
Batch 100: Loss 0.9170 (Cls: 0.8058) LR: 0.000071
Batch 110: Loss 0.9051 (Cls: 0.7990) LR: 0.000071
Batch 120: Loss 1.0351 (Cls: 0.9076) LR: 0.000071
Batch 130: Loss 0.9454 (Cls: 0.8257) LR: 0.000071
Batch 140: Loss 0.8076 (Cls: 0.6990) LR: 0.000070
Batch 150: Loss 0.9371 (Cls: 0.8109) LR: 0.000070
Batch 160: Loss 0.8135 (Cls: 0.6975) LR: 0.000070
Epoch 28: Loss 0.8854, Acc 0.5973
Val Loss 0.8026, Val Acc 0.7257
Batch 0: Loss 0.8905 (Cls: 0.7799) LR: 0.000070
Batch 10: Loss 0.8408 (Cls: 0.7136) LR: 0.000069
Batch 20: Loss 0.8767 (Cls: 0.7567) LR: 0.000069
Batch 30: Loss 0.8733 (Cls: 0.7577) LR: 0.000069
Batch 40: Loss 0.7952 (Cls: 0.6920) LR: 0.000069
Batch 50: Loss 0.9800 (Cls: 0.8673) LR: 0.000068
Batch 60: Loss 0.7480 (Cls: 0.6399) LR: 0.000068
Batch 70: Loss 0.8077 (Cls: 0.6954) LR: 0.000068
Batch 80: Loss 0.9941 (Cls: 0.8865) LR: 0.000068
Batch 90: Loss 0.9548 (Cls: 0.8365) LR: 0.000067
Batch 100: Loss 0.8428 (Cls: 0.7366) LR: 0.000067
Batch 110: Loss 0.7445 (Cls: 0.6381) LR: 0.000067
Batch 120: Loss 0.9703 (Cls: 0.8530) LR: 0.000067
Batch 130: Loss 0.7761 (Cls: 0.6684) LR: 0.000066
Batch 140: Loss 0.7590 (Cls: 0.6505) LR: 0.000066
Batch 150: Loss 0.8288 (Cls: 0.7287) LR: 0.000066
Batch 160: Loss 0.8571 (Cls: 0.7354) LR: 0.000066
Epoch 29: Loss 0.8817, Acc 0.6050
Val Loss 0.8247, Val Acc 0.6976
Batch 0: Loss 0.9395 (Cls: 0.8350) LR: 0.000065
Batch 10: Loss 0.7907 (Cls: 0.6729) LR: 0.000065
Batch 20: Loss 0.8223 (Cls: 0.7048) LR: 0.000065
Batch 30: Loss 0.7723 (Cls: 0.6550) LR: 0.000065
Batch 40: Loss 1.0156 (Cls: 0.9028) LR: 0.000064
Batch 50: Loss 0.7923 (Cls: 0.6797) LR: 0.000064
Batch 60: Loss 0.8872 (Cls: 0.7652) LR: 0.000064
Batch 70: Loss 0.7853 (Cls: 0.6722) LR: 0.000064
Batch 80: Loss 0.8443 (Cls: 0.7358) LR: 0.000063
Batch 90: Loss 0.9061 (Cls: 0.7946) LR: 0.000063
Batch 100: Loss 0.7311 (Cls: 0.6225) LR: 0.000063
Batch 110: Loss 0.8049 (Cls: 0.6916) LR: 0.000063
Batch 120: Loss 0.9133 (Cls: 0.8063) LR: 0.000062
Batch 130: Loss 0.8348 (Cls: 0.7292) LR: 0.000062
Batch 140: Loss 0.7313 (Cls: 0.6147) LR: 0.000062
Batch 150: Loss 0.7180 (Cls: 0.6029) LR: 0.000062
Batch 160: Loss 0.8672 (Cls: 0.7443) LR: 0.000061
Epoch 30: Loss 0.8597, Acc 0.6227
Val Loss 0.8103, Val Acc 0.7158
Batch 0: Loss 0.7729 (Cls: 0.6598) LR: 0.000061
Batch 10: Loss 0.8720 (Cls: 0.7564) LR: 0.000061
Batch 20: Loss 0.8532 (Cls: 0.7376) LR: 0.000061
Batch 30: Loss 0.8424 (Cls: 0.7346) LR: 0.000060
Batch 40: Loss 0.7414 (Cls: 0.6193) LR: 0.000060
Batch 50: Loss 0.7592 (Cls: 0.6513) LR: 0.000060
Batch 60: Loss 0.7985 (Cls: 0.6794) LR: 0.000060
Batch 70: Loss 0.8176 (Cls: 0.6972) LR: 0.000059
Batch 80: Loss 0.8023 (Cls: 0.6969) LR: 0.000059
Batch 90: Loss 0.9061 (Cls: 0.8042) LR: 0.000059
Batch 100: Loss 0.7715 (Cls: 0.6634) LR: 0.000058
Batch 110: Loss 0.8411 (Cls: 0.7315) LR: 0.000058
Batch 120: Loss 0.8097 (Cls: 0.6890) LR: 0.000058
Batch 130: Loss 0.8002 (Cls: 0.6939) LR: 0.000058
Batch 140: Loss 0.8325 (Cls: 0.7160) LR: 0.000057
Batch 150: Loss 0.8527 (Cls: 0.7418) LR: 0.000057
Batch 160: Loss 0.8525 (Cls: 0.7320) LR: 0.000057
Epoch 31: Loss 0.8486, Acc 0.6297
Val Loss 0.8624, Val Acc 0.6740
Batch 0: Loss 0.7455 (Cls: 0.6193) LR: 0.000057
Batch 10: Loss 0.6237 (Cls: 0.5112) LR: 0.000056
Batch 20: Loss 0.9334 (Cls: 0.8186) LR: 0.000056
Batch 30: Loss 0.7867 (Cls: 0.6775) LR: 0.000056
Batch 40: Loss 0.7302 (Cls: 0.6250) LR: 0.000056
Batch 50: Loss 0.9145 (Cls: 0.7935) LR: 0.000055
Batch 60: Loss 0.8278 (Cls: 0.7294) LR: 0.000055
Batch 70: Loss 0.7649 (Cls: 0.6606) LR: 0.000055
Batch 80: Loss 0.8795 (Cls: 0.7648) LR: 0.000055
Batch 90: Loss 0.8373 (Cls: 0.7161) LR: 0.000054
Batch 100: Loss 0.7901 (Cls: 0.6673) LR: 0.000054
Batch 110: Loss 0.9334 (Cls: 0.8152) LR: 0.000054
Batch 120: Loss 0.8084 (Cls: 0.7079) LR: 0.000053
Batch 130: Loss 0.6778 (Cls: 0.5741) LR: 0.000053
Batch 140: Loss 0.8874 (Cls: 0.7808) LR: 0.000053
Batch 150: Loss 0.7924 (Cls: 0.6875) LR: 0.000053
Batch 160: Loss 0.7790 (Cls: 0.6604) LR: 0.000052
Epoch 32: Loss 0.8306, Acc 0.6473
Val Loss 1.0047, Val Acc 0.6147
Batch 0: Loss 0.6692 (Cls: 0.5622) LR: 0.000052
Batch 10: Loss 0.7467 (Cls: 0.6371) LR: 0.000052
Batch 20: Loss 0.8794 (Cls: 0.7707) LR: 0.000052
Batch 30: Loss 0.7337 (Cls: 0.6236) LR: 0.000051
Batch 40: Loss 0.7174 (Cls: 0.6095) LR: 0.000051
Batch 50: Loss 0.7908 (Cls: 0.6745) LR: 0.000051
Batch 60: Loss 0.6841 (Cls: 0.5847) LR: 0.000051
Batch 70: Loss 0.8903 (Cls: 0.7701) LR: 0.000050
Batch 80: Loss 0.8144 (Cls: 0.7007) LR: 0.000050
Batch 90: Loss 0.7739 (Cls: 0.6686) LR: 0.000050
Batch 100: Loss 0.9015 (Cls: 0.7860) LR: 0.000050
Batch 110: Loss 0.8371 (Cls: 0.7187) LR: 0.000049
Batch 120: Loss 0.7468 (Cls: 0.6295) LR: 0.000049
Batch 130: Loss 0.8392 (Cls: 0.7193) LR: 0.000049
Batch 140: Loss 0.7210 (Cls: 0.6072) LR: 0.000048
Batch 150: Loss 0.8910 (Cls: 0.7773) LR: 0.000048
Batch 160: Loss 0.7931 (Cls: 0.6857) LR: 0.000048
Epoch 33: Loss 0.8228, Acc 0.6490
Val Loss 0.8314, Val Acc 0.6976
Batch 0: Loss 0.8580 (Cls: 0.7405) LR: 0.000048
Batch 10: Loss 0.8555 (Cls: 0.7559) LR: 0.000047
Batch 20: Loss 0.7808 (Cls: 0.6686) LR: 0.000047
Batch 30: Loss 0.7465 (Cls: 0.6389) LR: 0.000047
Batch 40: Loss 0.7017 (Cls: 0.5810) LR: 0.000047
Batch 50: Loss 0.7487 (Cls: 0.6393) LR: 0.000046
Batch 60: Loss 0.7899 (Cls: 0.6791) LR: 0.000046
Batch 70: Loss 0.8226 (Cls: 0.7202) LR: 0.000046
Batch 80: Loss 0.8442 (Cls: 0.7278) LR: 0.000046
Batch 90: Loss 0.7208 (Cls: 0.6113) LR: 0.000045
Batch 100: Loss 0.9269 (Cls: 0.8085) LR: 0.000045
Batch 110: Loss 0.8587 (Cls: 0.7280) LR: 0.000045
Batch 120: Loss 0.7751 (Cls: 0.6661) LR: 0.000045
Batch 130: Loss 0.8115 (Cls: 0.7107) LR: 0.000044
Batch 140: Loss 0.9265 (Cls: 0.8137) LR: 0.000044
Batch 150: Loss 0.8905 (Cls: 0.7706) LR: 0.000044
Batch 160: Loss 0.9094 (Cls: 0.8032) LR: 0.000043
Epoch 34: Loss 0.8103, Acc 0.6655
Val Loss 0.7935, Val Acc 0.7226
Batch 0: Loss 0.9575 (Cls: 0.8396) LR: 0.000043
Batch 10: Loss 0.9364 (Cls: 0.8232) LR: 0.000043
Batch 20: Loss 0.7171 (Cls: 0.6053) LR: 0.000043
Batch 30: Loss 0.8552 (Cls: 0.7304) LR: 0.000042
Batch 40: Loss 0.7612 (Cls: 0.6535) LR: 0.000042
Batch 50: Loss 0.7172 (Cls: 0.6077) LR: 0.000042
Batch 60: Loss 0.7116 (Cls: 0.6038) LR: 0.000042
Batch 70: Loss 0.7917 (Cls: 0.6693) LR: 0.000041
Batch 80: Loss 0.6981 (Cls: 0.5986) LR: 0.000041
Batch 90: Loss 0.6887 (Cls: 0.5839) LR: 0.000041
Batch 100: Loss 0.7893 (Cls: 0.6785) LR: 0.000041
Batch 110: Loss 0.8373 (Cls: 0.7326) LR: 0.000040
Batch 120: Loss 0.8705 (Cls: 0.7528) LR: 0.000040
Batch 130: Loss 0.8107 (Cls: 0.6986) LR: 0.000040
Batch 140: Loss 1.0178 (Cls: 0.8957) LR: 0.000040
Batch 150: Loss 0.7238 (Cls: 0.6106) LR: 0.000039
Batch 160: Loss 0.8936 (Cls: 0.7671) LR: 0.000039
Epoch 35: Loss 0.8044, Acc 0.6612
Val Loss 0.7526, Val Acc 0.7614
New best model saved with Val Acc: 0.7614
Batch 0: Loss 0.7229 (Cls: 0.6130) LR: 0.000039
Batch 10: Loss 0.8058 (Cls: 0.6992) LR: 0.000039
Batch 20: Loss 0.5795 (Cls: 0.4621) LR: 0.000038
Batch 30: Loss 0.7432 (Cls: 0.6389) LR: 0.000038
Batch 40: Loss 0.7384 (Cls: 0.6104) LR: 0.000038
Batch 50: Loss 0.6848 (Cls: 0.5656) LR: 0.000038
Batch 60: Loss 1.0200 (Cls: 0.9096) LR: 0.000037
Batch 70: Loss 0.6783 (Cls: 0.5662) LR: 0.000037
Batch 80: Loss 0.7811 (Cls: 0.6707) LR: 0.000037
Batch 90: Loss 0.7547 (Cls: 0.6559) LR: 0.000037
Batch 100: Loss 0.8915 (Cls: 0.7734) LR: 0.000036
Batch 110: Loss 0.9136 (Cls: 0.7856) LR: 0.000036
Batch 120: Loss 0.8897 (Cls: 0.7729) LR: 0.000036
Batch 130: Loss 0.9710 (Cls: 0.8581) LR: 0.000035
Batch 140: Loss 1.0002 (Cls: 0.8815) LR: 0.000035
Batch 150: Loss 0.7031 (Cls: 0.5803) LR: 0.000035
Batch 160: Loss 0.8645 (Cls: 0.7499) LR: 0.000035
Epoch 36: Loss 0.8003, Acc 0.6742
Val Loss 0.7667, Val Acc 0.7622
New best model saved with Val Acc: 0.7622
Batch 0: Loss 0.8763 (Cls: 0.7563) LR: 0.000034
Batch 10: Loss 0.7180 (Cls: 0.5968) LR: 0.000034
Batch 20: Loss 0.7545 (Cls: 0.6323) LR: 0.000034
Batch 30: Loss 1.0396 (Cls: 0.9165) LR: 0.000034
Batch 40: Loss 0.6981 (Cls: 0.5819) LR: 0.000033
Batch 50: Loss 0.8257 (Cls: 0.7209) LR: 0.000033
Batch 60: Loss 0.7127 (Cls: 0.5965) LR: 0.000033
Batch 70: Loss 0.7675 (Cls: 0.6603) LR: 0.000033
Batch 80: Loss 0.7338 (Cls: 0.6296) LR: 0.000032
Batch 90: Loss 0.6729 (Cls: 0.5549) LR: 0.000032
Batch 100: Loss 0.8239 (Cls: 0.7156) LR: 0.000032
Batch 110: Loss 0.7698 (Cls: 0.6519) LR: 0.000032
Batch 120: Loss 0.6909 (Cls: 0.5899) LR: 0.000032
Batch 130: Loss 0.9330 (Cls: 0.8186) LR: 0.000031
Batch 140: Loss 0.7872 (Cls: 0.6770) LR: 0.000031
Batch 150: Loss 0.6748 (Cls: 0.5720) LR: 0.000031
Batch 160: Loss 0.9967 (Cls: 0.8865) LR: 0.000031
Epoch 37: Loss 0.7724, Acc 0.6987
Val Loss 0.7952, Val Acc 0.7371
Batch 0: Loss 0.8078 (Cls: 0.7070) LR: 0.000030
Batch 10: Loss 0.7100 (Cls: 0.5894) LR: 0.000030
Batch 20: Loss 0.6737 (Cls: 0.5620) LR: 0.000030
Batch 30: Loss 0.7849 (Cls: 0.6854) LR: 0.000030
Batch 40: Loss 0.7577 (Cls: 0.6540) LR: 0.000029
Batch 50: Loss 0.7141 (Cls: 0.6103) LR: 0.000029
Batch 60: Loss 0.7659 (Cls: 0.6596) LR: 0.000029
Batch 70: Loss 0.8768 (Cls: 0.7667) LR: 0.000029
Batch 80: Loss 0.8121 (Cls: 0.6978) LR: 0.000028
Batch 90: Loss 0.6649 (Cls: 0.5506) LR: 0.000028
Batch 100: Loss 0.7408 (Cls: 0.6385) LR: 0.000028
Batch 110: Loss 0.7169 (Cls: 0.6072) LR: 0.000028
Batch 120: Loss 0.7423 (Cls: 0.6245) LR: 0.000027
Batch 130: Loss 0.8897 (Cls: 0.7782) LR: 0.000027
Batch 140: Loss 0.7985 (Cls: 0.6856) LR: 0.000027
Batch 150: Loss 0.7317 (Cls: 0.6302) LR: 0.000027
Batch 160: Loss 0.7221 (Cls: 0.6008) LR: 0.000026
Epoch 38: Loss 0.7673, Acc 0.7033
Val Loss 0.7797, Val Acc 0.7508
Batch 0: Loss 0.5998 (Cls: 0.4988) LR: 0.000026
Batch 10: Loss 0.8841 (Cls: 0.7695) LR: 0.000026
Batch 20: Loss 0.6185 (Cls: 0.5135) LR: 0.000026
Batch 30: Loss 0.8532 (Cls: 0.7382) LR: 0.000026
Batch 40: Loss 0.8454 (Cls: 0.7405) LR: 0.000025
Batch 50: Loss 0.6997 (Cls: 0.5809) LR: 0.000025
Batch 60: Loss 0.7487 (Cls: 0.6384) LR: 0.000025
Batch 70: Loss 0.7797 (Cls: 0.6768) LR: 0.000025
Batch 80: Loss 0.6804 (Cls: 0.5779) LR: 0.000024
Batch 90: Loss 0.7477 (Cls: 0.6402) LR: 0.000024
Batch 100: Loss 0.6578 (Cls: 0.5302) LR: 0.000024
Batch 110: Loss 0.8075 (Cls: 0.7104) LR: 0.000024
Batch 120: Loss 0.7922 (Cls: 0.6835) LR: 0.000024
Batch 130: Loss 0.6470 (Cls: 0.5325) LR: 0.000023
Batch 140: Loss 0.7591 (Cls: 0.6525) LR: 0.000023
Batch 150: Loss 0.7714 (Cls: 0.6596) LR: 0.000023
Batch 160: Loss 0.7776 (Cls: 0.6782) LR: 0.000023
Epoch 39: Loss 0.7542, Acc 0.7115
Val Loss 0.7706, Val Acc 0.7599
Batch 0: Loss 0.6394 (Cls: 0.5194) LR: 0.000022
Batch 10: Loss 0.8091 (Cls: 0.7076) LR: 0.000022
Batch 20: Loss 0.6652 (Cls: 0.5550) LR: 0.000022
Batch 30: Loss 0.7501 (Cls: 0.6311) LR: 0.000022
Batch 40: Loss 0.6829 (Cls: 0.5790) LR: 0.000022
Batch 50: Loss 0.8405 (Cls: 0.7327) LR: 0.000021
Batch 60: Loss 0.9672 (Cls: 0.8596) LR: 0.000021
Batch 70: Loss 0.8737 (Cls: 0.7665) LR: 0.000021
Batch 80: Loss 0.6826 (Cls: 0.5768) LR: 0.000021
Batch 90: Loss 0.7804 (Cls: 0.6817) LR: 0.000020
Batch 100: Loss 0.8771 (Cls: 0.7637) LR: 0.000020
Batch 110: Loss 0.6922 (Cls: 0.5832) LR: 0.000020
Batch 120: Loss 0.8547 (Cls: 0.7423) LR: 0.000020
Batch 130: Loss 0.7895 (Cls: 0.6864) LR: 0.000020
Batch 140: Loss 0.7091 (Cls: 0.5934) LR: 0.000019
Batch 150: Loss 0.7295 (Cls: 0.6129) LR: 0.000019
Batch 160: Loss 0.7162 (Cls: 0.6102) LR: 0.000019
Epoch 40: Loss 0.7429, Acc 0.7156
Val Loss 0.8519, Val Acc 0.6991
Batch 0: Loss 0.9343 (Cls: 0.8197) LR: 0.000019
Batch 10: Loss 0.8394 (Cls: 0.7247) LR: 0.000019
Batch 20: Loss 0.6881 (Cls: 0.5723) LR: 0.000018
Batch 30: Loss 0.7512 (Cls: 0.6318) LR: 0.000018
Batch 40: Loss 0.7494 (Cls: 0.6377) LR: 0.000018
Batch 50: Loss 0.7832 (Cls: 0.6716) LR: 0.000018
Batch 60: Loss 0.8288 (Cls: 0.7182) LR: 0.000018
Batch 70: Loss 0.6878 (Cls: 0.5766) LR: 0.000017
Batch 80: Loss 0.7304 (Cls: 0.6257) LR: 0.000017
Batch 90: Loss 0.7280 (Cls: 0.6181) LR: 0.000017
Batch 100: Loss 0.6790 (Cls: 0.5511) LR: 0.000017
Batch 110: Loss 0.7642 (Cls: 0.6558) LR: 0.000017
Batch 120: Loss 0.8239 (Cls: 0.7077) LR: 0.000016
Batch 130: Loss 0.7743 (Cls: 0.6524) LR: 0.000016
Batch 140: Loss 0.6156 (Cls: 0.4991) LR: 0.000016
Batch 150: Loss 0.7133 (Cls: 0.5983) LR: 0.000016
Batch 160: Loss 0.7071 (Cls: 0.5954) LR: 0.000016
Epoch 41: Loss 0.7354, Acc 0.7226
Val Loss 0.7914, Val Acc 0.7454
Traceback (most recent call last):
  File "/root/miniconda3/envs/fire_damage_cls/lib/python3.10/runpy.py", line 196, in _run_module_as_main
    return _run_code(code, main_globals, None,
  File "/root/miniconda3/envs/fire_damage_cls/lib/python3.10/runpy.py", line 86, in _run_code
    exec(code, run_globals)
  File "/root/shared-nvme/fp/src/training/train.py", line 349, in <module>
    main(args.config, args.exp_name)
  File "/root/shared-nvme/fp/src/training/train.py", line 270, in main
    loss, metrics = train_one_epoch(
  File "/root/shared-nvme/fp/src/training/train.py", line 141, in train_one_epoch
    outputs = model(imgs, coarse_txt, fine_txt)
  File "/root/miniconda3/envs/fire_damage_cls/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/fire_damage_cls/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/shared-nvme/fp/src/models/method.py", line 245, in forward
    z_fine, mu_f, logvar_f, recon_f = self.text_vae_fine(img, fine_text)
  File "/root/miniconda3/envs/fire_damage_cls/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/fire_damage_cls/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/shared-nvme/fp/src/models/method.py", line 43, in forward
    features = self.encoder(x)
  File "/root/miniconda3/envs/fire_damage_cls/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/fire_damage_cls/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/shared-nvme/fp/src/models/components/encoders.py", line 40, in forward
    return self.backbone(x)
  File "/root/miniconda3/envs/fire_damage_cls/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/fire_damage_cls/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/fire_damage_cls/lib/python3.10/site-packages/timm/models/vision_transformer.py", line 992, in forward
    x = self.forward_features(x, attn_mask=attn_mask)
  File "/root/miniconda3/envs/fire_damage_cls/lib/python3.10/site-packages/timm/models/vision_transformer.py", line 947, in forward_features
    x = self.blocks(x)
  File "/root/miniconda3/envs/fire_damage_cls/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/fire_damage_cls/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/fire_damage_cls/lib/python3.10/site-packages/torch/nn/modules/container.py", line 250, in forward
    input = module(input)
  File "/root/miniconda3/envs/fire_damage_cls/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/fire_damage_cls/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/fire_damage_cls/lib/python3.10/site-packages/timm/models/vision_transformer.py", line 156, in forward
    x = x + self.drop_path2(self.ls2(self.mlp(self.norm2(x))))
  File "/root/miniconda3/envs/fire_damage_cls/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/fire_damage_cls/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/fire_damage_cls/lib/python3.10/site-packages/timm/layers/mlp.py", line 52, in forward
    x = self.fc2(x)
  File "/root/miniconda3/envs/fire_damage_cls/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/root/miniconda3/envs/fire_damage_cls/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/root/miniconda3/envs/fire_damage_cls/lib/python3.10/site-packages/torch/nn/modules/linear.py", line 125, in forward
    return F.linear(input, self.weight, self.bias)
KeyboardInterrupt
Batch 0: Loss 0.8241 (Cls: 0.7087) LR: 0.000015
Batch 10: Loss 0.7202 (Cls: 0.6035) LR: 0.000015
Batch 20: Loss 0.7347 (Cls: 0.6224) LR: 0.000015
Batch 30: Loss 0.8218 (Cls: 0.7146) LR: 0.000015
Batch 40: Loss 0.7188 (Cls: 0.6075) LR: 0.000015
Batch 50: Loss 0.6602 (Cls: 0.5510) LR: 0.000014
Batch 60: Loss 0.6895 (Cls: 0.5858) LR: 0.000014
Batch 70: Loss 0.8836 (Cls: 0.7661) LR: 0.000014
Batch 80: Loss 0.7910 (Cls: 0.6838) LR: 0.000014
Batch 90: Loss 0.6832 (Cls: 0.5727) LR: 0.000014
