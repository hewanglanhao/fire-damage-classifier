Loaded config: {'data': {'batch_size': 32, 'image_size': 224, 'num_workers': 4, 'path': 'data/', 'seq_len': 50, 'vocab_size': 5000}, 'model': {'backbone': 'vit_base_patch16_224', 'latent_dim': 512, 'method_option': 'vae', 'num_classes': 5, 'type': 'baseline'}, 'project': {'name': 'FireDamageClassification', 'version': '1.0'}, 'training': {'drop_path_rate': 0.1, 'dropout': 0.2, 'epochs': 50, 'lambda_align': 0.1, 'lambda_cls': 1.0, 'lambda_vae': 0.1, 'lr': '1e-4', 'weight_decay': '1e-3'}}
Results will be saved to: results/1_baseline
Using device: cuda
Model Type: baseline
Splitting by Region:
Train Regions: ['GLA', 'VAL', 'MOS', 'CAS', 'MIL', 'POST', 'CRE', 'DIN', 'MON', 'SCU', 'FOR', 'BEU']
Val Regions: ['BEA', 'AUG', 'ZOG']
Test Regions: ['CAL', 'FAI', 'DIX']
Scanning images in data/image for mode train...
Loaded 5393 samples for mode train
Scanning images in data/image for mode val...
Loaded 1316 samples for mode val
Scanning images in data/image for mode test...
Loaded 1122 samples for mode test
Calculating class weights for WeightedRandomSampler...
Train Class Counts: {1: 204, 0: 2281, 4: 766, 3: 2060, 2: 82}
Data loaded: Train=5393, Val=1316, Test=1122
Parameters: 85,804,039
FLOPs: 16847738112.0
Starting training loop...
Batch 0: Loss 1.2265 (Cls: 1.2265) LR: 0.000004
Batch 10: Loss 1.2445 (Cls: 1.2445) LR: 0.000004
Batch 20: Loss 1.1348 (Cls: 1.1348) LR: 0.000004
Batch 30: Loss 1.1052 (Cls: 1.1052) LR: 0.000004
Batch 40: Loss 1.0741 (Cls: 1.0741) LR: 0.000004
Batch 50: Loss 1.0644 (Cls: 1.0644) LR: 0.000004
Batch 60: Loss 1.0809 (Cls: 1.0809) LR: 0.000004
Batch 70: Loss 1.2011 (Cls: 1.2011) LR: 0.000004
Batch 80: Loss 1.0896 (Cls: 1.0896) LR: 0.000004
Batch 90: Loss 1.1640 (Cls: 1.1640) LR: 0.000004
Batch 100: Loss 1.0432 (Cls: 1.0432) LR: 0.000004
Batch 110: Loss 1.0692 (Cls: 1.0692) LR: 0.000004
Batch 120: Loss 1.0048 (Cls: 1.0048) LR: 0.000005
Batch 130: Loss 1.0426 (Cls: 1.0426) LR: 0.000005
Batch 140: Loss 1.0664 (Cls: 1.0664) LR: 0.000005
Batch 150: Loss 1.0882 (Cls: 1.0882) LR: 0.000005
Batch 160: Loss 1.1113 (Cls: 1.1113) LR: 0.000005
Epoch 1: Loss 1.1263, Acc 0.2373
Val Loss 1.1075, Val Acc 0.3989
New best model saved with Val Acc: 0.3989
Batch 0: Loss 1.0947 (Cls: 1.0947) LR: 0.000005
Batch 10: Loss 1.1318 (Cls: 1.1318) LR: 0.000005
Batch 20: Loss 1.1479 (Cls: 1.1479) LR: 0.000005
Batch 30: Loss 1.1165 (Cls: 1.1165) LR: 0.000005
Batch 40: Loss 1.0274 (Cls: 1.0274) LR: 0.000006
Batch 50: Loss 1.0524 (Cls: 1.0524) LR: 0.000006
Batch 60: Loss 1.0581 (Cls: 1.0581) LR: 0.000006
Batch 70: Loss 1.1164 (Cls: 1.1164) LR: 0.000006
Batch 80: Loss 0.9152 (Cls: 0.9152) LR: 0.000006
Batch 90: Loss 1.0120 (Cls: 1.0120) LR: 0.000006
Batch 100: Loss 1.2033 (Cls: 1.2033) LR: 0.000007
Batch 110: Loss 1.0934 (Cls: 1.0934) LR: 0.000007
Batch 120: Loss 1.0858 (Cls: 1.0858) LR: 0.000007
Batch 130: Loss 1.0412 (Cls: 1.0412) LR: 0.000007
Batch 140: Loss 0.9563 (Cls: 0.9563) LR: 0.000008
Batch 150: Loss 1.1214 (Cls: 1.1214) LR: 0.000008
Batch 160: Loss 1.0311 (Cls: 1.0311) LR: 0.000008
Epoch 2: Loss 1.0818, Acc 0.2687
Val Loss 1.0898, Val Acc 0.3465
Batch 0: Loss 1.2219 (Cls: 1.2219) LR: 0.000008
Batch 10: Loss 1.1568 (Cls: 1.1568) LR: 0.000008
Batch 20: Loss 1.1071 (Cls: 1.1071) LR: 0.000009
Batch 30: Loss 1.0153 (Cls: 1.0153) LR: 0.000009
Batch 40: Loss 1.1443 (Cls: 1.1443) LR: 0.000009
Batch 50: Loss 1.1051 (Cls: 1.1051) LR: 0.000009
Batch 60: Loss 1.0037 (Cls: 1.0037) LR: 0.000010
Batch 70: Loss 1.0299 (Cls: 1.0299) LR: 0.000010
Batch 80: Loss 1.0600 (Cls: 1.0600) LR: 0.000010
Batch 90: Loss 0.9850 (Cls: 0.9850) LR: 0.000011
Batch 100: Loss 1.1734 (Cls: 1.1734) LR: 0.000011
Batch 110: Loss 1.2203 (Cls: 1.2203) LR: 0.000011
Batch 120: Loss 1.0534 (Cls: 1.0534) LR: 0.000012
Batch 130: Loss 1.0691 (Cls: 1.0691) LR: 0.000012
Batch 140: Loss 1.1088 (Cls: 1.1088) LR: 0.000012
Batch 150: Loss 1.1280 (Cls: 1.1280) LR: 0.000013
Batch 160: Loss 1.2104 (Cls: 1.2104) LR: 0.000013
Epoch 3: Loss 1.0781, Acc 0.3021
Val Loss 1.0918, Val Acc 0.3936
Batch 0: Loss 1.0931 (Cls: 1.0931) LR: 0.000013
Batch 10: Loss 1.0828 (Cls: 1.0828) LR: 0.000014
Batch 20: Loss 1.0374 (Cls: 1.0374) LR: 0.000014
Batch 30: Loss 0.9478 (Cls: 0.9478) LR: 0.000014
Batch 40: Loss 1.0497 (Cls: 1.0497) LR: 0.000015
Batch 50: Loss 1.0224 (Cls: 1.0224) LR: 0.000015
Batch 60: Loss 1.0656 (Cls: 1.0656) LR: 0.000015
Batch 70: Loss 1.0890 (Cls: 1.0890) LR: 0.000016
Batch 80: Loss 1.1078 (Cls: 1.1078) LR: 0.000016
Batch 90: Loss 1.0247 (Cls: 1.0247) LR: 0.000017
Batch 100: Loss 1.0912 (Cls: 1.0912) LR: 0.000017
Batch 110: Loss 1.0320 (Cls: 1.0320) LR: 0.000017
Batch 120: Loss 0.9821 (Cls: 0.9821) LR: 0.000018
Batch 130: Loss 0.9507 (Cls: 0.9507) LR: 0.000018
Batch 140: Loss 1.1558 (Cls: 1.1558) LR: 0.000019
Batch 150: Loss 0.9631 (Cls: 0.9631) LR: 0.000019
Batch 160: Loss 1.0071 (Cls: 1.0071) LR: 0.000020
Epoch 4: Loss 1.0581, Acc 0.3045
Val Loss 1.0078, Val Acc 0.4643
New best model saved with Val Acc: 0.4643
Batch 0: Loss 1.0096 (Cls: 1.0096) LR: 0.000020
Batch 10: Loss 1.1455 (Cls: 1.1455) LR: 0.000020
Batch 20: Loss 1.0876 (Cls: 1.0876) LR: 0.000021
Batch 30: Loss 1.0296 (Cls: 1.0296) LR: 0.000021
Batch 40: Loss 0.9745 (Cls: 0.9745) LR: 0.000022
Batch 50: Loss 1.1134 (Cls: 1.1134) LR: 0.000022
Batch 60: Loss 1.0415 (Cls: 1.0415) LR: 0.000023
Batch 70: Loss 0.9892 (Cls: 0.9892) LR: 0.000023
Batch 80: Loss 1.0799 (Cls: 1.0799) LR: 0.000024
Batch 90: Loss 1.0929 (Cls: 1.0929) LR: 0.000024
Batch 100: Loss 1.0634 (Cls: 1.0634) LR: 0.000025
Batch 110: Loss 0.9825 (Cls: 0.9825) LR: 0.000025
Batch 120: Loss 0.9861 (Cls: 0.9861) LR: 0.000026
Batch 130: Loss 0.9495 (Cls: 0.9495) LR: 0.000026
Batch 140: Loss 0.9407 (Cls: 0.9407) LR: 0.000027
Batch 150: Loss 0.9794 (Cls: 0.9794) LR: 0.000027
Batch 160: Loss 0.9245 (Cls: 0.9245) LR: 0.000028
Epoch 5: Loss 1.0187, Acc 0.3334
Val Loss 1.0712, Val Acc 0.4711
New best model saved with Val Acc: 0.4711
Batch 0: Loss 1.1086 (Cls: 1.1086) LR: 0.000028
Batch 10: Loss 1.1043 (Cls: 1.1043) LR: 0.000029
Batch 20: Loss 1.0676 (Cls: 1.0676) LR: 0.000029
Batch 30: Loss 1.0530 (Cls: 1.0530) LR: 0.000030
Batch 40: Loss 1.0530 (Cls: 1.0530) LR: 0.000030
Batch 50: Loss 1.1196 (Cls: 1.1196) LR: 0.000031
Batch 60: Loss 1.0822 (Cls: 1.0822) LR: 0.000031
Batch 70: Loss 1.1038 (Cls: 1.1038) LR: 0.000032
Batch 80: Loss 1.0193 (Cls: 1.0193) LR: 0.000032
Batch 90: Loss 1.0620 (Cls: 1.0620) LR: 0.000033
Batch 100: Loss 0.9794 (Cls: 0.9794) LR: 0.000033
Batch 110: Loss 1.0659 (Cls: 1.0659) LR: 0.000034
Batch 120: Loss 1.0550 (Cls: 1.0550) LR: 0.000035
Batch 130: Loss 1.1022 (Cls: 1.1022) LR: 0.000035
Batch 140: Loss 0.9039 (Cls: 0.9039) LR: 0.000036
Batch 150: Loss 1.0797 (Cls: 1.0797) LR: 0.000036
Batch 160: Loss 0.8949 (Cls: 0.8949) LR: 0.000037
Epoch 6: Loss 1.0100, Acc 0.3529
Val Loss 0.9257, Val Acc 0.5600
New best model saved with Val Acc: 0.5600
Batch 0: Loss 0.9242 (Cls: 0.9242) LR: 0.000037
Batch 10: Loss 1.1262 (Cls: 1.1262) LR: 0.000038
Batch 20: Loss 1.0390 (Cls: 1.0390) LR: 0.000038
Batch 30: Loss 0.9885 (Cls: 0.9885) LR: 0.000039
Batch 40: Loss 0.8868 (Cls: 0.8868) LR: 0.000040
Batch 50: Loss 0.9027 (Cls: 0.9027) LR: 0.000040
Batch 60: Loss 1.2013 (Cls: 1.2013) LR: 0.000041
Batch 70: Loss 1.1260 (Cls: 1.1260) LR: 0.000041
Batch 80: Loss 1.0665 (Cls: 1.0665) LR: 0.000042
Batch 90: Loss 1.0384 (Cls: 1.0384) LR: 0.000042
Batch 100: Loss 0.9811 (Cls: 0.9811) LR: 0.000043
Batch 110: Loss 1.0987 (Cls: 1.0987) LR: 0.000044
Batch 120: Loss 0.9243 (Cls: 0.9243) LR: 0.000044
Batch 130: Loss 1.0879 (Cls: 1.0879) LR: 0.000045
Batch 140: Loss 1.0545 (Cls: 1.0545) LR: 0.000045
Batch 150: Loss 0.9603 (Cls: 0.9603) LR: 0.000046
Batch 160: Loss 0.9532 (Cls: 0.9532) LR: 0.000047
Epoch 7: Loss 0.9971, Acc 0.3746
Val Loss 0.9443, Val Acc 0.5631
New best model saved with Val Acc: 0.5631
Batch 0: Loss 0.8131 (Cls: 0.8131) LR: 0.000047
Batch 10: Loss 0.9805 (Cls: 0.9805) LR: 0.000048
Batch 20: Loss 1.0918 (Cls: 1.0918) LR: 0.000048
Batch 30: Loss 0.9456 (Cls: 0.9456) LR: 0.000049
Batch 40: Loss 0.9748 (Cls: 0.9748) LR: 0.000049
Batch 50: Loss 0.9783 (Cls: 0.9783) LR: 0.000050
Batch 60: Loss 0.9406 (Cls: 0.9406) LR: 0.000051
Batch 70: Loss 0.9324 (Cls: 0.9324) LR: 0.000051
Batch 80: Loss 0.9188 (Cls: 0.9188) LR: 0.000052
Batch 90: Loss 0.9299 (Cls: 0.9299) LR: 0.000052
Batch 100: Loss 0.9187 (Cls: 0.9187) LR: 0.000053
Batch 110: Loss 0.8591 (Cls: 0.8591) LR: 0.000054
Batch 120: Loss 0.8955 (Cls: 0.8955) LR: 0.000054
Batch 130: Loss 0.9436 (Cls: 0.9436) LR: 0.000055
Batch 140: Loss 0.9992 (Cls: 0.9992) LR: 0.000055
Batch 150: Loss 0.8249 (Cls: 0.8249) LR: 0.000056
Batch 160: Loss 0.9742 (Cls: 0.9742) LR: 0.000057
Epoch 8: Loss 0.9619, Acc 0.4178
Val Loss 0.9611, Val Acc 0.4772
Batch 0: Loss 0.9259 (Cls: 0.9259) LR: 0.000057
Batch 10: Loss 1.0527 (Cls: 1.0527) LR: 0.000058
Batch 20: Loss 0.8946 (Cls: 0.8946) LR: 0.000058
Batch 30: Loss 0.9029 (Cls: 0.9029) LR: 0.000059
Batch 40: Loss 1.0219 (Cls: 1.0219) LR: 0.000059
Batch 50: Loss 0.9760 (Cls: 0.9760) LR: 0.000060
Batch 60: Loss 0.9711 (Cls: 0.9711) LR: 0.000061
Batch 70: Loss 0.9150 (Cls: 0.9150) LR: 0.000061
Batch 80: Loss 1.0398 (Cls: 1.0398) LR: 0.000062
Batch 90: Loss 0.9539 (Cls: 0.9539) LR: 0.000062
Batch 100: Loss 0.9497 (Cls: 0.9497) LR: 0.000063
Batch 110: Loss 0.9249 (Cls: 0.9249) LR: 0.000064
Batch 120: Loss 0.9870 (Cls: 0.9870) LR: 0.000064
Batch 130: Loss 0.8979 (Cls: 0.8979) LR: 0.000065
Batch 140: Loss 0.9182 (Cls: 0.9182) LR: 0.000065
Batch 150: Loss 0.9678 (Cls: 0.9678) LR: 0.000066
Batch 160: Loss 1.0306 (Cls: 1.0306) LR: 0.000066
Epoch 9: Loss 0.9568, Acc 0.4165
Val Loss 1.0352, Val Acc 0.3936
Batch 0: Loss 0.8482 (Cls: 0.8482) LR: 0.000067
Batch 10: Loss 0.8564 (Cls: 0.8564) LR: 0.000067
Batch 20: Loss 0.9493 (Cls: 0.9493) LR: 0.000068
Batch 30: Loss 1.0435 (Cls: 1.0435) LR: 0.000069
Batch 40: Loss 1.0592 (Cls: 1.0592) LR: 0.000069
Batch 50: Loss 0.9189 (Cls: 0.9189) LR: 0.000070
Batch 60: Loss 0.9285 (Cls: 0.9285) LR: 0.000070
Batch 70: Loss 1.0199 (Cls: 1.0199) LR: 0.000071
Batch 80: Loss 0.9722 (Cls: 0.9722) LR: 0.000071
Batch 90: Loss 0.8612 (Cls: 0.8612) LR: 0.000072
Batch 100: Loss 0.9079 (Cls: 0.9079) LR: 0.000072
Batch 110: Loss 0.9197 (Cls: 0.9197) LR: 0.000073
Batch 120: Loss 0.9686 (Cls: 0.9686) LR: 0.000074
Batch 130: Loss 0.8654 (Cls: 0.8654) LR: 0.000074
Batch 140: Loss 1.0600 (Cls: 1.0600) LR: 0.000075
Batch 150: Loss 1.0083 (Cls: 1.0083) LR: 0.000075
Batch 160: Loss 0.9512 (Cls: 0.9512) LR: 0.000076
Epoch 10: Loss 0.9458, Acc 0.4259
Val Loss 0.9230, Val Acc 0.5365
Batch 0: Loss 0.9429 (Cls: 0.9429) LR: 0.000076
Batch 10: Loss 0.8988 (Cls: 0.8988) LR: 0.000077
Batch 20: Loss 1.0751 (Cls: 1.0751) LR: 0.000077
Batch 30: Loss 0.9055 (Cls: 0.9055) LR: 0.000078
Batch 40: Loss 0.8558 (Cls: 0.8558) LR: 0.000078
Batch 50: Loss 0.9359 (Cls: 0.9359) LR: 0.000079
Batch 60: Loss 0.9231 (Cls: 0.9231) LR: 0.000079
Batch 70: Loss 0.8612 (Cls: 0.8612) LR: 0.000080
Batch 80: Loss 0.8578 (Cls: 0.8578) LR: 0.000080
Batch 90: Loss 1.0391 (Cls: 1.0391) LR: 0.000081
Batch 100: Loss 0.8557 (Cls: 0.8557) LR: 0.000081
Batch 110: Loss 0.9743 (Cls: 0.9743) LR: 0.000082
Batch 120: Loss 0.9423 (Cls: 0.9423) LR: 0.000082
Batch 130: Loss 0.8299 (Cls: 0.8299) LR: 0.000082
Batch 140: Loss 1.1012 (Cls: 1.1012) LR: 0.000083
Batch 150: Loss 0.9473 (Cls: 0.9473) LR: 0.000083
Batch 160: Loss 1.1053 (Cls: 1.1053) LR: 0.000084
Epoch 11: Loss 0.9377, Acc 0.4419
Val Loss 0.9362, Val Acc 0.4704
Batch 0: Loss 0.8997 (Cls: 0.8997) LR: 0.000084
Batch 10: Loss 0.9815 (Cls: 0.9815) LR: 0.000085
Batch 20: Loss 0.9890 (Cls: 0.9890) LR: 0.000085
Batch 30: Loss 0.9377 (Cls: 0.9377) LR: 0.000085
Batch 40: Loss 0.9839 (Cls: 0.9839) LR: 0.000086
Batch 50: Loss 0.9544 (Cls: 0.9544) LR: 0.000086
Batch 60: Loss 0.9253 (Cls: 0.9253) LR: 0.000087
Batch 70: Loss 0.9223 (Cls: 0.9223) LR: 0.000087
Batch 80: Loss 1.0310 (Cls: 1.0310) LR: 0.000088
Batch 90: Loss 0.8309 (Cls: 0.8309) LR: 0.000088
Batch 100: Loss 0.9448 (Cls: 0.9448) LR: 0.000088
Batch 110: Loss 0.8793 (Cls: 0.8793) LR: 0.000089
Batch 120: Loss 0.9146 (Cls: 0.9146) LR: 0.000089
Batch 130: Loss 0.8987 (Cls: 0.8987) LR: 0.000089
Batch 140: Loss 0.8750 (Cls: 0.8750) LR: 0.000090
Batch 150: Loss 0.8600 (Cls: 0.8600) LR: 0.000090
Batch 160: Loss 1.0085 (Cls: 1.0085) LR: 0.000091
Epoch 12: Loss 0.9365, Acc 0.4408
Val Loss 0.9662, Val Acc 0.5000
Batch 0: Loss 0.9856 (Cls: 0.9856) LR: 0.000091
Batch 10: Loss 0.9033 (Cls: 0.9033) LR: 0.000091
Batch 20: Loss 0.9379 (Cls: 0.9379) LR: 0.000092
Batch 30: Loss 0.8161 (Cls: 0.8161) LR: 0.000092
Batch 40: Loss 0.9937 (Cls: 0.9937) LR: 0.000092
Batch 50: Loss 0.9810 (Cls: 0.9810) LR: 0.000093
Batch 60: Loss 1.0013 (Cls: 1.0013) LR: 0.000093
Batch 70: Loss 0.9177 (Cls: 0.9177) LR: 0.000093
Batch 80: Loss 0.9028 (Cls: 0.9028) LR: 0.000093
Batch 90: Loss 1.0520 (Cls: 1.0520) LR: 0.000094
Batch 100: Loss 0.9486 (Cls: 0.9486) LR: 0.000094
Batch 110: Loss 0.9558 (Cls: 0.9558) LR: 0.000094
Batch 120: Loss 0.9431 (Cls: 0.9431) LR: 0.000095
Batch 130: Loss 0.9749 (Cls: 0.9749) LR: 0.000095
Batch 140: Loss 0.7528 (Cls: 0.7528) LR: 0.000095
Batch 150: Loss 0.9960 (Cls: 0.9960) LR: 0.000095
Batch 160: Loss 1.0455 (Cls: 1.0455) LR: 0.000096
Epoch 13: Loss 0.9168, Acc 0.4639
Val Loss 0.9369, Val Acc 0.5410
Batch 0: Loss 0.8332 (Cls: 0.8332) LR: 0.000096
Batch 10: Loss 0.7620 (Cls: 0.7620) LR: 0.000096
Batch 20: Loss 0.9430 (Cls: 0.9430) LR: 0.000096
Batch 30: Loss 0.9422 (Cls: 0.9422) LR: 0.000097
Batch 40: Loss 0.9052 (Cls: 0.9052) LR: 0.000097
Batch 50: Loss 1.0570 (Cls: 1.0570) LR: 0.000097
Batch 60: Loss 0.9162 (Cls: 0.9162) LR: 0.000097
Batch 70: Loss 0.8227 (Cls: 0.8227) LR: 0.000097
Batch 80: Loss 0.7900 (Cls: 0.7900) LR: 0.000098
Batch 90: Loss 0.9183 (Cls: 0.9183) LR: 0.000098
Batch 100: Loss 0.8305 (Cls: 0.8305) LR: 0.000098
Batch 110: Loss 0.8573 (Cls: 0.8573) LR: 0.000098
Batch 120: Loss 0.8253 (Cls: 0.8253) LR: 0.000098
Batch 130: Loss 0.9018 (Cls: 0.9018) LR: 0.000098
Batch 140: Loss 1.0250 (Cls: 1.0250) LR: 0.000099
Batch 150: Loss 0.9893 (Cls: 0.9893) LR: 0.000099
Batch 160: Loss 0.9473 (Cls: 0.9473) LR: 0.000099
Epoch 14: Loss 0.9148, Acc 0.4637
Val Loss 0.7667, Val Acc 0.6824
New best model saved with Val Acc: 0.6824
Batch 0: Loss 0.9365 (Cls: 0.9365) LR: 0.000099
Batch 10: Loss 0.8445 (Cls: 0.8445) LR: 0.000099
Batch 20: Loss 0.8268 (Cls: 0.8268) LR: 0.000099
Batch 30: Loss 0.8103 (Cls: 0.8103) LR: 0.000099
Batch 40: Loss 0.9828 (Cls: 0.9828) LR: 0.000099
Batch 50: Loss 1.0359 (Cls: 1.0359) LR: 0.000099
Batch 60: Loss 0.8166 (Cls: 0.8166) LR: 0.000100
Batch 70: Loss 0.8345 (Cls: 0.8345) LR: 0.000100
Batch 80: Loss 0.8198 (Cls: 0.8198) LR: 0.000100
Batch 90: Loss 0.7795 (Cls: 0.7795) LR: 0.000100
Batch 100: Loss 0.8759 (Cls: 0.8759) LR: 0.000100
Batch 110: Loss 0.7959 (Cls: 0.7959) LR: 0.000100
Batch 120: Loss 0.8896 (Cls: 0.8896) LR: 0.000100
Batch 130: Loss 0.9187 (Cls: 0.9187) LR: 0.000100
Batch 140: Loss 0.9280 (Cls: 0.9280) LR: 0.000100
Batch 150: Loss 1.0185 (Cls: 1.0185) LR: 0.000100
Batch 160: Loss 0.9412 (Cls: 0.9412) LR: 0.000100
Epoch 15: Loss 0.8934, Acc 0.4884
Val Loss 0.8251, Val Acc 0.6406
Batch 0: Loss 0.9125 (Cls: 0.9125) LR: 0.000100
Batch 10: Loss 0.8194 (Cls: 0.8194) LR: 0.000100
Batch 20: Loss 0.7979 (Cls: 0.7979) LR: 0.000100
Batch 30: Loss 0.9007 (Cls: 0.9007) LR: 0.000100
Batch 40: Loss 0.9014 (Cls: 0.9014) LR: 0.000100
Batch 50: Loss 0.9166 (Cls: 0.9166) LR: 0.000100
Batch 60: Loss 1.0542 (Cls: 1.0542) LR: 0.000100
Batch 70: Loss 0.7200 (Cls: 0.7200) LR: 0.000100
Batch 80: Loss 0.7973 (Cls: 0.7973) LR: 0.000100
Batch 90: Loss 0.8513 (Cls: 0.8513) LR: 0.000100
Batch 100: Loss 0.8764 (Cls: 0.8764) LR: 0.000100
Batch 110: Loss 1.0347 (Cls: 1.0347) LR: 0.000100
Batch 120: Loss 0.9783 (Cls: 0.9783) LR: 0.000100
Batch 130: Loss 0.9643 (Cls: 0.9643) LR: 0.000100
Batch 140: Loss 0.9337 (Cls: 0.9337) LR: 0.000100
Batch 150: Loss 0.8791 (Cls: 0.8791) LR: 0.000100
Batch 160: Loss 0.9585 (Cls: 0.9585) LR: 0.000100
Epoch 16: Loss 0.8815, Acc 0.5019
Val Loss 0.9837, Val Acc 0.5236
Batch 0: Loss 0.9226 (Cls: 0.9226) LR: 0.000100
Batch 10: Loss 0.8323 (Cls: 0.8323) LR: 0.000100
Batch 20: Loss 0.7752 (Cls: 0.7752) LR: 0.000100
Batch 30: Loss 0.8015 (Cls: 0.8015) LR: 0.000100
Batch 40: Loss 0.8541 (Cls: 0.8541) LR: 0.000100
Batch 50: Loss 0.8815 (Cls: 0.8815) LR: 0.000100
Batch 60: Loss 0.8817 (Cls: 0.8817) LR: 0.000100
Batch 70: Loss 0.8153 (Cls: 0.8153) LR: 0.000100
Batch 80: Loss 0.9030 (Cls: 0.9030) LR: 0.000100
Batch 90: Loss 0.7903 (Cls: 0.7903) LR: 0.000100
Batch 100: Loss 0.8607 (Cls: 0.8607) LR: 0.000099
Batch 110: Loss 0.9088 (Cls: 0.9088) LR: 0.000099
Batch 120: Loss 0.7433 (Cls: 0.7433) LR: 0.000099
Batch 130: Loss 0.9909 (Cls: 0.9909) LR: 0.000099
Batch 140: Loss 0.9243 (Cls: 0.9243) LR: 0.000099
Batch 150: Loss 0.8749 (Cls: 0.8749) LR: 0.000099
Batch 160: Loss 0.9371 (Cls: 0.9371) LR: 0.000099
Epoch 17: Loss 0.8815, Acc 0.4997
Val Loss 0.8113, Val Acc 0.6413
Batch 0: Loss 0.8170 (Cls: 0.8170) LR: 0.000099
Batch 10: Loss 0.8500 (Cls: 0.8500) LR: 0.000099
Batch 20: Loss 1.0693 (Cls: 1.0693) LR: 0.000099
Batch 30: Loss 0.8124 (Cls: 0.8124) LR: 0.000099
Batch 40: Loss 0.7776 (Cls: 0.7776) LR: 0.000099
Batch 50: Loss 1.1436 (Cls: 1.1436) LR: 0.000099
Batch 60: Loss 0.9325 (Cls: 0.9325) LR: 0.000099
Batch 70: Loss 0.8573 (Cls: 0.8573) LR: 0.000099
Batch 80: Loss 0.8267 (Cls: 0.8267) LR: 0.000099
Batch 90: Loss 1.0095 (Cls: 1.0095) LR: 0.000099
Batch 100: Loss 0.7766 (Cls: 0.7766) LR: 0.000099
Batch 110: Loss 0.7907 (Cls: 0.7907) LR: 0.000099
Batch 120: Loss 0.7779 (Cls: 0.7779) LR: 0.000099
Batch 130: Loss 1.0083 (Cls: 1.0083) LR: 0.000098
Batch 140: Loss 0.7925 (Cls: 0.7925) LR: 0.000098
Batch 150: Loss 0.8804 (Cls: 0.8804) LR: 0.000098
Batch 160: Loss 0.8184 (Cls: 0.8184) LR: 0.000098
Epoch 18: Loss 0.8675, Acc 0.5157
Val Loss 0.8540, Val Acc 0.6109
Batch 0: Loss 0.8083 (Cls: 0.8083) LR: 0.000098
Batch 10: Loss 0.8344 (Cls: 0.8344) LR: 0.000098
Batch 20: Loss 1.0462 (Cls: 1.0462) LR: 0.000098
Batch 30: Loss 0.8849 (Cls: 0.8849) LR: 0.000098
Batch 40: Loss 0.7475 (Cls: 0.7475) LR: 0.000098
Batch 50: Loss 0.7842 (Cls: 0.7842) LR: 0.000098
Batch 60: Loss 0.8630 (Cls: 0.8630) LR: 0.000098
Batch 70: Loss 0.8275 (Cls: 0.8275) LR: 0.000098
Batch 80: Loss 0.9307 (Cls: 0.9307) LR: 0.000098
Batch 90: Loss 0.8761 (Cls: 0.8761) LR: 0.000097
Batch 100: Loss 0.8139 (Cls: 0.8139) LR: 0.000097
Batch 110: Loss 0.8406 (Cls: 0.8406) LR: 0.000097
Batch 120: Loss 0.9146 (Cls: 0.9146) LR: 0.000097
Batch 130: Loss 0.8071 (Cls: 0.8071) LR: 0.000097
Batch 140: Loss 0.9904 (Cls: 0.9904) LR: 0.000097
Batch 150: Loss 0.9363 (Cls: 0.9363) LR: 0.000097
Batch 160: Loss 0.8887 (Cls: 0.8887) LR: 0.000097
Epoch 19: Loss 0.8609, Acc 0.5303
Val Loss 0.8705, Val Acc 0.5722
Batch 0: Loss 0.6951 (Cls: 0.6951) LR: 0.000097
Batch 10: Loss 0.8343 (Cls: 0.8343) LR: 0.000097
Batch 20: Loss 0.8317 (Cls: 0.8317) LR: 0.000097
Batch 30: Loss 0.9198 (Cls: 0.9198) LR: 0.000097
Batch 40: Loss 0.8375 (Cls: 0.8375) LR: 0.000096
Batch 50: Loss 1.0292 (Cls: 1.0292) LR: 0.000096
Batch 60: Loss 0.7904 (Cls: 0.7904) LR: 0.000096
Batch 70: Loss 0.8539 (Cls: 0.8539) LR: 0.000096
Batch 80: Loss 0.8531 (Cls: 0.8531) LR: 0.000096
Batch 90: Loss 0.7518 (Cls: 0.7518) LR: 0.000096
Batch 100: Loss 0.7762 (Cls: 0.7762) LR: 0.000096
Batch 110: Loss 0.8543 (Cls: 0.8543) LR: 0.000096
Batch 120: Loss 0.7469 (Cls: 0.7469) LR: 0.000096
Batch 130: Loss 0.8033 (Cls: 0.8033) LR: 0.000095
Batch 140: Loss 0.7323 (Cls: 0.7323) LR: 0.000095
Batch 150: Loss 0.9812 (Cls: 0.9812) LR: 0.000095
Batch 160: Loss 0.8869 (Cls: 0.8869) LR: 0.000095
Epoch 20: Loss 0.8516, Acc 0.5379
Val Loss 0.7629, Val Acc 0.6938
New best model saved with Val Acc: 0.6938
Batch 0: Loss 0.8934 (Cls: 0.8934) LR: 0.000095
Batch 10: Loss 0.8727 (Cls: 0.8727) LR: 0.000095
Batch 20: Loss 0.7570 (Cls: 0.7570) LR: 0.000095
Batch 30: Loss 0.8733 (Cls: 0.8733) LR: 0.000095
Batch 40: Loss 0.9106 (Cls: 0.9106) LR: 0.000095
Batch 50: Loss 0.8353 (Cls: 0.8353) LR: 0.000094
Batch 60: Loss 0.7481 (Cls: 0.7481) LR: 0.000094
Batch 70: Loss 0.7791 (Cls: 0.7791) LR: 0.000094
Batch 80: Loss 0.8156 (Cls: 0.8156) LR: 0.000094
Batch 90: Loss 0.8050 (Cls: 0.8050) LR: 0.000094
Batch 100: Loss 0.8945 (Cls: 0.8945) LR: 0.000094
Batch 110: Loss 0.7821 (Cls: 0.7821) LR: 0.000094
Batch 120: Loss 0.7408 (Cls: 0.7408) LR: 0.000094
Batch 130: Loss 0.7960 (Cls: 0.7960) LR: 0.000093
Batch 140: Loss 0.8696 (Cls: 0.8696) LR: 0.000093
Batch 150: Loss 0.8540 (Cls: 0.8540) LR: 0.000093
Batch 160: Loss 0.8682 (Cls: 0.8682) LR: 0.000093
Epoch 21: Loss 0.8299, Acc 0.5565
Val Loss 1.0490, Val Acc 0.4970
Batch 0: Loss 0.8683 (Cls: 0.8683) LR: 0.000093
Batch 10: Loss 0.7801 (Cls: 0.7801) LR: 0.000093
Batch 20: Loss 0.7758 (Cls: 0.7758) LR: 0.000093
Batch 30: Loss 0.9790 (Cls: 0.9790) LR: 0.000092
Batch 40: Loss 0.7261 (Cls: 0.7261) LR: 0.000092
Batch 50: Loss 0.8964 (Cls: 0.8964) LR: 0.000092
Batch 60: Loss 0.7660 (Cls: 0.7660) LR: 0.000092
Batch 70: Loss 0.9275 (Cls: 0.9275) LR: 0.000092
Batch 80: Loss 0.8939 (Cls: 0.8939) LR: 0.000092
Batch 90: Loss 0.7750 (Cls: 0.7750) LR: 0.000092
Batch 100: Loss 1.0532 (Cls: 1.0532) LR: 0.000091
Batch 110: Loss 0.7284 (Cls: 0.7284) LR: 0.000091
Batch 120: Loss 0.8519 (Cls: 0.8519) LR: 0.000091
Batch 130: Loss 0.8137 (Cls: 0.8137) LR: 0.000091
Batch 140: Loss 0.8495 (Cls: 0.8495) LR: 0.000091
Batch 150: Loss 0.8462 (Cls: 0.8462) LR: 0.000091
Batch 160: Loss 0.8344 (Cls: 0.8344) LR: 0.000091
Epoch 22: Loss 0.8349, Acc 0.5555
Val Loss 0.8832, Val Acc 0.5691
Batch 0: Loss 0.8491 (Cls: 0.8491) LR: 0.000090
Batch 10: Loss 0.8644 (Cls: 0.8644) LR: 0.000090
Batch 20: Loss 0.8109 (Cls: 0.8109) LR: 0.000090
Batch 30: Loss 0.8309 (Cls: 0.8309) LR: 0.000090
Batch 40: Loss 0.7128 (Cls: 0.7128) LR: 0.000090
Batch 50: Loss 0.7321 (Cls: 0.7321) LR: 0.000090
Batch 60: Loss 0.8691 (Cls: 0.8691) LR: 0.000089
Batch 70: Loss 0.7455 (Cls: 0.7455) LR: 0.000089
Batch 80: Loss 0.9884 (Cls: 0.9884) LR: 0.000089
Batch 90: Loss 0.8845 (Cls: 0.8845) LR: 0.000089
Batch 100: Loss 1.0333 (Cls: 1.0333) LR: 0.000089
Batch 110: Loss 0.7842 (Cls: 0.7842) LR: 0.000089
Batch 120: Loss 0.7702 (Cls: 0.7702) LR: 0.000088
Batch 130: Loss 0.9281 (Cls: 0.9281) LR: 0.000088
Batch 140: Loss 0.8608 (Cls: 0.8608) LR: 0.000088
Batch 150: Loss 0.8381 (Cls: 0.8381) LR: 0.000088
Batch 160: Loss 0.7902 (Cls: 0.7902) LR: 0.000088
Epoch 23: Loss 0.8269, Acc 0.5615
Val Loss 0.8425, Val Acc 0.5912
Batch 0: Loss 0.8664 (Cls: 0.8664) LR: 0.000088
Batch 10: Loss 0.8783 (Cls: 0.8783) LR: 0.000087
Batch 20: Loss 0.8659 (Cls: 0.8659) LR: 0.000087
Batch 30: Loss 0.7306 (Cls: 0.7306) LR: 0.000087
Batch 40: Loss 0.9055 (Cls: 0.9055) LR: 0.000087
Batch 50: Loss 0.7973 (Cls: 0.7973) LR: 0.000087
Batch 60: Loss 0.7807 (Cls: 0.7807) LR: 0.000087
Batch 70: Loss 0.7951 (Cls: 0.7951) LR: 0.000086
Batch 80: Loss 0.7244 (Cls: 0.7244) LR: 0.000086
Batch 90: Loss 0.7292 (Cls: 0.7292) LR: 0.000086
Batch 100: Loss 0.8511 (Cls: 0.8511) LR: 0.000086
Batch 110: Loss 0.8232 (Cls: 0.8232) LR: 0.000086
Batch 120: Loss 0.8684 (Cls: 0.8684) LR: 0.000085
Batch 130: Loss 0.7137 (Cls: 0.7137) LR: 0.000085
Batch 140: Loss 0.8957 (Cls: 0.8957) LR: 0.000085
Batch 150: Loss 0.6294 (Cls: 0.6294) LR: 0.000085
Batch 160: Loss 0.9294 (Cls: 0.9294) LR: 0.000085
Epoch 24: Loss 0.8161, Acc 0.5668
Val Loss 0.8400, Val Acc 0.6147
Batch 0: Loss 0.6945 (Cls: 0.6945) LR: 0.000085
Batch 10: Loss 0.8144 (Cls: 0.8144) LR: 0.000084
Batch 20: Loss 0.8280 (Cls: 0.8280) LR: 0.000084
Batch 30: Loss 0.8146 (Cls: 0.8146) LR: 0.000084
Batch 40: Loss 0.7090 (Cls: 0.7090) LR: 0.000084
Batch 50: Loss 0.7834 (Cls: 0.7834) LR: 0.000084
Batch 60: Loss 0.8448 (Cls: 0.8448) LR: 0.000083
Batch 70: Loss 0.9381 (Cls: 0.9381) LR: 0.000083
Batch 80: Loss 0.7749 (Cls: 0.7749) LR: 0.000083
Batch 90: Loss 0.8961 (Cls: 0.8961) LR: 0.000083
Batch 100: Loss 0.8016 (Cls: 0.8016) LR: 0.000083
Batch 110: Loss 0.8992 (Cls: 0.8992) LR: 0.000082
Batch 120: Loss 0.8417 (Cls: 0.8417) LR: 0.000082
Batch 130: Loss 0.6266 (Cls: 0.6266) LR: 0.000082
Batch 140: Loss 0.7498 (Cls: 0.7498) LR: 0.000082
Batch 150: Loss 0.8967 (Cls: 0.8967) LR: 0.000082
Batch 160: Loss 0.6783 (Cls: 0.6783) LR: 0.000081
Epoch 25: Loss 0.7985, Acc 0.5900
Val Loss 0.7925, Val Acc 0.6611
Batch 0: Loss 0.6612 (Cls: 0.6612) LR: 0.000081
Batch 10: Loss 0.7625 (Cls: 0.7625) LR: 0.000081
Batch 20: Loss 0.7590 (Cls: 0.7590) LR: 0.000081
Batch 30: Loss 0.6795 (Cls: 0.6795) LR: 0.000081
Batch 40: Loss 0.8130 (Cls: 0.8130) LR: 0.000080
Batch 50: Loss 0.9760 (Cls: 0.9760) LR: 0.000080
Batch 60: Loss 0.7832 (Cls: 0.7832) LR: 0.000080
Batch 70: Loss 0.7185 (Cls: 0.7185) LR: 0.000080
Batch 80: Loss 0.7740 (Cls: 0.7740) LR: 0.000079
Batch 90: Loss 0.8173 (Cls: 0.8173) LR: 0.000079
Batch 100: Loss 0.8353 (Cls: 0.8353) LR: 0.000079
Batch 110: Loss 0.8660 (Cls: 0.8660) LR: 0.000079
Batch 120: Loss 0.7410 (Cls: 0.7410) LR: 0.000079
Batch 130: Loss 0.6673 (Cls: 0.6673) LR: 0.000078
Batch 140: Loss 0.9101 (Cls: 0.9101) LR: 0.000078
Batch 150: Loss 0.6522 (Cls: 0.6522) LR: 0.000078
Batch 160: Loss 0.6781 (Cls: 0.6781) LR: 0.000078
Epoch 26: Loss 0.7907, Acc 0.5950
Val Loss 0.7726, Val Acc 0.6809
Batch 0: Loss 0.8397 (Cls: 0.8397) LR: 0.000078
Batch 10: Loss 0.9827 (Cls: 0.9827) LR: 0.000077
Batch 20: Loss 0.6850 (Cls: 0.6850) LR: 0.000077
Batch 30: Loss 0.7572 (Cls: 0.7572) LR: 0.000077
Batch 40: Loss 0.6972 (Cls: 0.6972) LR: 0.000077
Batch 50: Loss 0.8022 (Cls: 0.8022) LR: 0.000076
Batch 60: Loss 0.7732 (Cls: 0.7732) LR: 0.000076
Batch 70: Loss 0.9379 (Cls: 0.9379) LR: 0.000076
Batch 80: Loss 0.8575 (Cls: 0.8575) LR: 0.000076
Batch 90: Loss 0.7563 (Cls: 0.7563) LR: 0.000075
Batch 100: Loss 0.6485 (Cls: 0.6485) LR: 0.000075
Batch 110: Loss 0.8896 (Cls: 0.8896) LR: 0.000075
Batch 120: Loss 0.7132 (Cls: 0.7132) LR: 0.000075
Batch 130: Loss 0.9787 (Cls: 0.9787) LR: 0.000075
Batch 140: Loss 0.7303 (Cls: 0.7303) LR: 0.000074
Batch 150: Loss 0.8040 (Cls: 0.8040) LR: 0.000074
Batch 160: Loss 0.6831 (Cls: 0.6831) LR: 0.000074
Epoch 27: Loss 0.7755, Acc 0.6095
Val Loss 0.8549, Val Acc 0.5942
Batch 0: Loss 0.7039 (Cls: 0.7039) LR: 0.000074
Batch 10: Loss 0.6625 (Cls: 0.6625) LR: 0.000073
Batch 20: Loss 0.7481 (Cls: 0.7481) LR: 0.000073
Batch 30: Loss 0.8219 (Cls: 0.8219) LR: 0.000073
Batch 40: Loss 0.7470 (Cls: 0.7470) LR: 0.000073
Batch 50: Loss 0.9408 (Cls: 0.9408) LR: 0.000072
Batch 60: Loss 0.7136 (Cls: 0.7136) LR: 0.000072
Batch 70: Loss 1.0047 (Cls: 1.0047) LR: 0.000072
Batch 80: Loss 0.8636 (Cls: 0.8636) LR: 0.000072
Batch 90: Loss 0.7936 (Cls: 0.7936) LR: 0.000072
Batch 100: Loss 0.7316 (Cls: 0.7316) LR: 0.000071
Batch 110: Loss 0.8154 (Cls: 0.8154) LR: 0.000071
Batch 120: Loss 0.7577 (Cls: 0.7577) LR: 0.000071
Batch 130: Loss 0.6568 (Cls: 0.6568) LR: 0.000071
Batch 140: Loss 0.6043 (Cls: 0.6043) LR: 0.000070
Batch 150: Loss 0.6938 (Cls: 0.6938) LR: 0.000070
Batch 160: Loss 0.8301 (Cls: 0.8301) LR: 0.000070
Epoch 28: Loss 0.7663, Acc 0.6156
Val Loss 0.8991, Val Acc 0.5729
Batch 0: Loss 0.7303 (Cls: 0.7303) LR: 0.000070
Batch 10: Loss 0.7417 (Cls: 0.7417) LR: 0.000069
Batch 20: Loss 0.6520 (Cls: 0.6520) LR: 0.000069
Batch 30: Loss 0.6567 (Cls: 0.6567) LR: 0.000069
Batch 40: Loss 0.8200 (Cls: 0.8200) LR: 0.000069
Batch 50: Loss 0.6741 (Cls: 0.6741) LR: 0.000068
Batch 60: Loss 0.8674 (Cls: 0.8674) LR: 0.000068
Batch 70: Loss 0.7148 (Cls: 0.7148) LR: 0.000068
Batch 80: Loss 0.7488 (Cls: 0.7488) LR: 0.000068
Batch 90: Loss 0.8043 (Cls: 0.8043) LR: 0.000067
Batch 100: Loss 0.6146 (Cls: 0.6146) LR: 0.000067
Batch 110: Loss 0.7835 (Cls: 0.7835) LR: 0.000067
Batch 120: Loss 0.7904 (Cls: 0.7904) LR: 0.000067
Batch 130: Loss 0.8433 (Cls: 0.8433) LR: 0.000066
Batch 140: Loss 0.8830 (Cls: 0.8830) LR: 0.000066
Batch 150: Loss 0.7858 (Cls: 0.7858) LR: 0.000066
Batch 160: Loss 0.7441 (Cls: 0.7441) LR: 0.000066
Epoch 29: Loss 0.7558, Acc 0.6293
Val Loss 0.8719, Val Acc 0.5881
Batch 0: Loss 0.8169 (Cls: 0.8169) LR: 0.000065
Batch 10: Loss 0.8375 (Cls: 0.8375) LR: 0.000065
Batch 20: Loss 0.8695 (Cls: 0.8695) LR: 0.000065
Batch 30: Loss 0.8022 (Cls: 0.8022) LR: 0.000065
Batch 40: Loss 0.7461 (Cls: 0.7461) LR: 0.000064
Batch 50: Loss 0.7558 (Cls: 0.7558) LR: 0.000064
Batch 60: Loss 1.0626 (Cls: 1.0626) LR: 0.000064
Batch 70: Loss 0.7619 (Cls: 0.7619) LR: 0.000064
Batch 80: Loss 0.7027 (Cls: 0.7027) LR: 0.000063
Batch 90: Loss 0.6126 (Cls: 0.6126) LR: 0.000063
Batch 100: Loss 0.9623 (Cls: 0.9623) LR: 0.000063
Batch 110: Loss 0.8832 (Cls: 0.8832) LR: 0.000063
Batch 120: Loss 0.7294 (Cls: 0.7294) LR: 0.000062
Batch 130: Loss 0.8786 (Cls: 0.8786) LR: 0.000062
Batch 140: Loss 0.8770 (Cls: 0.8770) LR: 0.000062
Batch 150: Loss 0.7986 (Cls: 0.7986) LR: 0.000062
Batch 160: Loss 0.7273 (Cls: 0.7273) LR: 0.000061
Epoch 30: Loss 0.7472, Acc 0.6471
Val Loss 0.8546, Val Acc 0.6322
Batch 0: Loss 0.7801 (Cls: 0.7801) LR: 0.000061
Batch 10: Loss 0.8650 (Cls: 0.8650) LR: 0.000061
Batch 20: Loss 0.9042 (Cls: 0.9042) LR: 0.000061
Batch 30: Loss 0.7013 (Cls: 0.7013) LR: 0.000060
Batch 40: Loss 0.7037 (Cls: 0.7037) LR: 0.000060
Batch 50: Loss 0.8145 (Cls: 0.8145) LR: 0.000060
Batch 60: Loss 0.6476 (Cls: 0.6476) LR: 0.000060
Batch 70: Loss 0.6760 (Cls: 0.6760) LR: 0.000059
Batch 80: Loss 0.7044 (Cls: 0.7044) LR: 0.000059
Batch 90: Loss 0.6674 (Cls: 0.6674) LR: 0.000059
Batch 100: Loss 0.7658 (Cls: 0.7658) LR: 0.000058
Batch 110: Loss 0.6979 (Cls: 0.6979) LR: 0.000058
Batch 120: Loss 0.7552 (Cls: 0.7552) LR: 0.000058
Batch 130: Loss 0.7523 (Cls: 0.7523) LR: 0.000058
Batch 140: Loss 0.6893 (Cls: 0.6893) LR: 0.000057
Batch 150: Loss 0.6292 (Cls: 0.6292) LR: 0.000057
Batch 160: Loss 0.8158 (Cls: 0.8158) LR: 0.000057
Epoch 31: Loss 0.7433, Acc 0.6466
Val Loss 0.8136, Val Acc 0.6277
Batch 0: Loss 0.7273 (Cls: 0.7273) LR: 0.000057
Batch 10: Loss 0.7387 (Cls: 0.7387) LR: 0.000056
Batch 20: Loss 0.8189 (Cls: 0.8189) LR: 0.000056
Batch 30: Loss 0.7086 (Cls: 0.7086) LR: 0.000056
Batch 40: Loss 0.8084 (Cls: 0.8084) LR: 0.000056
Batch 50: Loss 0.7785 (Cls: 0.7785) LR: 0.000055
Batch 60: Loss 0.8928 (Cls: 0.8928) LR: 0.000055
Batch 70: Loss 0.7977 (Cls: 0.7977) LR: 0.000055
Batch 80: Loss 0.8291 (Cls: 0.8291) LR: 0.000055
Batch 90: Loss 0.7234 (Cls: 0.7234) LR: 0.000054
Batch 100: Loss 0.8323 (Cls: 0.8323) LR: 0.000054
Batch 110: Loss 0.8353 (Cls: 0.8353) LR: 0.000054
Batch 120: Loss 0.6549 (Cls: 0.6549) LR: 0.000053
Batch 130: Loss 0.6753 (Cls: 0.6753) LR: 0.000053
Batch 140: Loss 0.7100 (Cls: 0.7100) LR: 0.000053
Batch 150: Loss 0.7251 (Cls: 0.7251) LR: 0.000053
Batch 160: Loss 0.7229 (Cls: 0.7229) LR: 0.000052
Epoch 32: Loss 0.7321, Acc 0.6514
Val Loss 0.8363, Val Acc 0.6322
Batch 0: Loss 0.7005 (Cls: 0.7005) LR: 0.000052
Batch 10: Loss 0.8152 (Cls: 0.8152) LR: 0.000052
Batch 20: Loss 0.8165 (Cls: 0.8165) LR: 0.000052
Batch 30: Loss 0.7792 (Cls: 0.7792) LR: 0.000051
Batch 40: Loss 0.7445 (Cls: 0.7445) LR: 0.000051
Batch 50: Loss 0.7107 (Cls: 0.7107) LR: 0.000051
Batch 60: Loss 0.6323 (Cls: 0.6323) LR: 0.000051
Batch 70: Loss 0.7745 (Cls: 0.7745) LR: 0.000050
Batch 80: Loss 0.6120 (Cls: 0.6120) LR: 0.000050
Batch 90: Loss 0.7367 (Cls: 0.7367) LR: 0.000050
Batch 100: Loss 0.7542 (Cls: 0.7542) LR: 0.000050
Batch 110: Loss 0.7177 (Cls: 0.7177) LR: 0.000049
Batch 120: Loss 0.6608 (Cls: 0.6608) LR: 0.000049
Batch 130: Loss 0.6543 (Cls: 0.6543) LR: 0.000049
Batch 140: Loss 0.7973 (Cls: 0.7973) LR: 0.000048
Batch 150: Loss 0.6939 (Cls: 0.6939) LR: 0.000048
Batch 160: Loss 0.8314 (Cls: 0.8314) LR: 0.000048
Epoch 33: Loss 0.7221, Acc 0.6594
Val Loss 0.8765, Val Acc 0.5973
Batch 0: Loss 0.6578 (Cls: 0.6578) LR: 0.000048
Batch 10: Loss 0.7102 (Cls: 0.7102) LR: 0.000047
Batch 20: Loss 0.6161 (Cls: 0.6161) LR: 0.000047
Batch 30: Loss 0.7463 (Cls: 0.7463) LR: 0.000047
Batch 40: Loss 0.6932 (Cls: 0.6932) LR: 0.000047
Batch 50: Loss 0.6519 (Cls: 0.6519) LR: 0.000046
Batch 60: Loss 0.7770 (Cls: 0.7770) LR: 0.000046
Batch 70: Loss 0.7121 (Cls: 0.7121) LR: 0.000046
Batch 80: Loss 0.7316 (Cls: 0.7316) LR: 0.000046
Batch 90: Loss 0.7794 (Cls: 0.7794) LR: 0.000045
Batch 100: Loss 0.6741 (Cls: 0.6741) LR: 0.000045
Batch 110: Loss 0.7132 (Cls: 0.7132) LR: 0.000045
Batch 120: Loss 0.6539 (Cls: 0.6539) LR: 0.000045
Batch 130: Loss 0.7612 (Cls: 0.7612) LR: 0.000044
Batch 140: Loss 0.6459 (Cls: 0.6459) LR: 0.000044
Batch 150: Loss 0.6850 (Cls: 0.6850) LR: 0.000044
Batch 160: Loss 0.9048 (Cls: 0.9048) LR: 0.000043
Epoch 34: Loss 0.7069, Acc 0.6775
Val Loss 0.8496, Val Acc 0.6033
Batch 0: Loss 0.6352 (Cls: 0.6352) LR: 0.000043
Batch 10: Loss 0.6943 (Cls: 0.6943) LR: 0.000043
Batch 20: Loss 0.6803 (Cls: 0.6803) LR: 0.000043
Batch 30: Loss 0.6784 (Cls: 0.6784) LR: 0.000042
Batch 40: Loss 0.8346 (Cls: 0.8346) LR: 0.000042
Batch 50: Loss 0.5633 (Cls: 0.5633) LR: 0.000042
Batch 60: Loss 0.6373 (Cls: 0.6373) LR: 0.000042
Batch 70: Loss 0.6917 (Cls: 0.6917) LR: 0.000041
Batch 80: Loss 0.7672 (Cls: 0.7672) LR: 0.000041
Batch 90: Loss 0.7049 (Cls: 0.7049) LR: 0.000041
Batch 100: Loss 0.6933 (Cls: 0.6933) LR: 0.000041
Batch 110: Loss 0.7312 (Cls: 0.7312) LR: 0.000040
Batch 120: Loss 0.7968 (Cls: 0.7968) LR: 0.000040
Batch 130: Loss 0.6638 (Cls: 0.6638) LR: 0.000040
Batch 140: Loss 0.6119 (Cls: 0.6119) LR: 0.000040
Batch 150: Loss 0.7975 (Cls: 0.7975) LR: 0.000039
Batch 160: Loss 0.7665 (Cls: 0.7665) LR: 0.000039
Epoch 35: Loss 0.7008, Acc 0.6826
Val Loss 0.8134, Val Acc 0.6467
Batch 0: Loss 0.6884 (Cls: 0.6884) LR: 0.000039
Batch 10: Loss 0.7782 (Cls: 0.7782) LR: 0.000039
Batch 20: Loss 0.7631 (Cls: 0.7631) LR: 0.000038
Batch 30: Loss 0.7736 (Cls: 0.7736) LR: 0.000038
Batch 40: Loss 0.7060 (Cls: 0.7060) LR: 0.000038
Batch 50: Loss 0.7413 (Cls: 0.7413) LR: 0.000038
Batch 60: Loss 0.8124 (Cls: 0.8124) LR: 0.000037
Batch 70: Loss 0.5774 (Cls: 0.5774) LR: 0.000037
Batch 80: Loss 0.7870 (Cls: 0.7870) LR: 0.000037
Batch 90: Loss 0.6630 (Cls: 0.6630) LR: 0.000037
Batch 100: Loss 0.7619 (Cls: 0.7619) LR: 0.000036
Batch 110: Loss 0.6473 (Cls: 0.6473) LR: 0.000036
Batch 120: Loss 0.6775 (Cls: 0.6775) LR: 0.000036
Batch 130: Loss 0.7069 (Cls: 0.7069) LR: 0.000035
Batch 140: Loss 0.6150 (Cls: 0.6150) LR: 0.000035
Batch 150: Loss 0.7557 (Cls: 0.7557) LR: 0.000035
Batch 160: Loss 0.6846 (Cls: 0.6846) LR: 0.000035
Epoch 36: Loss 0.6910, Acc 0.6846
Val Loss 0.8200, Val Acc 0.6322
Batch 0: Loss 0.6490 (Cls: 0.6490) LR: 0.000034
Batch 10: Loss 0.8154 (Cls: 0.8154) LR: 0.000034
Batch 20: Loss 0.7889 (Cls: 0.7889) LR: 0.000034
Batch 30: Loss 0.5798 (Cls: 0.5798) LR: 0.000034
Batch 40: Loss 0.6265 (Cls: 0.6265) LR: 0.000033
Batch 50: Loss 0.5899 (Cls: 0.5899) LR: 0.000033
Batch 60: Loss 0.7692 (Cls: 0.7692) LR: 0.000033
Batch 70: Loss 0.7519 (Cls: 0.7519) LR: 0.000033
Batch 80: Loss 0.5801 (Cls: 0.5801) LR: 0.000032
Batch 90: Loss 0.6992 (Cls: 0.6992) LR: 0.000032
Batch 100: Loss 0.6691 (Cls: 0.6691) LR: 0.000032
Batch 110: Loss 0.7487 (Cls: 0.7487) LR: 0.000032
Batch 120: Loss 0.7229 (Cls: 0.7229) LR: 0.000032
Batch 130: Loss 0.7130 (Cls: 0.7130) LR: 0.000031
Batch 140: Loss 0.6113 (Cls: 0.6113) LR: 0.000031
Batch 150: Loss 0.6447 (Cls: 0.6447) LR: 0.000031
Batch 160: Loss 0.7049 (Cls: 0.7049) LR: 0.000031
Epoch 37: Loss 0.6680, Acc 0.7183
Val Loss 0.8271, Val Acc 0.6429
Batch 0: Loss 0.5151 (Cls: 0.5151) LR: 0.000030
Batch 10: Loss 0.7598 (Cls: 0.7598) LR: 0.000030
Batch 20: Loss 0.7052 (Cls: 0.7052) LR: 0.000030
Batch 30: Loss 0.7635 (Cls: 0.7635) LR: 0.000030
Batch 40: Loss 0.6349 (Cls: 0.6349) LR: 0.000029
Batch 50: Loss 0.6028 (Cls: 0.6028) LR: 0.000029
Batch 60: Loss 0.6924 (Cls: 0.6924) LR: 0.000029
Batch 70: Loss 0.6869 (Cls: 0.6869) LR: 0.000029
Batch 80: Loss 0.8353 (Cls: 0.8353) LR: 0.000028
Batch 90: Loss 0.7152 (Cls: 0.7152) LR: 0.000028
Batch 100: Loss 0.6432 (Cls: 0.6432) LR: 0.000028
Batch 110: Loss 0.5708 (Cls: 0.5708) LR: 0.000028
Batch 120: Loss 0.8052 (Cls: 0.8052) LR: 0.000027
Batch 130: Loss 0.5577 (Cls: 0.5577) LR: 0.000027
Batch 140: Loss 0.6513 (Cls: 0.6513) LR: 0.000027
Batch 150: Loss 0.5769 (Cls: 0.5769) LR: 0.000027
Batch 160: Loss 0.7798 (Cls: 0.7798) LR: 0.000026
Epoch 38: Loss 0.6644, Acc 0.7128
Val Loss 0.7801, Val Acc 0.6664
Batch 0: Loss 0.7102 (Cls: 0.7102) LR: 0.000026
Batch 10: Loss 0.5903 (Cls: 0.5903) LR: 0.000026
Batch 20: Loss 0.6308 (Cls: 0.6308) LR: 0.000026
Batch 30: Loss 0.6344 (Cls: 0.6344) LR: 0.000026
Batch 40: Loss 0.7042 (Cls: 0.7042) LR: 0.000025
Batch 50: Loss 0.7604 (Cls: 0.7604) LR: 0.000025
Batch 60: Loss 0.5401 (Cls: 0.5401) LR: 0.000025
Batch 70: Loss 0.6138 (Cls: 0.6138) LR: 0.000025
Batch 80: Loss 0.6719 (Cls: 0.6719) LR: 0.000024
Batch 90: Loss 0.6298 (Cls: 0.6298) LR: 0.000024
Batch 100: Loss 0.6845 (Cls: 0.6845) LR: 0.000024
Batch 110: Loss 0.6398 (Cls: 0.6398) LR: 0.000024
Batch 120: Loss 0.7275 (Cls: 0.7275) LR: 0.000024
Batch 130: Loss 0.6376 (Cls: 0.6376) LR: 0.000023
Batch 140: Loss 0.6242 (Cls: 0.6242) LR: 0.000023
Batch 150: Loss 0.6258 (Cls: 0.6258) LR: 0.000023
Batch 160: Loss 0.7719 (Cls: 0.7719) LR: 0.000023
Epoch 39: Loss 0.6515, Acc 0.7293
Val Loss 0.8419, Val Acc 0.6269
Batch 0: Loss 0.6045 (Cls: 0.6045) LR: 0.000022
Batch 10: Loss 0.7584 (Cls: 0.7584) LR: 0.000022
Batch 20: Loss 0.6421 (Cls: 0.6421) LR: 0.000022
Batch 30: Loss 0.7252 (Cls: 0.7252) LR: 0.000022
Batch 40: Loss 0.6338 (Cls: 0.6338) LR: 0.000022
Batch 50: Loss 0.6501 (Cls: 0.6501) LR: 0.000021
Batch 60: Loss 0.6575 (Cls: 0.6575) LR: 0.000021
Batch 70: Loss 0.6913 (Cls: 0.6913) LR: 0.000021
Batch 80: Loss 0.5693 (Cls: 0.5693) LR: 0.000021
Batch 90: Loss 0.6971 (Cls: 0.6971) LR: 0.000020
Batch 100: Loss 0.6285 (Cls: 0.6285) LR: 0.000020
Batch 110: Loss 0.6897 (Cls: 0.6897) LR: 0.000020
Batch 120: Loss 0.6181 (Cls: 0.6181) LR: 0.000020
Batch 130: Loss 0.5735 (Cls: 0.5735) LR: 0.000020
Batch 140: Loss 0.5524 (Cls: 0.5524) LR: 0.000019
Batch 150: Loss 0.6027 (Cls: 0.6027) LR: 0.000019
Batch 160: Loss 0.6989 (Cls: 0.6989) LR: 0.000019
Epoch 40: Loss 0.6393, Acc 0.7385
Val Loss 0.8221, Val Acc 0.6299
Batch 0: Loss 0.7176 (Cls: 0.7176) LR: 0.000019
Batch 10: Loss 0.6216 (Cls: 0.6216) LR: 0.000019
Batch 20: Loss 0.5854 (Cls: 0.5854) LR: 0.000018
Batch 30: Loss 0.6251 (Cls: 0.6251) LR: 0.000018
Batch 40: Loss 0.6080 (Cls: 0.6080) LR: 0.000018
Batch 50: Loss 0.6178 (Cls: 0.6178) LR: 0.000018
Batch 60: Loss 0.7694 (Cls: 0.7694) LR: 0.000018
Batch 70: Loss 0.5916 (Cls: 0.5916) LR: 0.000017
Batch 80: Loss 0.6969 (Cls: 0.6969) LR: 0.000017
Batch 90: Loss 0.7230 (Cls: 0.7230) LR: 0.000017
Batch 100: Loss 0.5020 (Cls: 0.5020) LR: 0.000017
Batch 110: Loss 0.6824 (Cls: 0.6824) LR: 0.000017
Batch 120: Loss 0.5917 (Cls: 0.5917) LR: 0.000016
Batch 130: Loss 0.6551 (Cls: 0.6551) LR: 0.000016
Batch 140: Loss 0.5939 (Cls: 0.5939) LR: 0.000016
Batch 150: Loss 0.6662 (Cls: 0.6662) LR: 0.000016
Batch 160: Loss 0.7359 (Cls: 0.7359) LR: 0.000016
Epoch 41: Loss 0.6376, Acc 0.7434
Val Loss 0.7882, Val Acc 0.6641
Batch 0: Loss 0.5771 (Cls: 0.5771) LR: 0.000015
Batch 10: Loss 0.7822 (Cls: 0.7822) LR: 0.000015
Batch 20: Loss 0.5992 (Cls: 0.5992) LR: 0.000015
Batch 30: Loss 0.6169 (Cls: 0.6169) LR: 0.000015
Batch 40: Loss 0.6614 (Cls: 0.6614) LR: 0.000015
Batch 50: Loss 0.6925 (Cls: 0.6925) LR: 0.000014
Batch 60: Loss 0.6974 (Cls: 0.6974) LR: 0.000014
Batch 70: Loss 0.5808 (Cls: 0.5808) LR: 0.000014
Batch 80: Loss 0.5715 (Cls: 0.5715) LR: 0.000014
Batch 90: Loss 0.4333 (Cls: 0.4333) LR: 0.000014
Batch 100: Loss 0.6679 (Cls: 0.6679) LR: 0.000014
Batch 110: Loss 0.4742 (Cls: 0.4742) LR: 0.000013
Batch 120: Loss 0.7798 (Cls: 0.7798) LR: 0.000013
Batch 130: Loss 0.6325 (Cls: 0.6325) LR: 0.000013
Batch 140: Loss 0.6795 (Cls: 0.6795) LR: 0.000013
Batch 150: Loss 0.5535 (Cls: 0.5535) LR: 0.000013
Batch 160: Loss 0.7113 (Cls: 0.7113) LR: 0.000012
Epoch 42: Loss 0.6237, Acc 0.7539
Val Loss 0.7935, Val Acc 0.6619
Batch 0: Loss 0.6982 (Cls: 0.6982) LR: 0.000012
Batch 10: Loss 0.6711 (Cls: 0.6711) LR: 0.000012
Batch 20: Loss 0.6103 (Cls: 0.6103) LR: 0.000012
Batch 30: Loss 0.7012 (Cls: 0.7012) LR: 0.000012
Batch 40: Loss 0.4919 (Cls: 0.4919) LR: 0.000012
Batch 50: Loss 0.6528 (Cls: 0.6528) LR: 0.000011
Batch 60: Loss 0.5480 (Cls: 0.5480) LR: 0.000011
Batch 70: Loss 0.5966 (Cls: 0.5966) LR: 0.000011
Batch 80: Loss 0.6104 (Cls: 0.6104) LR: 0.000011
Batch 90: Loss 0.4783 (Cls: 0.4783) LR: 0.000011
Batch 100: Loss 0.5697 (Cls: 0.5697) LR: 0.000011
Batch 110: Loss 0.6884 (Cls: 0.6884) LR: 0.000010
Batch 120: Loss 0.5618 (Cls: 0.5618) LR: 0.000010
Batch 130: Loss 0.7817 (Cls: 0.7817) LR: 0.000010
Batch 140: Loss 0.4834 (Cls: 0.4834) LR: 0.000010
Batch 150: Loss 0.5232 (Cls: 0.5232) LR: 0.000010
Batch 160: Loss 0.6159 (Cls: 0.6159) LR: 0.000010
Epoch 43: Loss 0.6151, Acc 0.7591
Val Loss 0.7733, Val Acc 0.6801
Batch 0: Loss 0.6312 (Cls: 0.6312) LR: 0.000010
Batch 10: Loss 0.5952 (Cls: 0.5952) LR: 0.000009
Batch 20: Loss 0.7564 (Cls: 0.7564) LR: 0.000009
Batch 30: Loss 0.5973 (Cls: 0.5973) LR: 0.000009
Batch 40: Loss 0.6473 (Cls: 0.6473) LR: 0.000009
Batch 50: Loss 0.5661 (Cls: 0.5661) LR: 0.000009
Batch 60: Loss 0.5897 (Cls: 0.5897) LR: 0.000009
Batch 70: Loss 0.5494 (Cls: 0.5494) LR: 0.000008
Batch 80: Loss 0.6086 (Cls: 0.6086) LR: 0.000008
Batch 90: Loss 0.6319 (Cls: 0.6319) LR: 0.000008
Batch 100: Loss 0.4974 (Cls: 0.4974) LR: 0.000008
Batch 110: Loss 0.6222 (Cls: 0.6222) LR: 0.000008
Batch 120: Loss 0.6388 (Cls: 0.6388) LR: 0.000008
Batch 130: Loss 0.5740 (Cls: 0.5740) LR: 0.000008
Batch 140: Loss 0.7993 (Cls: 0.7993) LR: 0.000007
Batch 150: Loss 0.6257 (Cls: 0.6257) LR: 0.000007
Batch 160: Loss 0.6602 (Cls: 0.6602) LR: 0.000007
Epoch 44: Loss 0.6193, Acc 0.7558
Val Loss 0.7824, Val Acc 0.6611
Batch 0: Loss 0.4352 (Cls: 0.4352) LR: 0.000007
Batch 10: Loss 0.6441 (Cls: 0.6441) LR: 0.000007
Batch 20: Loss 0.4682 (Cls: 0.4682) LR: 0.000007
Batch 30: Loss 0.5121 (Cls: 0.5121) LR: 0.000007
Batch 40: Loss 0.4687 (Cls: 0.4687) LR: 0.000007
Batch 50: Loss 0.6392 (Cls: 0.6392) LR: 0.000006
Batch 60: Loss 0.7121 (Cls: 0.7121) LR: 0.000006
Batch 70: Loss 0.5856 (Cls: 0.5856) LR: 0.000006
Batch 80: Loss 0.7467 (Cls: 0.7467) LR: 0.000006
Batch 90: Loss 0.4904 (Cls: 0.4904) LR: 0.000006
Batch 100: Loss 0.6196 (Cls: 0.6196) LR: 0.000006
Batch 110: Loss 0.7462 (Cls: 0.7462) LR: 0.000006
Batch 120: Loss 0.6434 (Cls: 0.6434) LR: 0.000006
Batch 130: Loss 0.5858 (Cls: 0.5858) LR: 0.000005
Batch 140: Loss 0.6781 (Cls: 0.6781) LR: 0.000005
Batch 150: Loss 0.5130 (Cls: 0.5130) LR: 0.000005
Batch 160: Loss 0.7055 (Cls: 0.7055) LR: 0.000005
Epoch 45: Loss 0.6176, Acc 0.7582
Val Loss 0.7651, Val Acc 0.6831
Batch 0: Loss 0.5414 (Cls: 0.5414) LR: 0.000005
Batch 10: Loss 0.6491 (Cls: 0.6491) LR: 0.000005
Batch 20: Loss 0.5682 (Cls: 0.5682) LR: 0.000005
Batch 30: Loss 0.6590 (Cls: 0.6590) LR: 0.000005
Batch 40: Loss 0.6531 (Cls: 0.6531) LR: 0.000004
Batch 50: Loss 0.6850 (Cls: 0.6850) LR: 0.000004
Batch 60: Loss 0.5481 (Cls: 0.5481) LR: 0.000004
Batch 70: Loss 0.5796 (Cls: 0.5796) LR: 0.000004
Batch 80: Loss 0.6009 (Cls: 0.6009) LR: 0.000004
Batch 90: Loss 0.5356 (Cls: 0.5356) LR: 0.000004
Batch 100: Loss 0.6121 (Cls: 0.6121) LR: 0.000004
Batch 110: Loss 0.5321 (Cls: 0.5321) LR: 0.000004
Batch 120: Loss 0.5619 (Cls: 0.5619) LR: 0.000004
Batch 130: Loss 0.5969 (Cls: 0.5969) LR: 0.000004
Batch 140: Loss 0.5621 (Cls: 0.5621) LR: 0.000003
Batch 150: Loss 0.7608 (Cls: 0.7608) LR: 0.000003
Batch 160: Loss 0.6015 (Cls: 0.6015) LR: 0.000003
Epoch 46: Loss 0.6032, Acc 0.7716
Val Loss 0.7383, Val Acc 0.7036
New best model saved with Val Acc: 0.7036
Batch 0: Loss 0.5135 (Cls: 0.5135) LR: 0.000003
Batch 10: Loss 0.5491 (Cls: 0.5491) LR: 0.000003
Batch 20: Loss 0.5957 (Cls: 0.5957) LR: 0.000003
Batch 30: Loss 0.5600 (Cls: 0.5600) LR: 0.000003
Batch 40: Loss 0.6949 (Cls: 0.6949) LR: 0.000003
Batch 50: Loss 0.4747 (Cls: 0.4747) LR: 0.000003
Batch 60: Loss 0.5850 (Cls: 0.5850) LR: 0.000003
Batch 70: Loss 0.4850 (Cls: 0.4850) LR: 0.000003
Batch 80: Loss 0.6843 (Cls: 0.6843) LR: 0.000002
Batch 90: Loss 0.7449 (Cls: 0.7449) LR: 0.000002
Batch 100: Loss 0.4747 (Cls: 0.4747) LR: 0.000002
Batch 110: Loss 0.6088 (Cls: 0.6088) LR: 0.000002
Batch 120: Loss 0.6219 (Cls: 0.6219) LR: 0.000002
Batch 130: Loss 0.6201 (Cls: 0.6201) LR: 0.000002
Batch 140: Loss 0.5618 (Cls: 0.5618) LR: 0.000002
Batch 150: Loss 0.5625 (Cls: 0.5625) LR: 0.000002
Batch 160: Loss 0.6341 (Cls: 0.6341) LR: 0.000002
Epoch 47: Loss 0.6011, Acc 0.7777
Val Loss 0.7662, Val Acc 0.6816
Batch 0: Loss 0.5604 (Cls: 0.5604) LR: 0.000002
Batch 10: Loss 0.7831 (Cls: 0.7831) LR: 0.000002
Batch 20: Loss 0.7150 (Cls: 0.7150) LR: 0.000002
Batch 30: Loss 0.6216 (Cls: 0.6216) LR: 0.000002
Batch 40: Loss 0.5457 (Cls: 0.5457) LR: 0.000002
Batch 50: Loss 0.5197 (Cls: 0.5197) LR: 0.000001
Batch 60: Loss 0.5821 (Cls: 0.5821) LR: 0.000001
Batch 70: Loss 0.4990 (Cls: 0.4990) LR: 0.000001
Batch 80: Loss 0.5457 (Cls: 0.5457) LR: 0.000001
Batch 90: Loss 0.5318 (Cls: 0.5318) LR: 0.000001
Batch 100: Loss 0.5266 (Cls: 0.5266) LR: 0.000001
Batch 110: Loss 0.6181 (Cls: 0.6181) LR: 0.000001
Batch 120: Loss 0.5389 (Cls: 0.5389) LR: 0.000001
Batch 130: Loss 0.5329 (Cls: 0.5329) LR: 0.000001
Batch 140: Loss 0.6666 (Cls: 0.6666) LR: 0.000001
Batch 150: Loss 0.6090 (Cls: 0.6090) LR: 0.000001
Batch 160: Loss 0.5405 (Cls: 0.5405) LR: 0.000001
Epoch 48: Loss 0.5945, Acc 0.7775
Val Loss 0.7672, Val Acc 0.6877
Batch 0: Loss 0.6792 (Cls: 0.6792) LR: 0.000001
Batch 10: Loss 0.6743 (Cls: 0.6743) LR: 0.000001
Batch 20: Loss 0.7591 (Cls: 0.7591) LR: 0.000001
Batch 30: Loss 0.5239 (Cls: 0.5239) LR: 0.000001
Batch 40: Loss 0.4682 (Cls: 0.4682) LR: 0.000001
Batch 50: Loss 0.5746 (Cls: 0.5746) LR: 0.000001
Batch 60: Loss 0.4787 (Cls: 0.4787) LR: 0.000001
Batch 70: Loss 0.5800 (Cls: 0.5800) LR: 0.000000
Batch 80: Loss 0.6838 (Cls: 0.6838) LR: 0.000000
Batch 90: Loss 0.5715 (Cls: 0.5715) LR: 0.000000
Batch 100: Loss 0.6399 (Cls: 0.6399) LR: 0.000000
Batch 110: Loss 0.5951 (Cls: 0.5951) LR: 0.000000
Batch 120: Loss 0.5336 (Cls: 0.5336) LR: 0.000000
Batch 130: Loss 0.4645 (Cls: 0.4645) LR: 0.000000
Batch 140: Loss 0.6162 (Cls: 0.6162) LR: 0.000000
Batch 150: Loss 0.5542 (Cls: 0.5542) LR: 0.000000
Batch 160: Loss 0.7033 (Cls: 0.7033) LR: 0.000000
Epoch 49: Loss 0.6006, Acc 0.7784
Val Loss 0.7637, Val Acc 0.6900
Batch 0: Loss 0.5445 (Cls: 0.5445) LR: 0.000000
Batch 10: Loss 0.4815 (Cls: 0.4815) LR: 0.000000
Batch 20: Loss 0.5528 (Cls: 0.5528) LR: 0.000000
Batch 30: Loss 0.6208 (Cls: 0.6208) LR: 0.000000
Batch 40: Loss 0.4837 (Cls: 0.4837) LR: 0.000000
Batch 50: Loss 0.6503 (Cls: 0.6503) LR: 0.000000
Batch 60: Loss 0.5376 (Cls: 0.5376) LR: 0.000000
Batch 70: Loss 0.5366 (Cls: 0.5366) LR: 0.000000
Batch 80: Loss 0.6662 (Cls: 0.6662) LR: 0.000000
Batch 90: Loss 0.5848 (Cls: 0.5848) LR: 0.000000
Batch 100: Loss 0.8130 (Cls: 0.8130) LR: 0.000000
Batch 110: Loss 0.5647 (Cls: 0.5647) LR: 0.000000
Batch 120: Loss 0.7699 (Cls: 0.7699) LR: 0.000000
Batch 130: Loss 0.6780 (Cls: 0.6780) LR: 0.000000
Batch 140: Loss 0.5870 (Cls: 0.5870) LR: 0.000000
Batch 150: Loss 0.5947 (Cls: 0.5947) LR: 0.000000
Batch 160: Loss 0.5483 (Cls: 0.5483) LR: 0.000000
Epoch 50: Loss 0.6072, Acc 0.7716
/root/shared-nvme/fp/src/training/train.py:318: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model.load_state_dict(torch.load(best_model_path))
Val Loss 0.7641, Val Acc 0.6900
Running Test Evaluation...
Loaded best model for testing.
Test Loss: 0.8543, Test Acc: 0.6141
